[["index.html", "Data Science Book Chapter 1 INTRO", " Data Science Book ukasz Muszyski 2023-04-03 Chapter 1 INTRO This is notebok about AI algorithms. "],["introduction.html", "Chapter 2 INTRODUCTION 2.1 TO DO 2.2 draft 2.3 Introduction 2.4 AI problems and algorithms classification 2.5 Meta issues 2.6 Rozne podzialy algorytmow", " Chapter 2 INTRODUCTION 2.1 TO DO Feature encoding iml variance / covariance shift Jak laczy sie optimizer i loss w pytorch: https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step https://ichi.pro/pl/badanie-porownawcze-architektur-segmentacji-obrazu-z-wykorzystaniem-glebokiego-uczenia-60763173541384 Zdecydowa gdzie maj by materiay o deep learningu. ICA i blind source separation (dobry artyku: Independent Component Analysis: Algorithm and Application by Aapo Hyvarien and Erkki Oja). ICA ma 2 bardzo wane zaoenia. Ukryte koponenty musz by ortogonalne i musz mie rozkad Niegaussowski. Liczba obserwacji nie musi by wiksza ni liczba wymiarów. Johnson-Lindenstrauss lemma (o tym e macierz wielowymiarow mona aproksymowa macierz o niszej iloc wymiarów zachowujc odlegoci midzy obserwacjami poprzez przemnoenie jest przez macierz losow). Jest wykorzystywany przez technik (dostpn w sklearn) o nazwie RAndom Projection suc do redukcji wymiarowoci. ensembled models conformal prediction calibration of probability isitonic regression possion model correlation between imbalanced variables survuval analysis miary do modeli klasyfikacyjnych feature importance isolated forest GLM - gdzie sa reszty? Standaryzacja do analizy klasrowania a wymóg duej zmiennoci (min. 10%). Przecie po standaryzacji kada zmienna bdzie miaa do samo odchylenie. Jak rozpozna zmienn zestandaryzowan która pierwotnie miaa ma zmienno. Chyba trzeba to zbada przed standaryacja. blad w implementacji xg boosta w sklearn - w kolejnych krokach obserwacje si z prob nakladaja. notat. ARIMA skalowanie platta : svm i score w kalibracji prawdopodobienstwa. optymalizacja byesowska w grid searchu mutual information , entrophy, cross entrophy, kty do szukania outliers wezly czyli metoda MARS kalibracja w ryzyku kredytowym - dodawanie zewnetrznych informacji git random forest w szeregach czasowych - nie lapie trendow. Uniform Manifold Approximation and Projection (UMAP) autokorelacja i autokorelacja czastkowa colaborative filtering C4.5, GAN uczenie przyrostowe dla gradientów. Czy ma sens? Niezbalansowanie próby a róne analizy (korelacja , PCA, miary) Dobry model a jego predykcje w przyszoci. Zaómy e mam super model odrzucajcy zych kredytobiorców. Z czasem pojawi si problem e w próbie nie bdzie zych kredytów. Jak reestymowa model liczba dunbara jak zrobi dobry minitoring modeli. Jak wyznaczy przedziay ufnoci dla predykcji Jak wykrywa podpopulacje? Jedna silna zmienna. Niestabilno relacji w podpopulacjach. Kalibrowanie prawdopodobiestwo. covariate shift conformal prediction link Czy w clusteringu jest sens podziau na train i test set. Jak mierzy over/under fitting? Pami w rozkadach prawdopodobiestwa image segmentatnion to nie to samo co semantic segmentation : link Pami w macierzach markowa (First-order Markov model, Second-order Markov model). Hidden Markov model. Fajny przykad z psem i pogod: link Co to s sieci syjamskie zerkn na notatki które mam w telefonie bo tam te jest zanotowanych wiele ciekawych problemów statystycznych lini o PCA i eigenvalues: https://wiki.pathmind.com/eigenvector https://medium.com/analytics-vidhya/eigenvectors-and-eigenvalues-and-there-use-in-principal-component-analysis-machine-learning-1f97fdbdb303 2.2 draft 2.3 Introduction 2.4 AI problems and algorithms classification 2.5 Meta issues 2.5.1 Knowledge representation 2.5.2 Types of learning 2.5.2.1 Ensemble (zespoowe) Dowód matematyczny skutecznoci podejcia wielomodelowego: (Eugeniusz 2008) s 84 Intuicyjne wyjanienie skutecznoci dziaania podejcia Ensemble: (Geron 2018) Ensemble jest traktowany jako metoda dedykowana pod uczenie nadzorowane. Dla klastrowania istnieje odpowiednik nazywany Consensus clustering. Zakadamy e: Mamy e w danych mamy dosy skomplikowane zalenoci przy których nie poradzimy sobie z uyciem prostego modelu Zakadamy e moemy zbudowa n prostych modeli (weak learners) z których kady bdzie mia znaczco rónic si wiedz od pozostaych modeli. Ale kady model jest jednak na tyle dobry, e nie jest modelem dajcym czysto losowe rezultaty. Wtedy ensemble moe by sensown alternatyw dla robienia jednego duego i skomplikowanego modelu. Utworzenie duego modelu naraa na komplikacje w jego specyfikacji oraz na problemy z overfittingiem. Wane jednak jest aby modele byy róne pod ktem wiedzy. Uzyskujemy to poprzez: budowanie kadego modelu innym algorytmem stosowanie algorytmów które maj du wariancj (niewielka zmiana w danych moe istotnie wpyn na predykcje). Dlatego najpopularniejszym algorytmem jest drzewo. budowanie modeli na innych podpróbkach budowanie modeli na innym zestawie zmiennych objaniajcych 2.5.2.1.1 Bagging and Pasting W bagging tworzymy kolejne podpróbki poprzez losowanie boostrapowe. W pasting tworzymy kolejne próbki poprzez losowanie ze zwracaniem. Pros Many weak learners aggregated typically outperform a single learner over the entire set, and has less overfit Removes variance in high-variance low-bias data sets[7] Can be performed in parallel, as each separate bootstrap can be processed on its own before combination[8] Cons In a data set with high bias, bagging will also carry high bias into its aggregate[7] Loss of interpretability of a model. Can be computationally expensive depending on the data set 2.5.2.1.2 Boosting 2.5.2.1.3 Stacking 2.5.3 Model complexity 2.5.4 Overfittng / underfitting 2.5.5 Bias / Variance trade off 2.5.6 Curse of dimentionality 2.5.7 Sparious phenomenon 2.6 Rozne podzialy algorytmow "],["auxiliary-fields.html", "Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.8 Math_2  Statistics 3.9 Math_3 - Deep Learning 3.10 Math_4  Game theory 3.11 Math_5  Optimisation 3.12 Math_5  Other issues", " Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.7.1 Basics 3.7.2 Univariate distributions 3.7.3 Multivariate distributions 3.7.4 Stochastic processes 3.8 Math_2  Statistics 3.8.1 Descriptive statistics 3.8.1.1 Univariate 3.8.1.2 Multivariate 3.8.1.2.1 Correlation Rozbudowane omówienie waciwoci korelacji: link Kowariancja sumy zmiennych losowych: Wariancja sumy zmiennych losowych: Róne waciwoci korelacji (np. przechodnio): link Geometryczna interpretacja korelacji wielorakiej: Semipartial correlation The semipartial (or part) correlation statistic is similar to the partial correlation statistic. Both measure variance after certain factors are controlled for, but to calculate the semipartial correlation one holds the third variable constant for **either X or Y**, whereas for partial correlations one holds the third variable constant for **both**.[6] The semipartial correlation measures unique and joint variance while the partial correlation measures unique variance[clarification needed]. The semipartial (or part) correlation can be viewed as more practically relevant because it is scaled to (i.e., relative to) the total variability in the dependent (response) variable. [7] Conversely, it is less theoretically useful because it is less precise about the unique contribution of the independent variable. Although it may seem paradoxical, the semipartial correlation of X with Y is always less than or equal to the partial correlation of X with Y 3.8.1.2.2 Autocerrelation link:towardsdatascience 3.8.2 Statistical and ecometrical inference 3.8.2.1 Basics 3.8.2.2 Pameters estimation algorithms 3.8.2.2.1 General moments methods 3.8.2.2.2 Maxium likehood 3.8.2.2.3 Nonparametric techniques 3.8.2.3 Sampling techniques 3.8.2.3.1 Holdout 3.8.2.3.2 Boostrap link W bootstrapie z probki n-elementowej losujemy k raz podpróbki n-elementowe ALE ze zwracaniem. Losowanie próbek z podpróbek powinno symulowa losowanie próbek z populacji (The population is to the sample as the sample is to the bootstrap samples). Bootstrap dzielimy na: parametryczny: losowanie jest wykonywane z zadanego rozkadu a nie z samej próbki (czyli zakadamy e znamy klas rozkadu, np. jest to rozkad normalny) semi-parametryczny : The semiparametric bootstrap assumes that the population includes other items that are similar to the observed sample by sampling from a smoothed version of the sample histogram. It turns out that this can be done very simply by first taking a sample with replacement from the observed sample (just like the nonparametric bootstrap) and then adding noise. nieparametryczny. Elementy s losowanie z konkretnych elementów z samej próbki. Boostrap dla regresji (Biecek2008?) s 13. (link)[https://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients] Sample paired response-predictor: Randomly resample pairs of \\(y_ix_i\\), and apply linear regression to each run. After m runs, we obtain a collection of estimated coefficients \\(\\hat{_j},j=1,,m\\).. Finally, compute the quantile of \\(\\hat{_j}\\). Sample error: First apply linear regression on the original observed data, from this model we obtain \\(\\hat{_o}\\) and the error \\(_i\\). Afterwards, randomly resample the error \\(_i\\) and compute the new data with \\(\\hat{_o}\\) and \\(y_i=\\hat{_o}x_i+_i\\). Apply once again linear regression. After m runs, we obtain a collection of estimated coefficeints \\(\\hat{_j},j=1,,m\\). Finally, compute the quantile of \\(\\hat{_j}\\). 3.8.2.3.3 Jacknife 3.8.2.3.4 Permutation (Biecek2008?) s 13 3.8.2.3.5 Cross validation 3.8.2.3.6 Monte Carlo Hasting algorithm. Bardzo dobry filmik link : 3.8.2.3.7 Stratified sampling 3.8.3 Other issues 3.8.3.1 Similarity and dissimilarity meassures for observations 3.8.3.2 Similarity and dissimilarity meassures for distributions 3.8.3.3 Measures of disorder/randomness 3.8.3.4 Conformal prediction 3.8.3.4.1 Przykad knn dla random forest (klasyfikacja i regresja) 3.8.3.4.2 Przykad dla knn dla klasyfikacji (link) Lets do an exercise. Training set: 1. Positive samples: (0, 3), (2, 2), (3, 3) 2. Negative samples: (-1, 1), (-1, -1), (0, 1) Test sample: (0, 0) Let us calculate the euclidean distance from the new sample to each training sample. As conformity measure, use the distance to the nearest sample of a different class divided by the distance to the nearest sample of the same class. 1. Assume the label of (0, 0) is +1. The test sample is the strangest, so the p-value is 1/7 = 0.143. 2. Assume the label of (0, 0) is -1. The test sample is the second most conforming; its rank is 6, so the p-value is 6/7 = 0.857. The p-values are 0.143 (for +1) and 0.857 (for -1). We can predict -1, but our prediction does not achieve statistical significance of 5% for that we should at least have 20 training observations. 3.8.3.5 Paradoxes 3 paradoxes from kdnugget: link 3.9 Math_3 - Deep Learning Dobry zestaw materiaów dm.in do Deep LEarningu z Pytorch : link Wyprowadzenie CNN i z sieci FC (fully connected): link 3.9.1 Róne uwagi 3.9.1.1 Ogólnie o sieciach Jeeli mamy w sieci kilka outputów i dla kadego z nich inn funkcj straty, to wtedy w celu umoliwianie wstecznej propagacji tworzymy jedn funkcj straty które jest np. waon funkcj wszystkich poszczególnych funkcji straty. Kolejno elementów po warstwie trenowanej: The correct order is: Conv &gt; Normalization &gt; Activation &gt; Dropout &gt; Pooling 3.9.1.2 Funkcje aktywacyjne https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/ Wybór funkcji aktywacyjnej do warstw ukrytych (hidden): Wybór funkcji aktywacyjnych do warstwy wyjciowe (output) Dropout: Na czym on polega? Wydaje mi si e wyczenie nauronu to po prostu naoenie na niego funkcji aktywacyjnej które niezalenie wejcia zwraca zero. Funkcja taka ma zerowy gradient, wic jeeli robimy wsteczn propagacj to nie bdziemy modyfikowali wag wyczonego nauronu. Dodatkowo naley pamita o przeskalowaniu. Jeeli w danej warstwie wyczam p procent neuronów to output z pozostaych skaluje przez 1/(1+p). 3.9.1.3 RNN link link propagacja (Backpropagation Through Time (BPTT) wsteczna recznie rozpisana: link link Poniej warto pamita e Wx i Wh s we wszystkich miejscach te same. To zakada oczywicie stao rozmiaru xt i ht w kolejnych warstwach. LSTM: Parametry w funkcjach: input_dim = rozmiar wektora dla konkretnego t n_layers - ilo warstw hidden_dim - rozmiar output = rozmiar stanu ukrytwgo h (pamiec krotkowerminowa) = rozmiar stanu ukrytego c (pamiec dlugoterminowa). Embeding plus LSTM: link Zmienny rozmiar inputu z LSTM (batch = 1): link 3.9.1.4 CNN dwuwymiarowe: Filtry konwolucyjne s wielowymiarowe natomiast warstwy pooling s paskie. Po przepuszczeniu przez nie obrazu zmieniaj si tylko wymiary dot. szerokoci i wysokoci. 3.9.1.4.1 R-CNN (Region-CNN) podstawowe : link Architektura sieci: Zdjcie jest przepuszczane przez algorytm który daje propozycje regionów gdzie mog by obiekty (ROI). Kady proponowany ROI jest przepuszczany przez sie CNN. Ostatnia warstwa pooling w sieci CNN daje nam parametry do regresji która koryguje wspórzdne bounding boxa. Dane s przepuszczane dalej do sieci FC (fully connected) która przekazuje wektor do modelu SVM który dokonuje klasyfikacji. Trenowanie sieci: Przygotowuje sobie jaki algorytm do tworzenia ROI (region of interest), np. Selective Search. Czyli algorytm ten zwróci nam propozycje bounding boxów gdzie mog by jakie elementy. Ewentualne trenowania/dostrajanie tego algorytmu jest dodatkowym zagadnieniem które nas tutaj nie interesuje. Zakadamy e mamy po prostu gotowy dziaajcy algorytm. Mam zbiór ze zdjciami gdzie na kadym jest jeden obiekt danej klasy. Na tym zbiorze wstpnie trenuje sie (np. VGG-16). Teraz zakadam e mam ju zbiór ze zdjciami gdzie kade z nich moe zawiera wiele elementów ronych klas. Dodatkowo s na nich zaznaczona poprawne (ground truth) bounding boxy. Z wstpnie wytrenowanej sieci usuwam ostatni warstw (output layer mogc prognozowa k klas ) i zastpuj j warstw która ma moliwo prognozowania k+1 klas. Ta dodatkowa klasa jest po to aby umoliwi dotrenowanie sieci tak aby umoliwi wykrywanie dodatkowej klasy jak jest to (background). Zatem: Przepuszczam zdjcie przez algorytm ROI. Zaproponowany przez ROI (proposed) region przekazuje do sieci. Nastpnie sie klasyfikuje obiekt (wczeniej musimy zrobi resize, bo kady obraz trafiajcy do sieci musi by tego samego rozmiaru). Jeeli proponowany region pokrywa si z którym z prawdziwych regionów w przynajmniej 50% (pokrycie nazywa si: IoU - Intersection over Union) to sie robi predykcje do jakiej klasy naley obiekt znajdujcy si w tym regionie. Jeeli pokrycie jest poniej 50% to klasyfikujemy region jako to (ta nie maj adnych bounding boxów w groud truh). W kolejnym etapie, znowu usuwam ostatni klasyfikacyjn warstw (output layer). Na bazie wyników z nowej ostatniej warstwy (jest to wektor wynikowy z warstwy FC (fully connected)), trenujemy modele binarne SVM w rozpoznawaniu obiektów. Jeeli mamy k klas to modeli SVM bdzie k. Tutaj znowu zdjcie przechodzi przez algorytm ROI i potem przez sie. Jeeli dla konkretnego i-tego modelu SVM (roznajcego i-t klas) IoU &lt; threshold to automatycznie mamy wynik negtywny, w przeciwnym razie SVM klasyfikuje region binarnie. Dodatkowo równolegle trenujemy mechanizm do korekty rozmiaru i pozycji bounding boxa, tak eby by jak najlepiej dopasowany do obiektu. S to 4 modele regresyjne ( (1) modyfikacja szerokoci (2) modyfikacja wysokoci (3) przesunicie w pionie (4) przesunicie w poziomie). Tak wic poprzez regresje wytrenowan na zalenoci midzy proposed region i ground truth region dostaniemy mechanizm do korekty. Wartoci tych korekt bd oczywicie dynamiczne w zalenoci od zdjcia. Aby t dynamiczno uzyska wartociami które s parametrami regresji staj si features from the last pooling layer of the CNN. 3.9.1.4.2 Fast R-CNN link 3.9.1.4.3 Fast-er R-CNN link; link Jest to ulepszenie R-CNN i Fast R-CNN. Tutaj do tworzenia RoI nie potrzebujemy dodatkowego algorytmu. RoI bd powstaway w ramach naszej sieci i bdzie to tzw RPN (Region proposal network). Dodatkowo co CNN trafia cay obraz a nie RoI to przypiesza trenowanie sieci. RoI s dopiero wykorzystywane na featurach zwracanych przez CNN. Architektura: Na zdjciu ustalam tzw. anchors/boxes. S to okna o rónych rozmiarach i proporcjach. Ilo tych okien i ich rozmiar jest hiperparametrem który moemy ustawia. Kady anchor jest przeliczany dla kadego pixela (czyli slidujemy po kadym pixelu). Zdjcia przepuszczamy przez sie CNN. Wyniki z CNN oraz anchors trafiaj nastpnie do sieci RPN (link), odpowiadajc za tworzenie RoI. Sie zwraca dla kadego anchors prawdopodobiestwa e zawieraj obiekt. Musimy wyselekcjonowa najlepsze anchors. Robimy to poprzez NMS czyli Non-maximum suppression (link): Select the box that has the highest score. Compute its overlap with all other boxes, and remove boxes that overlap it more than the IOU threshold Go back to step 1 and iterate until theres no more boxes with a lower score than the current selected box. Otrzymane najlepsze anchors s dodatkowo korygowane pod wzgldem rozmiaru i pozycji modelem regresyjnym i dziki temu otrzymujemy finalne RoI. Te RoI nastpnie su do wycinania elementów z wyników CNN (czyli wycinamy na featurach, a nie na oryginalnym zdjciu). Nastpnie przepuszczam te wyniki przez warstw RoiI pooling która standaryzuje mi rozmiar outputu. Warstwa ta ma dynamiczny rozmiar w zalenoci od rozmiaru zdjcia. Dynamiczny wymiar umoliwia wanie stay rozmiar outputu przy zmiennym rozmiarze inputu (wycinki mog mie róny rozmiar i proporcje). Standaryzacja zdjcia jest potrzebna aby mona byo je potem przepuci do warstw FC (które w przeciwiestwie do warstw konwolucyjnych musz mie okrelone rozmiary). Dalej wyniki przepuszczam przez kolejne warstwy (tutaj nie wiem czy s to po prostu od razu FC (fully connected), czy jaka wiksza architektura. Ale raczej od razu powinno by FC. Tak czy inaczej na kocu zawsze jest jakie FC i standaryzacja przez RoI pooling ma sens). Po czym mamy rozgazienie aby otrzyma dwie predykcje: klas obiektu (konkretna klasa a nie tylko binarne rozrónienie to/obiekt). korekt (drug - bo pierwsza bya liczona w algorytmie RNP) na wspórzdne bounding box. O tym jak robi to rozgazienie trzeba poczyta w materiaach o podstawowym R-CNN. Trenowanie sieci: RPN - Siec ma rozpoznawa 2 rzeczy: po pierwsze dla kadej pozycji kadego anchora ma binarnie okrela czy mamy do czynienia z obiektem czy z tem. Po drugie musimy mie wspórzdne i wymiary bounding boxów które maj najwiksze prawdopodobiestwo e znajduje si w nich obiekty. Zatem aby wytrenowa RPN pod ktem rozpoznawania to/obiekt zakadamy, e mamy zbiór danych z gronud truch bounding boxes. Dla kadego anchor musz okreli jakie jest prawdopodobiestwo, e zawiera obiekt. Zakadam e stopie w jakim anchor pokrywa si z ground truth bounding box okrela prawdopodobiestwo, e ten anchor rzeczywicie zawiera obiekt a nie to. Zatem moemy zbudowa sie o nastpujcym inpucie: Lets say the 600x800 image shinks 16 times to a 39x51 feature map after applying CNNs. Every position (dla kaego pixela) in the feature map has 9 anchors, and every anchor has two possible labels (background, foreground). If we make the depth of the feature map as 18 (9 anchors x 2 labels), we will make every anchor have a vector with two values (normal called logit) representing foreground and background. Na outpucie siecie dajemy softmax/logistic regression activation function i trenujemy model który nam zwróci prawdopodobiestwo. Nastpnie trenujemy te regresj do korekty proponowanych anchores. Tutaj pamitamy, e to (backgroud) nie ma gorund truth boxes. Ma j je tylko obiekty. Kontynujc zaoenia z przykadu do trenowania labeli mamy: The depth of feature map is 32 (9 anchors x 4 positions (4 bo : wysoko, szeroko, przesunicie w pionie, przesunicie w poziomie)). The overall loss of the RPN is a combination of the classification loss and the regression loss. Jeeli chodzi o trenowanie pozostaych elementów sieci to tutaj trzeba chyba poczyta o zwykym RCNN. Bo de facto w porównaniu do zwykego R-CNN zmieni nam si sposób budowy i lokalizacji algorytmu RoI, ale reszta elementów powinna dziaa podobnie. Pytanie czy RoI mona trenowa równolegle z pozostaymi elementami sieci. 3.9.1.4.4 Yolo Architektura: Przepuszczam obaz przez sie po czym dostaj predykcj bounding boxów i class elementów w bounding boxach. Nastpnie stosuje algorytm NMS. Ma 2 thresholdy. Jeden usuwa boxy któRe maj mae prawdopodobiestwa wystpienia obiektu w boxie (niskie pc). Drui odpowiada za IoU. Jeeli IoU wynosi n to elementy z tej samej klasy dla których bounding boxy pokrywaj si w takim stopniu e IoU jest wikszy ni *n* to usuwamy je (z wyjtkiem tego o najwikszym Pc). Wady sieci. W jednej komórce moe wykry tylko jeden obiekt danej klasy. Trenowanie: Przygotowujemy zbiory obrazow dla których mamy podzia na grid i dla kadego grida okrelone elementy wektora: pc1 c1  cn x1 y1 w1 h1 pc2 c1  cn x2 y2 w2 h2 3.9.2 Embedding Impelementacja word2vec w pytorch: https://gist.github.com/GavinXing/9954ea846072e115bb07d9758892382c Logika dzialania embedding w przypadku pytorch-a dla tekstu (wedug moich domysów!) jezeli chcialbym to zaimplementowac zwykla warstwa Linear: Dostaje obserwacja postawi calego zdanie. Zdanie to dekomponuje na macierz o wymiarach vxd. v - o slownik (wszystkie mozliwe unikalne wyrazy we wszystkich probkach) a d to ilosc slow. Nastepnie slowo po slowie laduje do sieci i dostaje kolejne wyniki o wymiarach e (e to wymiar embeddingu). 3.10 Math_4  Game theory 3.11 Math_5  Optimisation 3.11.1 Functions optimum 3.11.2 Functions optimum  constrained 3.11.2.1 Lagrange Multipliers 3.11.2.2 Linear programming 3.11.2.3 Nonlinear programming 3.11.2.4 Regularization 3.11.3 Functional optimum 3.11.3.1 Dynamic programming 3.11.3.2 Calculus of variations 3.11.4 Cost functions 3.11.5 Back propagation 3.11.6 Heuristic algorithms 3.11.6.1 Swarm algorithm 3.11.6.2 Ants algorithm 3.11.6.3 Genetics algorithms 3.12 Math_5  Other issues 3.12.1 Functions usefull in Data Science 3.12.2 Useful tricks 3.12.3 Linear algebra 3.12.4 Combinatorics 3.12.5 Numerical methods 3.12.6 Equations 3.12.6.1 x as number 3.12.6.2 X as derivative: differential and difference equations 3.12.6.3 Choas 3.12.6.3.1 logistic map link: varitasium channel 3.12.7 Fuzzy logic 3.12.8 Graphs "],["learning-patterns-discovering.html", "Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 4.3 Association 4.4 Dimentionality reduction 4.5 Casuality analysis 4.6 Dimentions decomoposition 4.7 Generative models", " Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 17 algorytmów clusteringowych: link. Rodzaje klastrowania (strona cse): link. Ogólne uwagi: Nie ma jednej precyzyjnie przyjtej definicji klastra. W ogólnoci zakada si e elementy wewntrz klastra powinny by podobne ze wzgldu na interesujce na kryterium, a obiekty z rónych klastrów powinny si istotnie rónic. Klastrowanie nie jest cakowicie nienadzorowanym procesem. Kiedy dokonujemy klastrowania to musimy wiedzie jak informacje maj nam da klastry. Przykadowo kraje wiata moemy poklastrowa przy pomocy setek zmiennych. Tylko jak informacj dadz nam klastry które s zrobione w oparciu o bardzo róne tematycznie nie zwizane ze sob zmienne? W klastrowaniu musimy sobie wyznaczy, e chcemy poklastrowa kraje ze wzgldu np. na poziom ycia. Albo ze wzgldu na warunki geograficzne. Ten ogólny cel podpowie nam jakie zmienne dobra i uatwi nam interpretowalon klastrów. Warto pamita o przemyleniu standaryzacji zmiennych i ewentyalnie nadaniu im wag. Zmienne do modelu powinny te cechowa si w miar du wariancj (min.10%) Jest problem ze zmiennymi jakociowymi. Wikszo algorytmów ich nie obsuguje. Pytanie czy zwyke przekodowanie na dummies bdzie wystarczajce. Nie bardzo wiadomo jak podej do redukcji wymiarowoci . Moe grozi dua utrata informacji. Tu jest dobry przykad kiedy PCA spowoduje utrat informacji: Generalnie zalecane jest eby spróbowa i zobaczy czy dane si lepiej klastruj po redukcji wymiarowoci. Nie bardzo wiadomo jak zdefiniowa over/under fitting. Co na temat rozwizania tego problemu napisane jest w rozdziale o consensu clustering. W algorytmach jest problem z kltw wymiarowoci. Dlatego przy duej iloci wymiarów potrzebne s bardzo due próby. 4.2.1 4.2.2 Introduction 4.2.3 Hierarchical (Connectivity-based) 4.2.3.1 Agglomerative/Divisive Przykadowy algorytm aglomeracyjny: Na pocztku kady element stanowi oddzielny klaster. Liczymy odlegoci pomidzy wszystkimi klastrami (tworz macierz odlegoci). W pierwszej iteracji kady element jest klastrem. Wzory na liczenie odlegoci midzy klastrami s róne (patrz np Gatner (2009) str. 414). Z macierzy wybieram 2 najblisze klastry i cz je. Procedur powtarzam a uzyskam jeden klaster obejmujcy wszystkie obserwacje. Przykadowy algorytm deglomeracyjny (ksika Gatnara): Dla kadego klastra wyznacz najbardziej odlege obserwacje (na pocztku w pierwszej iteracji mamy tylko jeden klaster). Ten klaster dla którego ta odlego jest najwiksza wybierz do podziau. Dla wybranego klastra dla kadej obserwacji oblicz redni odlego od pozostaych obiektów. Obiekt dla którego uzyskano redni najwiksz odlego stanowi zalek nowego klastra A. Pozostae obiekty staj si tymczasowym klastrem B. Dla kadej obserwacji w klastrze tymczasowym B obliczam redni odlego od pozostaych obiektów z B oraz oddzielnie redni odlego od obiektów z A (na pocztek mamy jeden element w A). Elementy z B które maj redni odlego blisz do A s przerzucane do A. Przerzucanie kontynuujemy a nie bdzie si dao ju przerzuci adnego nastpnego elementu. Cay proces znajdowania nowych klastrów kontynuujemy do momentu, a kady obserwacja stanie si pojedynczym klastrem. 4.2.3.2 CHAMELEON 4.2.3.3 BIRCH Klaster jest reprezentowany w syntetyczny sposób w postaci 3 informacji (tzw. CF): liczebnoci klastra sumy wartoci zmiennych po wszystkich obserwacjach, ale niezalenie po kadym wymiarze. duma kwadratów wartoci zmiennych (tutaj chyba jest jedna liczba bo robimy to na raz po wszystkich zmiennych) Pseudoalgorytm (mój intuicyjny): Mam zbiór obserwacji. Wybieram pierwsz z nich i alokuje j do pierwszego klastra wyliczajc CF. Wybieram kolejn obserwacj. Sprawdzam czy po przypisaniu jej do pierwszego klastra speniony jest warunek e rednia klastra nie moe by wiksza ni zaoona warto d. Jeeli warunek nie jest speniony to tworz nowy klaster. Wybieram kolejn obserwacj. Sprawdzam do którego klastra ma najbliej i znowu sprawdzam warunek rednicy klastrów. Jeeli nie da si obserwacji umieci w adnym klastrze to tworz nowy klaster. Jeeli liczba klastrów jest za dua, to musz je zagregowa. Agreguje si klastry które s blisko siebie. Proces kolejnych agregacji mona przedstawi w postaci drzewa CF (CF tree), co powoduje, e algorytm jest algorytmem hierarchicznym. Na wynikach drzewa (wartociach syntetycznych CF), mona przepuci standardowe algorytmy klastrujce. Pros: Scales linearly: finds a good clustering with a single scan and improves the quality with a few additional scans It has back-tracking capability. It is also an incremental method that does not require the whole data set in advance. Cons: It cannot handle non-numeric data. 4.2.3.4 HDBSCAN 4.2.3.5 ROCK 4.2.3.6 Echidna 4.2.3.7 Diana 4.2.3.8 Agnes 4.2.4 Partititonal 4.2.4.1 k-methods 4.2.4.1.1 k-means Pseudoalgorytm (mój intuicyjny): ustalam ilo centroidów k Zazwyczaj dobrze jest znormalizowa dane. Dane powinni by numeryczne. Wybieram losowo pozycj k centroidów. Obliczam dla kadego elementów odlego do poszczególnych centroidów. Kady element przypisuje do tego centroidu do którego ma najbliej Licz nowe wspórzdne kadego centroidu poprzez urednienie wspórzdnych elementów które s przypisane do danego centroidu. Poniewa mamy nowe wspórzdne cetroidów wracam do puntu 4 i robimy nowe przeliczenia. Obliczenia koczymy jeeli przekroczymy zadan liczb iteracji lub centroidy z kolejnych iteracjach nie zmieniaj pooenia w istotny sposób. Pros: Simple to understand Fast to cluster Widely available (there are several packages that implement K-means) Easy to implement Always yields a result (also a con, as it may be deceiving) Cons: We need to choose the number of clusters (remedy: elbow method) it is sensitive to initialization (remedy: kmeans++) Sensitive to outliers (remedy: remove outliers) Standardization 4.2.4.1.2 k-means ++ Tutaj jest modyfikacja w stosunku do standardowego k-means polegajca na modyfikacji wyboru pocztkowego pooenia centroidów: Choose one center uniformly at random among the data points. For each data point x not chosen yet, compute D(x), the distance between x and the nearest center that has already been chosen. Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to D(x)2. Repeat Steps 2 and 3 until k centers have been chosen. Now that the initial centers have been chosen, proceed using standard k-means clustering. 4.2.4.1.3 k-means ++ scalable Dokadny przykad numeryczny: link Podobnie ja w k_means++ tutaj mamy modyfikacj sposobu wyboru pocztkowego pooenia centroidów. Pseudoalgorym (mój intuicyjny): Ustalamy ilo docelow centroidów k. Wybieram losowo punkt który bdzie pierwszym centroidem. Liczymy odlego wszystkich punktów do tego centroidu. Kademu punktowi przypisujemy odlego do centroidu i w jego oparciu przypisujemy mu prawdopodobiestwo \\(p_x = l\\cdot d^2(x,\\mathcal{C})/\\phi_X(\\mathcal{C})\\) (prawdopodobiestwa dla wszystkich punktów nie musz si sumowa do 1. Wystarczy e prawdopodobiestwo dla kadego punktu bdzie z przedziau [0,1]. Mianownik tego wzoru to suma wszystkich odlegoci. Dla kadego punktu niezalenie losujmy warto z przedziau [0,1]. Wszystkie punkty których prawdopodobiestwo bdzie wiksze od wylosowanych dla nich wartoci traktujemy jako centroidy. Punkt 3 i 4 powtarzamy kilka razy otrzymujc du ilo centroidów. W punkcie 3 oczywicie zakadamy w obliczeniach e tym razem mamy wicej ni jeden centroid. Liczmy wagi dla kadego centroidu. Du wag ma centroid który ma duo punktów dla których jest on najbliej. Z listy centoridów losujemy kolejno k centroidów uwagldniajc wagi. Po wylosowaniu kolejno kadego centroidu usuwamy go ze zbioru, liczymy nowe wagi i dokonujemy kolejnego losowania. W tym momencie mamy list centroidów i moemy wykonywa normaln analiz k-means. Ogólny algorytm: Przykad (uywajc oznacze z powyszego algorytmu): Mamy zbiór danych: data points: (7,1),(3,4),(1,5),(5,8),(1,3),(7,8),(8,2),(5,9),(8,0) l = 2 // oversampling factor k = 3 // no. of desired clusters Step 1: Suppose the first centroid is \\(\\mathcal{C}\\) is \\(\\{c1\\} = \\{(8,0)\\}\\) X={x1,x2,x3,x4,x5,x6,x7,x8}={(7,1),(3,4),(1,5),(5,8),(1,3),(7,8),(8,2),(5,9)} \\(\\phi_X(\\mathcal{C})\\) is the sum of all smallest 2-norm distances (euclidean distance) from all points from the set X to all points from \\(\\mathcal{C}\\). In other words, for each point in X find the distance to the closest point in \\(\\mathcal{C}\\), in the end compute the sum of all those minimal distances, one for each point in X. Denote with \\(d^2_{\\mathcal{C}}(x_i)\\) as the distance from \\(x_i\\) to the closest point in \\(\\mathcal{C}\\). We then have \\(\\psi = \\sum_{i=1}^{n}d^2_{\\mathcal{C}}(x_i)\\) At step 2, \\(\\mathcal{C}\\) contains a single element (see step 1), and X is the set of all elements. Thus in this step the \\(d^2_{\\mathcal{C}}(x_i)\\) is simply the distance between the point in \\(\\mathcal{C}\\) and \\(x_i\\). Thus \\(\\phi = \\sum_{i=1}^{n}{||x_i-c||^2}\\). log(\\psi) = log(52.128) = 3.95 = 4 (rounded) \\(\\psi = \\sum_{i=1}^nd^2(x_i,c_1) = 1.41+6.4+8.6+8.54+7.61+8.06+2+9.4 = 52.128\\) \\(log(\\psi) = log(52.128) = 3.95 = 4 (rounded)\\). Note however that in step 3, the general formula is applied since \\(\\mathcal{C}\\) will contain more than one point. Step 3: The for loop is executed for \\(log(\\psi)\\) previously computed. The drawings are not like you understood. The drawings are independent, which means you will execute a draw for each point in X. So, for each point in X, denoted as \\(x_i\\), compute a probability from \\(p_x = l d^2(x,\\mathcal{C})/\\phi_X(\\mathcal{C})\\) . Here you have ll a factor given as parameter, \\(d^2(x,\\mathcal{C})\\) is the distance to the closest center, and \\(\\phi_X(\\mathcal{C})\\) is explained at step 2. The algorithm is simply: iterate in X to find all \\(x_i\\) for each \\(x_i\\) compute \\(p_{x_i}\\) generate an uniform number in [0,1], if is smaller than \\(p_{x_i}\\) select it to form \\(\\mathcal{C&#39;}\\) after you done all draws include selected points from \\(\\mathcal{C&#39;}\\) into \\(\\mathcal{C}\\) Note that at each step 3 executed in iteration (line 3 of the original algorithm) you expect to select ll points from XX (this is easily shown writing directly the formula for expectation). for(int i=0; i&lt;4; i++) { // compute d2 for each x_i int[] psi = new int[X.size()]; for(int i=0; i&lt;X.size(); i++) { double min = Double.POSITIVE_INFINITY; for(int j=0; j&lt;C.size(); j++) { if(min&gt;d2(x[i],c[j])) min = norm2(x[i],c[j]); } psi[i]=min; } // compute psi double phi_c = 0; for(int i=0; i&lt;X.size(); i++) phi_c += psi[i]; // do the drawings for(int i=0; i&lt;X.size(); i++) { double p_x = l*psi[i]/phi; if(p_x &gt;= Random.nextDouble()) { C.add(x[i]); X.remove(x[i]); } } } // in the end we have C with all centroid candidates return C; Step 4: A simple algorithm for that is to create a vector ww of size equals to the number of elements in \\(\\mathcal{C}\\), and initialize all its values with 0. Now iterate in X (elements not selected in as centroids), and for each \\(x_i \\in X\\), find the index j of the closest centroid (element from \\(\\mathcal{C}\\)) and increment w[j] with 1. In the end you will have the vector \\(w\\) computed properly. double[] w = new double[C.size()]; // by default all are zero for(int i=0; i&lt;X.size(); i++) { double min = norm2(X[i], C[0]); double index = 0; for(int j=1; j&lt;C.size(); j++) { if(min&gt;norm2(X[i],C[j])) { min = norm2(X[i],C[j]); index = j; } } // we found the minimum index, so we increment corresp. weight w[index]++; } Step 5: Considering the weights \\(w\\) computed at the previous step, you follow kmeans++ algorithm to select only \\(k\\) points as starting centroids. Thus, you will execute \\(k\\) for loops, at each loop selecting a single element, drawn randomly with probability for each element being \\(p(i) = w(i)/\\sum_{j=1}^m{w_j}\\). At each step you select one element, and remove it from candidates, also removing its corresponding weight. for(int k=0; k&lt;K; k++) { // select one centroid from candidates, randomly, // weighted by w // see kmeans++ and you first idea (which is wrong for step 3) ... } All the previous steps continues, as in the case of kmeans++, with the normal flow of the clustering algorithm I hope is clearer now. I found also a presentation made by authors, where you can not clearly that at each iteration multiple points might be selected. The presentation is here. It is obvious that \\(log(\\psi)\\) depends on data and the issue you raised would be a real problem if the algorithm would be executed on a single host/machine/computer. However you have to note that this variant of kmeans clustering is dedicated to large problems, and for running on distributed systems. Even more, the authors, in the following paragraphs above the algorithm description state the following: Notice that the size of \\(\\mathcal{C}\\) is significantly smaller than the input size; the reclustering can therefore be done quickly. For instance, in MapReduce, since the number of centers is small they can all be assigned to a single machine and any provable approximation algorithm (such as k-means++) can be used to cluster the points to obtain k centers. A MapReduce implementation of Algorithm 2 is discussed in Section 3.5. While our algorithm is very simple and lends itself to a natural parallel implementation (in \\(log(\\psi)\\) rounds ), the challenging part is to show that it has provable guarantees. Another thing to note is the following note on the same page which states: In practice, our experimental results in Section 5 show that only a few rounds are enough to reach a good solution. Which means you could run the algorithm not for \\(log(\\psi\\))$ times, but for a given constant time. 4.2.4.1.4 c-fuzzy means Szczegóowy opis: link Przykd numeryczny: link Pseudoalgorym (mój intuicyjny): Ustalam ilo klastrów k. Inicjalizuje partitioning matrix. Okrela on stopie przynalenoci kadej obserwacji do kadej klasy. Ma wic ilo kolumn i wierszy proporcjonaln do iloci klastrów i obserwacji. Zainicjowana macierz jest wypeniona zerami i jedynkami. Obliczam wspórzdne centroidów uredniajc zmienne po wszystkich obserwacjach i wac wagami którymi s elementy partition matrix: \\(C_j = \\frac{\\sum\\limits_{x \\in C_j} u_{ij}^m x}{\\sum\\limits_{x \\in C_j} u_{ij}^m}\\) Majc wyliczone centroidy mog policzy odlegoci i w oparciu nowe wagi (stopnie przynalenoci) i tym samym zaktualizowa wartoci partition matrix:ameila \\(u_{ij}^m = \\frac{1}{\\sum\\limits_{l=1}^k \\left( \\frac{| x_i - c_j |}{| x_i - c_k |}\\right)^{\\frac{2}{m-1}}}\\) \\(c_j\\) is the centroid of the cluster j \\(u_{ij}\\) is the degree to which an observation \\(x_i\\) belongs to a cluster \\(c_j\\) Wracam do punktu 3. Wykonuj iteracje a zmiany w partition matrix nie bd istotne. Algorytym minimalizuje wyraenie: \\(\\sum\\limits_{j=1}^k \\sum\\limits_{x_i \\in C_j} u_{ij}^m (x_i - \\mu_j)^2\\) Where, $u_{ij} is the degree to which an observation xixi belongs to a cluster cjcj \\(\\mu_j\\) is the center of the cluster j \\(u_{ij}\\) is the degree to which an observation xixi belongs to a cluster $c_j$ m is the fuzzifier. 4.2.4.1.5 k-medoids Przkad numeryczny: link Medoids are representative objects of a data set or a cluster within a data set whose average dissimilarity to all the objects in the cluster is minimal.[1] Medoids are similar in concept to means or centroids, but medoids are always restricted to be members of the data set Pseudoalgorytm (mój intuicyjny dla Partitioning Around Medoids (PAM) czyli najpopularniejszej implementacji tej metody): Wybieram ilo klastrów k. Losuje k punktów jako medoid. Przypisuje kady punkt do medoidu do którego ma najbliej i otrzymuje klastry. Obliczam funkcj kosztu. Dla kadego medoidu liczymy sum odlegoci obserwacji od medoidu nastpnie wyniki z wszystkich medoidów sumujemy. Nastpnie dla kadego medoida losuje po kolei róne obserwacje i sprawdzam jaka jest nowa warto kosztu. Jeeli koszt spada to obserwacja zastpi analizowany medoidu. Punkt 4 (zamiany medoidów w innymi obserwacjami) do momentu a nie da si nic uzyska. Przykad liczenia kosztu: Poniej mam 10 obserwacji z dwoma zmiennymi (x,y). Obserwacje na czerwono to dwa medoidy. W dwóch ostatnich kolumnach mamy odlegoci obserwacji od medoidów. Punkty 1, 2, 5 id do medoidu C1 a 0, 3, 6, 7, 8 do medoidu C2. Koszt w tym wypadku wynosi: koszt=(3 + 4 + 4) + (3 + 1 + 1 + 2 + 2) = 20 Select k of the n data points as the medoids to minimize the cost Associate each data point to the closest medoid. (SWAP) While the cost of the configuration decreases: For each medoid m, and for each non-medoid data point o: Consider the swap of m and o, and compute the cost change If the cost change is the current best, remember this m and o combination Perform the best swap of \\(m_{best}\\) and \\(o_{best}\\) , if it decreases the cost function. Otherwise, the algorithm terminates. Pros: It is simple to understand and easy to implement. PAM is less sensitive to outliers than other partitioning algorithms. It works efficiently for small data sets. Cons: The main disadvantage of K-Medoid algorithms is that it is not suitable for clustering non-spherical (arbitrary shaped) groups of objects. This is because it relies on minimizing the distances between the non-medoid objects and the medoid (the cluster centre)  briefly, it uses compactness as clustering criteria instead of connectivity. It may obtain different results for different runs on the same dataset because the first k medoids are chosen randomly. It does not scale well for large data sets. Its processing is more expensive than k-means method 4.2.4.1.6 k-medians Jest to algorytm k-means ale przy obliczania wspórzdnych centroidów uywamy mediany a nie redniej. Dodatkowo jako metryka odlegoci jest uywany metryka miejska a nie euklidesowa. 4.2.4.1.7 k-modes Jest to k-means dla danych jakociowych. Do liczenia centroidów stosuje si tutaj dominanty, a do pomiaru odlegoci midzy obserwacjami s inne miary (doczyta jakie. W jednym artykule wymieniono np. cosinus, ale tutaj poniej jest przykad bardzo prostej miary). Przykad numeryczny: link Obliczanie odlegoci miedzy obserwacjami. Obliczanie centroidu poprzez dominanty: Input: Data objects X, Number of clusters K. Step 1: Randomly select the K initial modes from the data objects such that Cj, j = 1,2,,K Step 2: Find the matching dissimilarity between the each K initial cluster modes and each data objects. Step 3: Evaluate the fitness using the Eq.(1) Step 4: Find the minimum mode values in each data object i.e. finding the objects nearest to the initial cluster modes. Step 5: Assign the data objects to the nearest cluster centroid modes. Step 6: Update the modes by apply the frequency-based method on newly formed clusters. Step 7: Recalculate the similarity between the data objects and the updated modes. Step 8: Repeat the step 4 and step 5 until no changes in the cluster ship of data objects. Output: Clustered data objects 4.2.4.1.8 k-prototypes Przykad z Youtuba jak uyw w Pythonie: AIEngineering Jest to mix metody k-means i k-modes. Mona j zatem stosowa do zmiennych na mieszanych skalach (ilociowe i jakociowe). Oddzielnie Liczy si odlego ze wzgldu na zmienne ilociowe (metryka euklidesa) i oddzielnie ze wzgldu na jakociowe. Nastpnie obie odlegoci si agreguje. Przykad obliczania odlegoci (mamy trzy centroidy i punkt x ): Example 1. We assume that k = 3. Each current cluster center is as follows: C1=(A, A, A, 5), C2=(B, B, B, 7), and C3=(B, B, B, 8). As shown by Figure, we have to compute the distance between Xi=(B, B,B,4) and each of cluster centers (C1, C2, and C3) for assigning Xi to the cluster of the closest center. Firstly, we compute the distance of numerical attributes, dr(Xi,Cj) is 1, 9, and 16, respectively. Xi is the closest to C1 only with numerical attributes. In this example, objects consisted of three categorical attributes; the minimum value of possible distance is 0, and the maximum value is 3. The difference of numerical distance between dr(Xi,C1) and dr(Xi,C2) is 8. Thus, Xi continues to be the closest to C1, even if dc(Xi,C1) is calculated by 3 as the computable max difference value. Wzór na centroidy: D(x,p) = E(x,p) + C(x,p) Where, x = Any datapoint, y = Prototype of a cluster, D(x,p) = Dissimilarity measure between x and y, E(x,p) = Euclidean distance between continuous attributes of x and y, C(x,p) = number of mismatched categorical attributes between x and y, = weightage for categorical variable value. 4.2.4.1.9 k-means SGD (stochastic gradient descent) Tutaj aktualizacja pozycji centroidów opiera si o gradient minimalizowanej funkcji. 4.2.4.1.10 k-mini-batches Tutaj za kadym razem kiedy zaktualizujemy pozycj centroidów kolejne przeliczenia odlegoci robimy na kolejno losowanej podpróbce. 4.2.4.2 CLARANAS 4.2.4.3 CLARA Klasa to metoda k-medoidów z dodanym próbkowaniem: Create randomly, from the original dataset, multiple subsets with fixed size (sampsize) Compute PAM algorithm on each subset and choose the corresponding k representative objects (medoids). Assign each observation of the entire data set to the closest medoid. Calculate the mean (or the sum) of the dissimilarities of the observations to their closest medoid. This is used as a measure of the goodness of the clustering. Retain the sub-dataset for which the mean (or sum) is minimal. A further analysis is carried out on the final partition. 4.2.4.4 FCM 4.2.4.5 FCMdC 4.2.4.6 Fanny 4.2.5 Density /Latent distibutions Algorytm ten wystpuje te w wersji dla danych jakociowych i w wersji pónadzorowanej. Jest oparty o mieszanki gaussowskie ((gaussian mixtures)) (link) Expectation Maximisation algorithm: Mamy jednowymiarow próbk danych: Zakadamy e s tutaj ukryte 2 rozkady normalne. Losowy wybieramy 2 miejsca na osi i traktujemy je jako warto oczekiwan rozkadów. Wariancja rozkadów jest ustalona losowo. (czasem dla inicjalizacji stosuje si najpierw klastrowanie metod k-means. Na otrzymanych klasach liczy si pocztkowe parametry rozkadów) Dla kadej obserwacji wyliczam jakie jest prawdopodobiestwo jej wylosowania z kadego z rozkadów (x|b). Dodatkowo wyliczam prawdopodobiestwo warunkowe okrelajce jakie jest prawdopodobiestwo (b|x) e obserwacja pochodzi z okrelonego rozkadu. Nastpnie wyliczam now redni i wariancj dla kadego z rozkadów. Ale tutaj obserwacje s waone wagami. Wagi to dla kadej obserwacji prawdopodobiestwo e pochodzi z danego rozkadu (wedug udacity jest chyba dadatkowo zakadane, e dla kadego rozkadu bior tylko obserwacje które powinny by zaklasyfikowane do tego rozkadu , czyli maja najwiksze prawdopodobienstwo przynalezenia do danego rozkladu). Mam teraz nowy rozkad. Poprzednie kroki powtarzam do momentu a nie bdziemy zauwaali wikszych zmian w kolejnych iteracjach pod ktem parametrów rozkadu (u i sigma). Przykady praktycznego uycia: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&amp;rep=rep1&amp;type=pdf https://arxiv.org/abs/1205.6221 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.338&amp;rep=rep1&amp;type=pdf http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf https://www.youtube.com/watch?v=lLt9H6RFO6A 4.2.5.1 DBSCAN W algorytmie tym nie ustalamy z góry iloci klastrów. Pseudoalgorytm (mój intuicyjny): Ustalam parametry: \\(\\epsilon\\) okrelajcy odlego od punktów jaka bdzie analizowana (specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (eps), these points are considered neighbors). minPoints (the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region). Wybieram JEDEN losowy punkt. Sprawdzam czy w otoczeniu o szerokoci \\(\\epsilon\\) jest wystarczajce ilo innych obserwacji (minPoints). Jeeli tak to punkt ten traktuje jako core point. Dla mojego core point dla punktów które s w jego otoczeniu sprawdzam warunek bycia core point. I id tak iteracyjnie a skoczy si moliwo rozszerzania mojej bazy core points . Jeeli jaki punkt znajdzie si w otoczeniu jakiego core point a sam nie nie jest to jest to punkt brzegowy. Po wyczerpaniu przeszukiwania nasze wszystkie poaczone core points i punkty brzegowe traktujemy jako odwiedzone. Nie bd one bay udziay w dalszej analizie. Teraz losujemy kolejny punkt sporód punktów nieodwiedzonych i potarzamy ca procedur. Robimy to do wyczerpania wszystkich punktów. W trakcie analizy niektóre obserwacje nie zaapi si ani jako brzegowa ani jako core points. Bd one zaklasyfikowane jako szum. Na rusunku poniej niebieska obserwacja N to noise (szum), óte to punkty brzegowe, a czerwone to punkty corowe. Pros: a) It can handle noise efficiently. b) It can handle clusters of different shapes and sizes. c) It is faster. d) There is no need to define number of clusters in advance. Cons: a) Varying densities b) It is not suitable for High-dimensional data c) It has some difficulties in distinguishing separated clusters if they are located too close to 4.2.5.2 OPTICS Optics to rozwinicie DBSCAN. Umoliwia radzenie sobie z sytuacj, kiedy róne obszary danych maj rón gsto. Wtedy ustalony na sztywno parmetr \\(\\epsilon\\) nie jest dobrym rozwizaniem. Ustalamy maxEpsilon i tak jak w poprzednim algorytmie minPoints. Nastpnie dla obserwacji badamy jaki jest najmniejszy \\(\\epsilon\\) który jest obejmuje ilo punktów okrelonych przez minPoints. Jeeli jest on mniejszy od maxEpsilon to punkt traktujemy jako corowy. Dziki temu e nasz \\(\\epsilon\\) moe si zmienia moemy pracowa z rónymi gstociami punktów. Kiedy mamy nasz corowy punkt to badamy pod tym wzgldem kolejne punkty znajdujce si z jego otoczeniu \\(\\epsilon\\). Ale badany punkty w kolejnoci od najbliszego do najdalszego (nie bardzo rozumiem dlaczego to ma znaczenie, ale to jest chyba tylko kwestia optymalizacji w punktu widzenia uwagi któr dalej pisz ). Uwaga: Dodatkowo chyba jest tak e eby dwa core points byy poczone bezporednio to musz si znajdowa nawzajem w swoich \\(\\epsilon\\). Poniewa tutaj w przeciwiestwie do DBSCAN te wartoci mog si dla dwóch punktów róni nie jest to trywialna rzecz. 4.2.5.3 PreDeCon 4.2.5.4 SUBCLU 4.2.5.5 DENCLUE 4.2.5.6 DBCLASD 4.2.5.7 Graph based clustering 4.2.5.7.1 Spectral clustering Dosty matematyczny opis : link Filmik: Spectral Partitioning, Part 1 The Graph Laplacian Filmik - pierwszy z serii ze Stanfordu : Lecture 29 Najpierw ze zbioru danych tworz nieskierowany graf powizania obserwacji (similarity graph). Tutaj s popularne 3 podejcia: \\(\\epsilon\\) neighborhood graph: czymy punktu które s w odlegoci mniejszej ni zadany \\(\\epsilon\\). k-nn graph: cze dany punkt z innym jeeli jest wród jego k najbliszych ssiadów. Uwaga: nie ma tutaj zawsze przechodnioci. A moe nalee do k najbliszych ssiadów B, ale niekoniecznie musi tak by odwrotnie. Wtedy chyba wystarczy e jest poczenie w jedn stron eby uzna punkty za poczone. fully connected graph : podobiestwo midzy punktami czymy przez GAussowsk funkcj podobietwa z ustalanym parametrm \\(\\sigma\\) . Nastpnie w oparciu o graf tworzymy 3 macierze: Adjacency matrix (A). Elementy macierzy okrelaj czy dwa punty s ze sob poczone. Jeeli mamy graf niewaony to jest to okrelenie binarne (jak w przykadzie poniej). W grafie waonym mamy wagi okrelajce odlegoci midzy punktami. Degree matrix (D): Jest to macierz diagonalna gdzie dla kadego punktu zlicza si ile ma pocze z innymi punktami. Laplacia matrix (L) : L = D-A Z Lapacian matrix wyliczamy wartoci wasne i wektory wasne suce do analizy spektralnej. Ustalam ilo klastrów poprzez maksypalizacj eigengap. Eigengap to rónica pomidzy kolejnymi ssiadujcymi eigenvalues (s posortowane rosnco). Tam gdzie rónica jest najwiksza mamy najwiksz eigengap. Jeeli maksymalny eigengap jest miedzy 3 i 4 eigenvalue to ilo klastrów k = 3 Tworz macierz dla wektorów wasnych odpowiadajcych k najmniejszym niezerowym wartoci wasny. Wiersze tej macierzy grupuj algorytmem (np. k-means). Z Laplacian matrix s zwizane nastpujce pojcia: The collection of the eigenvalues of matrix L is called the spectrum of L. Spectral Gap: The first non-zero eigenvalue is called the Spectral Gap. The Spectral Gap gives us some notion of the density of the graph. Fiedler Value: The second eigenvalue is called the Fiedler Value, and the corresponding vector is the Fiedler vector. Each value in the Fiedler vector gives us information as to which side of the decision boundary a particular node belongs to. Intuicja dziaania algorytmu. Jeeli wemiemy drug warto wasn to si okazuje e jej wyliczenie jest zwizane z wyliczeniem wektora wasnego w taki sposób e minimalizuje on pewne wyraenie. Jeeli zaoymy e Laplacian jest binarny (graf niewaony), to pomnoenie Laplacian-u przez werktor wasny tak naprawd sumuje iloczyny swoich wartoci i 0 oraz jedynek (gdzie jedynka oznacza e 2 punkty s poczone). Wartoci wektora wasnego moemy traktowa jako swego rodzaju lebele przypisane obserwacj. Zagadnienie minimalizacji jest zwizane eby zminimalizowa sum rónic pomidzy wartociami wektorów wasnych, ale dla przypadków gdzie elementy s poczone. Dodatkowo suma elementów wektora wasnego musi wynosi 0. Okazuje si e jeeli przypisz elementom grupie elementów poczonych podobne wartoci to w wyniku sumowanie rónic wszystko mi si kasuje i minimalizuje warto wyraenia. Elementy s powizane wic jest duo odejmowa i odejmuje podobne wartoci wic to si kasuje. Jeeli sabo powizanym obserwacj przepisz podobne wartoci to to si nie bdzie kasowao. Bed mia mao odejmowa dla elementów o podobnych wartociach a duo dla elementów o rónych wartociach (gdzie rónice nie bd ju zerowe) Tak wic grypy mocno powizanych wartoci bd miay labele podobnej wartoci. W tej sposób wektor wasny (bdcy tymi labelami) wskae nam klastrowanie elementów. Pros: Elegant, and well-founded mathematically Works quite well when relations are approximately transitive (like similarity) Excellent quality under many different data forms Cons: Very noisy datasets cause problems Informative eigenvectors need not be in top few Performance can drop suddenly from good to terrible Expensive for very large datasets Computing eigenvectors is the bottleneck Much slower than KMeans Warning: Transforming distance to well-behaved similarities Note that if the values of your similarity matrix are not well distributed, e.g. with negative values or with a distance matrix rather than a similarity, the spectral problem will be singular and the problem not solvable. In which case it is advised to apply a transformation to the entries of the matrix. For instance, in the case of a signed distance matrix, is common to apply a heat kernel: similarity = np.exp(-beta * distance / distance.std()) 4.2.5.8 Mean Shift W tej metodzie nie jest z góry okrelone ile dostaniemy klastrów. Ilo klastrów jest porednio zalena od rednicy tzw. okna którym bdziemy si porusza po obserwacjach. Poniej mamy przykad. Na pocztku zdefiniowalimy losowo punkt startowy (niebieski krzyyk) i analizowany obszar (niebieskie kóko). Dla obserwacji które znajduj si w tym obszarze liczymy center of mass (punkt cikoci zbioru). Nastpnie cay obszar przesuwamy tak aby ten punkt by nowym rodkiem obszaru. Powtarzamy to iteracyjnie, a pozycja center of mass si ustabilizuje. Takich punktów startowych i obszarów wybieramy losowo kilka. Jeeli bd zaczynay dziaa w rónych pozycjach to wikszo z nich powinno znale swoje stabilne center of mass w rónych miejscach. Jeeli róne obszary zbiegy si do podobnych miejsc (czyli dostaniemy swego rodzaju duplikaty), to algorytm jest usunie. Po usuniciu duplikatów te obszary s miejscami poszczególnych klastrów. Szeroko obszarów wpynie na ilo klas. Due obszary to mniej klas. Mae to wicej klas. Chyba moe wystpi te taka moliwo e obszary nie pokryj jakich obserwacji. Nie wiem czy algorytm rozwizuje jako ten problem (moe alokacja to najbliszego klastra? ). Nie wiem te co w sytuacji kiedy obszary bd si czciowo pokryway (moe te zasada e przypisanie jest do bliszego centrum klastra). Pros: Mean shift cleverly exploits the density of the points in an attempt to generate a reasonable number of clusters Cons: The most glaring disadvantage is its slowness. More specifically, it is an N squared algorithm. 4.2.5.9 Substractive Methods 4.2.6 Grids 4.2.6.1 STING Przestrze dzieli si hierarchicznie na coraz mniejsze jednostki: Dla najmniejszych jednostek na dole wyliczam róne miary (rednia, mediana, odchylenie itp.). Potem je agreguje w gór. Dziki temu mam w hierarchiczny sposób uporzdkowane informacje o zbiorach na rónych poziomach mojego hierarchicznego grida. Te informacje pozwalaj mi selekcjonowa units grid-u ze wzgldu na klastry. Niestety nie wiem jak to dokadnie dziaa. Wiem tylko e poruszamy si z góry w dó do coraz mniejszych gidów. Pros: a) Query-independent, easy to parallelize, incremental update O(K), where K is the number of grid cells at the lowest level . b) It facilitates parallel processing and incremental updating. c) It is efficient. Cons: a) The quality of STING clustering depends on the granularity of the lowest level of the grid structure. b) It results into unaccurate clusters despite the fast processing time of the technique 4.2.6.2 CLIQUE Najpierw tworzymy wielowymiarow siatk w której kady hipercian to unit. Potem dla kadego wymiaru z osobna badamy rozkad obserwacji (na powyszym rysunku wykresy gstoci na obrzeach). Nastpnie w przestrzeni 2 D badamy paramy miary i wykrywamy obszary gdzie jest dua gsto na obu wymiarach (kolorowe kwadraty). Potem kontynuujemy w wyszych wymiarach (z 2D przechodzimy do 3D). Kiedy mamy wytypowane obszary i duej gstoci sprawdzamy które z nich ssiaduj ze sob. Takie ssiadujce skupiska bd kandydatami na klastry. Minimalne ilo przylegajcych do siebie units o duej gstoci to bd klastry. Pros: a) CLIQUE automatically finds subspaces of the highest dimensionality such that high-density clusters exist in those subspaces. b) It is insensitive to the order of input. c) It is scalable. Cons: a) Need to tune grid size and density threshold b) May fail if clusters are of widely differing densities, since the threshold is fixed c) Can still have high mining cost d) Same density threshold for low and high dimensionality 4.2.6.3 WaveCluster Jest oparte o falkow tranformacj danych. Róne transformacje pozwalaj na róne zmniejszanie rozdzielno. Przestrze dla tak przeksztaconych danych si griduje i szuka unitis o wikszej gstoci. Przylegajce units o duej gstoci s klastrami. Pros: a) It finds clusters for very large spatial databases. b) It provides unsupervised clustering. c) Wavelet transformation can automatically result in the removal of outliers. d) It can help detect clusters at varying levels of accuracy. e) Wavelet-based clustering is very fast. f) High-quality clusters g) The ability to work well in relatively high-dimensional spatial data Cons: 4.2.6.4 OptiGrid 4.2.7 Model Based 4.2.7.1 CLASSIT 4.2.7.2 SOMs 4.2.7.3 COBWEB 4.2.7.4 Neural Networks 4.2.8 Other 4.2.8.1 Affinity propagation Metoda jest dostpna w pakiecie sklearn. Numeryczny przykad: link Ciekawy opis (affinity propagation jest tutaj opisany jako drugi po k-means algorytm) : link W tej metodzie szukamy tak jak przy metodzie medodiów ze zbioru danych najlepszego reprezentanta dla klastrów. Robi si to poprzez passing messages pomidzy obserwacjami. Tutaj warto zwróci uwag na 2 rodzaje wartoci: Calculating responsibilities (odpowiada im macierz responsibility matrix): Responsibility r(i, k) reflects the accumulated evidence for how well-suited point k is to serve as the exemplar for point i, taking into account other potential exemplars for point i. Responsibility is sent from data point i to candidate exemplar point k. Calculating availabilities (odpowiada im macierz availibility matrix): Availability a(i, k) reflects the accumulated evidence for how appropriate it would be for point i to choose point k as its exemplar, taking into account the support from other points that point k should be an exemplar. Availability is sent from candidate exemplar point k to point i. Mamy poniszy zbiór danych. Tworz similarity matrix. S to sumy kwadratów rónic wartoci kolejnych zmiennych dla dwóch wybranych obserwacji. Warto ta jest mnoona przez -1. Przykadowo odlego dla pary Bob i Edna: \\(-1*((4-1)^2 + (3-1)^2 + (5-3)^2 + (1-2)^2 + (1-3)^1) = -22\\) Nastpnie w oparciu o similarity matrix wyliczam responsibility matrix. Przykad: Wybieram par obserwacji Dough i Cary. Dla wiersza Cary wybieram warto maksymaln (-6. W liczeniu max nie uwzgldnia kolumny Dough = -18). Nastpnie j odejmuje od wartoci w kolumnie odpowiadajcej Dough. -18-(-6) =-12 Tutaj sprawdzam na ile Doug jest dobrym kandydatem eby by dla Cary exemplar-em . Najbliszym punktem jest Alice (-6), wic Doug jest o -12 gorszy od niej. Poniej w peni przeliczona responsibility matrix. Teraz obliczam availibility matrix. Najpierw przeliczam elementy przektne. Sumuje po kolei elementy kolumny które s wiksze ni 0 (ujemne zastpuje zerem). Nie bior w przeliczaniach pod uwagi elementu na przektnej (-16) Przykad przeliczenia elementu przektnego dla Cary: 1+0+0+0 = 1 Nastpnie przeliczam elementy poza przektn. Przykad dla pary Bob i Dough. Bior kolumn Bob. Do elementu przektnego (-15) dodaje kolejne dodatnie elementy (ujemne zastpuje zerem) kolumny pomijajc wiesza dla Dough (-14). Jeeli otrzymana warto jest dodatnia to zastpuje j zerem. Przeliczona availability matrix. Obliczam criterion matrix jaku sum macierz availability i responsibility. Uwaga: Mamy tuta iteracyjno. Wydaje mi si e responsobility matrix jest przeliczana do momentu, a liczona po niej availibility matrix nie ustabilizuje si. Z powyszej macierzy wybieram wartoci maksymalne dla kadego wiersza. Zrónicowanie tych wartoci jest podstaw do klastrowania. Powyej uformoway si nam naturalnie 2 klastry dla wartoci 5 i -5 (wyboldowane w tabeli powyej). Alice i Dough bdziemy traktowali jako reprezentantów klastrów. 4.2.8.2 Random Foreset In unsupervised learning the data consist of a set of x -vectors of the same dimension with no class labels or response variables. There is no figure of merit to optimize, leaving the field open to ambiguous conclusions. The usual goal is to cluster the data - to see if it falls into different piles, each of which can be assigned some meaning. Utworzenie syntetycznego zbioru danych. The approach in random forests is to consider the original data as class 1 and to create a synthetic second class of the same size that will be labeled as class 2. The synthetic second class is created by sampling at random from the univariate distributions of the original data. Here is how a single member of class two is created - the first coordinate is sampled from the N values {x(1,n)}. The second coordinate is sampled independently from the N values {x(2,n)}, and so forth. Thus, class two has the distribution of independent random variables, each one having the same univariate distribution as the corresponding variable in the original data. Class 2 thus destroys the dependency structure in the original data. But now, there are two classes and this artificial two-class problem can be run through random forests. This allows all of the random forests options to be applied to the original unlabeled data set. If the oob misclassification rate in the two-class problem is, say, 40% or more, it implies that the x -variables look too much like independent variables to random forests. The dependencies do not have a large role and not much discrimination is taking place. If the misclassification rate is lower, then the dependencies are playing an important role. Formulating it as a two class problem has a number of payoffs. Missing values can be replaced effectively. Outliers can be found. Variable importance can be measured. Scaling can be performed (in this case, if the original data had labels, the unsupervised scaling often retains the structure of the original scaling). But the most important payoff is the possibility of clustering. Budowa modelu. Model klasyfikuj czy obserwacja jest ze zbioru pierwotnego czy syntetycznego. Proximity matrix. The key output we want is the proximity (or similarity/dissimilarity) matrix. This is an n \\times n matrix where each value is the proportion of times observation i and j where in the same terminal node. For example, if 100 trees were fit and the ij^{th} entry is 0.9, it means 90 times out of 100 observation i and j where in the same terminal node. Uycie innych algorytmow klastrujacych bazujcych na poximity matrix. With this matrix we can then perform a normal clustering procedure such as kmeans or PAM (number of cool things could be done once the proximity matrix is created) 4.2.9 Consensus Jest wiele podej do tego problemu. 4.2.9.1 Monti Consensus Tutaj jest algorytm który ma po czci rozwiza problem overfittingu: Losuje k podpróbek ze zbioru. Na kadej podpróbce buduje algorytm (Tutaj chyba musimy zakada e dla kadej próbki algorytm zwróci tak sam ilo klastrów. Dodatkowo zakadam e dla kadej próbki jest ten sam rodzaj algorytmu) Buduj macierz M(i,j) gdzie element o wspórzdnych i,j pokazuje w ilu procentach k sytuacji byo tak, e obserwacja i i obserwacja j byy w tej samej kasie. Powysza macierz informuje mnie o stabilnoci klastrowania Jeeli klastrowanie jest dosy stabilne, to macierz mog zamieni na macierz odlegoci (D (i,j) = 1- M(i,j) ) i zrobi na niej klastrowanie metod opart o macierze odlegoci (np. klastrowanie hierarchiczne). Powysza metoda moe by pomocna w ustalaniu iloci klastrów. Wybieramy tak ilo klastrów dla której macierz pokazuje najwiksz stabilno. Problem jest w tym e przy duej iloci obserwacji macierze s due i ciko okreli na oko co jest lepsze. Wtedy chyba najlepiej wzi wszystko powyej przektnej (macierz jest symetryczna) i wrzuci wszystkie wartoci na wykres gstoci. Wtedy porównujc róne wykresy gstoci zobaczymy gdzie jest stabilniej (jest duo wartoci bliskich 0 i 1). Drobna uwaga to powyszego. Jak sprawdzi które klastry s tymi samymi z dwóch rónych próbek. Mozna to chyba zrobi poprzez maximum jaccard similarity ( given two vectors, the jaccard similarity is the intersect / union). Sa te inne podobne algorytmy. Moemy np. boostrapowa próbk (w r pakiet ). Dla kadej próbki robi klastrowanie. Miar jaccard szukam najbliszych klastrów pomidzy kolejnymi próbkami. Potem sprawdzam jak stabilnie zachowuj si odlegoci pomidzy tymi najbliszymi klastrami. 4.2.9.2 Rand Index Jest to indeks pokazujcy czy dwa oddzielnie wykonane na tym samym zbiorze klastrowania (np. rónymi algorytmami) s do siebie podobne. Mamy dwa klastrowania (P i Q) i N elementów. Budujemy w oparciu o nie 4 zbiory.  A: pairs that are together in P, Q  B: together in P, not in Q  C: together in Q, not in P  D: not together in P, not together in Q  Rand index: r=(A+D)/(A+B+C+D), 0  r  1 A + D mona okreli jako sytuacje gdzie algorytmy s zgodne. B + D to sytuacje gdzie algorytmy si wykluczaj. Rand index wystpuje te w wersji skorygowanej. 4.2.10 By problems 4.2.10.1 Numerical and categorical 4.2.10.2 Text data 4.2.10.2.1 Latent Dirichlet allocation Linki: Jest to algorytm klastrujcy do danych tekstowych. Zaómy e mamy teksty o rónej tematyce. Zakadamy e mona je podzieli na k klas. Algorytm za pomoc rozkadu sów postara si tam poklastrowa teksty na te k klas. Algorytm zwraca prawdopodobiestwa przynalenoci do klasy wic jest to mikka separacja. Ogólna idea: Zaómy e memy 2 rozkady Dirichleta. Pierwszy dotyczy rozkadu dokumentów wzgldem tematów. Kademu punktowi dokumentowi (reprezentowanego przez pomaraczowy punkt) jest przypisany rozkad wielomianowy, który wynika ze wspórzdnych tego punktu (tabelki po prawej). Sam rozkad prawdopodobiestwa znalezienie si dokumentu w okrelonym miejscu trójkta modeluje rozkad Dirichleta: Drugi rozkad dotyczy rozkadu sów wzgldem tematów.: Znowu kademu punktowi (tym razem tematowi) jest przypisany rozkad wielomianowy sów znowu wynikajcy ze wspórzdnych tych punktów (tabelki po lewej stronie), ale sam rozkad punktów jest opisany znowu przez rozkad Dirichleta. Mamy zatem 2 rozkady. Chcielibymy aby te rozkady byy dobrze dopasowane do rozkadów sów/tematów w próbce dokumentów któr badamy. Wtedy na podstawie sów dokumentu moglibymy osign nasz cel czyli, okreli temat dokumentu. Jednak po pierwsze nie znamy iloci tematów. To jest parametr który musimy ustali apriori. Po drugie jak okreli co w takiej sytuacji oznacza dobrze dopasowanie rozkadów skoro to nie jest uczenie nadzorowane. Nie znamy rozkadów wielomianowych w tabelkach z rysunków powyej. Pomys jest aby stworzy generator dokumentów w oparciu o rozkady. Rozkady bdziemy stroi tak eby maszynka generowaa dokumenty jak najblisze oryginalnym dokumentom z próbki. Porównujemy tutaj po prostu czstotliwo sów (nie badamy tematów bo nie mamy przez etykiet z jakim dokumentem jest powizane sowo lub dokument). Przepis na maszynk w 4 etapach: etap 1 etap 2 etap 3 etap 4 Najpierw z pierwszego rozkadu losujemy punkt w rozkadzie dokumentów. Dla wylosowanego ótego punktu (dokumentu) mamy przypisany rozkad tematów. Nastpnie z drugiego rozkadu losujemy tematy i przypisane im rozkady sów. Teraz z wylosowanego w pierwszym punkcie rozkadu tematów który jest przypisany dokumentowi, dokonujemy losowania tematów. Potem dla kadego wylosowanego tematu losujemy sowa. Jeeli temat jest niebieski (science) to losujemy z puli sów przypisanych do tego tematu itd. Ostateczna lista sów to jest nasz wygenerowany dokument. Ilo wylosowanych sów jest modelowana przez rozkad Poissona (zakadamy e dokumenty róni si iloci sów). Teraz mamy maszynk do generowania sów i moemy sprawdzi czy jest ona dobrze skalibrowana ( co si sprowadza do tego czy rozkady Dirichleta s dobrze wyestymowane. Rozkady wielomianowe s tylko wspórzdnymi punktów i nie ma tu nic do estymowania) poprzez porównanie dokumentów wygenerowanych i tych z próbki. Jak wyestymowa rozkady Dirichleta? Uywa si do tego próbkowania Gibbsa: Losowo przypisuje sowa do tematów (tematy maj kolory wic losowo przypisuje kolory). Chcemy je przekolorowywa sowa tak aby zbiory sów i dokumenty stay si jak najbardziej monochromatyczne, czyli np. uzyska kocowo co takiego: Zaómy zatem e mamy losowe kolorowanie (rysunek poniej). W celu rozpoczcia przekolorowywania losujemy sowo. Niech to bdzie pierwsze sowo z pierwszego dokumentu (ball). Chcemy je przekolorowa. Przeprowadmy analiz. Na rysunku poniej dokumentów mamy 2 wiersze : (1) How much is Topic t in Doc 1 i (2) How much is ball in Topic t. Odpowiada to dwóm estymowanym rozkadom Dirichleta których potrzebujemy. Pierwszy wiersz podpowiada nam, e na pewno nie powinnimy kolorowa ball na zielono, bo w dokumencie nie ma tam takiego koloru (0). Jest remi midzy niebieskimi i czerwonym (2 do 2). Zatem zobaczmy po cay dokumencie jak s pokolorowane tylko sowa ball. Mamy 3 czerwone, 1 zielony i 0 niebieskich. Teraz zróbmy iloczyny wartoci w pierwszego i drugiego wiersza: 2*0=0 (niebieskie) 0+1=1 (zielone) 2*3 =6 (czerwone) Wygrywa czerwone. Jednak eby spowolni tempo zbienoci algorytmu wprowadzamy warunki agodzce. Po pierwsze dodajemy parametry alfa i beta (Parametry rozkadu Dirichleta), dziki czemu dla wszystkich kolorów dostaniemy zawsze warto wiksz ni 0. Ostatecznie losowanie koloru do przypisania jest losowane z rozkadów gdzie prawdopodobiestwo wylosowania koloru jest proporcjonalne do liczb otrzymanych liczeniu kolorów i proporcjonalnie do parametrów alfa i beta: Parametry alfa i beta te trzeba dobra. 4.2.10.3 Sound 4.2.10.4 Vision Dostpno metod typu external (zakadaj e mimo ze robimy klastrowanie to dane miay labels) w Sklearn: Dostpno metod typu internal (zakadaj e dane nie byo olabelowane) w Sklearn: 4.2.11 Results diagnostics Poniej s róne miary jakoci klastrowania (zakadaj e nie znamy ground truth): 4.2.11.1 BIC/AIC Jeeli metoda której uywamy jest oparta o okrelone rozkady statystyczne (np. mieszanki rozkadów) to moemy obliczy funkcj wiarygodnoci. A to umoliwia nam policzenie kryteriów BIC i AIC. Poka one jako cznego dopasowania rozkadów do danych, jednoczenie karajc za komplikacje modelu (dua ilo klastrów) . 4.2.11.2 Metoda okcia (elbow). Stosowana jest dla rónych miast. Najczciej dla SSE i tutaj podamy taki przykad. Obliczamy SSE (sum of squared error). Jednak poniewa nie mamy tutaj uczenia nadzorowanego to error przyjmuje tutaj inne znaczenie. SSE is defined as the sum of the squared distance between centroid and each member of the cluster. Im wicej klastrów tym SSE bdzie oczywicie spadao. Najmniejsz warto bdzie miao kiedy kada obserwacja jest oddzielnym klastrem. Sztuk jest wybra ma ilo klastrów gdzie jest stosunkowe mae SSE. Na poniszym przykadzie wybieram 6 klastrów. Zwikszanie iloci klastrów powyej 6, nie daje mi duego uzysku na zmniejszeniu SSE. Lista miar ze szczegóowym opisem dostepnych w sklearn link 4.2.11.3 silhouette: link Miara ta przyjmuje wartoci z przedziau [-1.1]. Miara ta mierzy jak punkt z danego klastra jest oddalony od innego ssiedniego klastra. Im dalej tym lepiej. Poniej mamy 2 klastry (po prawej stronie jest poklastrowany zbiór danych). Dla kadej obserwacji w ramach jej klastra jest wyliczana miara silhouette: Obserwacje w klastrach sortuje rosnco. Potem nakadam na obrócony o 90 stoni wykres supowy (wykres po lewej). Dodatkowo wyliczam redni warto silhouette dla wszystkich obserwacji i nanosz j jako czerwon przerywan lini. Tak warto urednion mona policzy dla rónych klastrowa i w oparciu o ni wybra najlepsze klastrowanie. Advantages The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster. Drawbacks The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. High computational complexity: O(n²) 4.2.11.4 Celinski Harabasz indeks Okrela jako klastrowania w oparciu o odlegoci zwizane z centroidami: W liczniku mamy miar okrelajc redni odlego kwadratów centroidów klastrów od globalnego centroidu (chcemy maksymalizowa) W mianowniku mamy miar uredniajc rednie odlegoci punktów od swoich centroidów (chcemy minimalizowa) Im wiksza warto indeksu tym lepiej. Advantages The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster. The score is fast to compute. Drawbacks The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN. 4.2.11.5 Davies-Bouldin Index This index signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. A lower Davies-Bouldin index relates to a model with better separation between the clusters. Advantages The computation of Davies-Bouldin is simpler than that of Silhouette scores. The index is computed only quantities and features inherent to the dataset. Drawbacks The usage of centroid distance limits the distance metric to Euclidean space. The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN. 4.2.12 Elements selection 4.2.13 IML 4.2.14 By problems 4.3 Association 4.3.1 Apriori 4.3.2 Euclat 4.3.3 FP-growth 4.3.4 ASSOC 4.3.5 OPUS 4.3.6 Neural Networks 4.3.7 Results diagnostics 4.3.8 Elements selection 4.3.9 IML 4.3.10 By problems 4.4 Dimentionality reduction Porównanie rónych algorytmów : link 4.4.1 Unsupervised 4.4.1.1 PCA 4.4.1.1.1 Basic PCA PCA moe by wyliczone na 2 sposoby: bezporednio z macierzy danych poprzez SVD (korzysta z tego sklearn) poprzez policzenie wartoci wasnych i wektorów wasnych na macierzy korelacji W przypadku PCA czsto stosuje si rekonstukcj. Jest to przeksztacanie danych z powrotem do pierwotnej przestrzeni. Ale tutaj mona wykorzysta tylko k-najwikszych wartoci wasnych (i odpowiadajcych im wektorów wasnych) , co mona potraktowa jako odszumienie lub skompresowanie danych. Mona policzy bd rekonstrukcji pokazujcy jak bardzo dane skompresowane róni si od pierwotnych. Ilo k-najwyszych wartoci wasnych wybieramy metod okcia. 4.4.1.1.2 Incremental PCA 4.4.1.1.3 Randomized PCA 4.4.1.1.4 kernel PCA Ta wersja PCA stosuje znany z metody wektorów nonych pomys na zrzutowanie odpowiednim funkcj (kernelem) danych do przestrzeni o wyszej liczbie wymiarów. Tutaj w celu usprawnienia oblicze równie stosuje si kernel trick. Uwaga: Jest tutaj problem z rekonstrukcj danych. PCA jest wykonywana w przestrzeni o wyszej liczbie wymiarów. Rekonstrukcja wykonana wzorami adekwatnymi dla zwykej PCA zrekonstruuje nam dane ale w przestrzeni o wyszej liczbie wymiarów. Dlatego dane trzeba dodatkowo przeksztaci (w celu otrzymania tzw. przeciwobrazu (ang. pre-image) ). Dopiero wtedy dostajemy pen rekonstrukcj. 4.4.1.2 t-SNE t-SNE to skrót od t-distributed stochastic neighbor embedding. Pseudoalgorytm (mój intuicyjny) : dla kadej obserwacji tworz niezalenie wielowymiarowy rozkad normalny. Jego wartoci oczekiwane s w miejscu obserwacji. Natomiast jago wariancje/korelacje s zalene od pozostaych punktów w przestrzeni zmiennych. Rozkad jest po prostu dopasowywany do tych punktów. Z wasnoci rozkadu normalnego bdziemy mieli mae wartoci prawdopodobiestwa dla odlegych obserwacji i wysokie dla pobliskich. Wic mamy tu wartoci w oparciu o które moemy budowa jak miar odlegoci. Nastpnie rozkady s standaryzowane. To powoduje e wartoci prawdopodobiestwa przypisane punktom znajdujcym si w pobliu analizowanego punktu s podobne, niezalenie od tego czy punktu te s upakowane gsto czy rzadziej (rzadziej czyli e s rednio rzecz bior nieco bardziej oddalone). Zestandaryzowane prawdopodobiestwa traktujemy jako miary odlegoci midzy punktami. Odlego z A do B nie musi by równa odlegoci z B do A przy liczeniu ich powyszym sposobem. Dlatego symetryzuje uredniajc je. W ten sposób dostajemy macierz odlegoci pomidzy punktami. W macierzy punkty sortujemy mniej wicej tak, eby punkty bliskie byo obok siebie. Teraz dane rzutujemy losowo na przestrze o niszej iloci wymiarów. Dla nowej przestrzeni znowu liczymy odlegoci midzy punktami, ale tym razem uywajc rozkadu t-studenta. Wynika to z tego e rozkad ten ma cisze ogony, co spowoduje e obserwacje które byy pierwotnie daleko bdzie atwiej wypchn na wiksz odlego i dziki temu dane nie bd miay tendencji do skupiania si na maym obszarze. Dostajemy now macierz odlegoci. Zachowujemy sortowanie punktów z poprzedniej macierzy. Poniewa punkty zostay losowo zrzutowane to nie bdzie zachowana regua, e punkty znajdujce si obok siebie w macierzy bd blisko siebie pod ktem odlegoci liczonej rozkadem t-studenta. Dokonujemy iteracyjne przesuni kolejnych obserwacji w taki sposób aby macierz nowa zacza si upodobnia do starej. Robi si to poprzez to, e punkty bdce blisko na pierwotnej macierzy mocniej przycigaj a znajdujce si daleko odpychaj analizowany punkt (bardziej technicznie: Jest tutaj uywany gradient descent, wzgldem odpowiednio zdefiniowanej funkcji straty. Funkcja straty przy pomocy miary Kullback-Lieblera mierzy rónic w odlegociach midzy pierwotnym przestrzeni a now. Chcemy te rónice minimalizowa) . W czasie liczenia iteracji bardzo wanym jest dobrze ustawi hiperparametr perplexity. Steruj on si przycigania przez bliskie punkty. Due przyciganie przez bliskie punkty sprzyja zachowaniu lokalnych struktur danych z pierwotnej przestrzeni. Niskie przyciganie zachowuje bardziej globalne struktury. 4.4.1.3 Local Linear Embedding (LLE) Pseudoalgorytm (mój intuicyjny) Ustalamy parametr k okrelajcych ilo najbliszych ssiadów obserwacji branych do analizy. Dla kadego punktu tworzymy regresj w oparciu o k najbliszych ssiadów. Wagi w dobieramy tak eby suma waonych ssiadów dawaa warto jak najbardziej zblion do wartoci analizowanej obserwacji. Nastpnie wchodzimy do przestrzeni o niszej liczbie wymiarów. Tutaj wagi w z regresji policzonej w wyej liczbie wymiarów traktujemy jako ustalone. Tym razem dopasowujemy wartoci z wspórzdnych obserwacji tak aby regresja dla kadego punktów wzgldem ssiadów bya jak najlepsza (w jest ustalone!!!). 4.4.1.4 Isomap To jest algorytm w którym przeprowadza si klasyczna skalowania wielowymiarowe (MDS), ale na specjalnie przygotowanej macierzy odlegoci. Isomap jest jednym z algorytów zakadajcych e dane s zlokalizowane na rozmaitoci. W takie sytuacji na poniszym rysunku odlego midzy punktami x1 i x2 chcemy policzy po czerwonej krzywej a nie po niebieskiej linii. Opis algorytmu (mój intuicyjny): Musze utworzy macierz odlegoci. Dla kadego punktu ustalam punktu w najbliszym ssiedztwie. Robi to albo: wybierajc k najbliszych ssiadów albo poprzez utworzenie kuli o zadanym promieniu Jeeli punkty s w swoim ssiedztwie to bd poczone na grafie. Graf bdzie waony odlegociami. Licz odlegoci pomidzy wszystkimi punktami uywajc algorytmu Dijkstra lub FloydWarshall. Tutaj podamy przykad dla tego drugiego algorytmu. Poniej zaczynamy od czciowo poczonego grafu (tutaj akurat jest graf skierowany, ale w ISOMAP chyba jest nieskierowany) który znajduje si po lewej stronie. Nastpnie wykonujemy kolejne iteracje (k) W iteracji k=0 po prostu wypisujemy wszystkie poczenia. W k=1 wykrylimy e ciek 2-&gt;3 (dugo = 3) mona zastpi krótsz ciek 2-&gt;1-&gt;3 (dugo = 4-2=2). W k = 2 majc wiedz o nowej optymalnej ciece ustalonej w k = 1, odkrywamy e mona zrobi ciek 4-&gt;2-&gt;1-&gt;3. Tutaj np. wykluczamy moliwo cieki 4-&gt;2-&gt;3 bo w nowej ciece 2-&gt;1-&gt;3. jest krótsze ni 2-&gt;3. Ostatecznie dla k = 4 znaleziono wszystkie cieki przechodzce tak, e poruszajc si po nich mamy zapewnione najkrótsze przejcia midzy punktami. Widzimy e cieki zawieraj w sobie podcieki i nie ma potrzeby eby wypisywa niezalenie kombinacje cieek dla wszystkich kombinacji punktów. Na macierzy odlegoci przeprowadzam klasyczne skalowanie wielowymiarowe (waciwie to jest PCA zrobione na macierzy odlegoci). 4.4.1.5 LDA as dimentional reduction Jak robi w Pythonie: link 4.4.1.6 Partial Least Squares 4.4.1.7 Multidimentional Scaling Idea skalowania polega na tym e transformujc zbiór do przestrzeni o niszej liczbie wymiarów staramy si zachowa proporcje odlegoci miedzy obsrwacjami 4.4.1.8 Correspondence Analysis 4.4.1.9 Kohonen Networks Odnonie powyszego rysunku zakadam e: mamy P elementów zbioru danych. Kady element jest n wymiarowy Mamy N neuronów. Kady neuron ma n wag ( tyle ile wymiarów danych) oznaczonych przez mi. Opis algorytmu (mój intuicyjny) Na pocztku wartoci mi (wagi) s ustalone losowo. Bierzemy obserwacje \\(x_i\\) i sprawdzamy metryk do wag którego neuronu jest jej najbliej (np. ze wzgldu na metryk euklidesow). \\(c = \\underset{i}{argmin}||x-m_i||\\) Gdzie c to szukamy neuron (Best maching unit - BMU). Aktualizujemy wagi \\(m_i\\) ale tylko w ssiedztwie neuronu c. S róne metody szukania najbliszych neuronów. Najprostszy topologiczny sposób jest podany poniej. Neurony ssiednie dla naszego zielonego neuronu c to neurony niebieskie. Aktualizacja wag przebiega tak, e wagi aktualizujemy aby byy blisze (np. ze wzgldu na metryk euklidesow) do \\(x_i\\). Dla BMU aktualizacja jest silniejsza. Dla neuronów ssiednich jest sabsza. Interpretacja wyników.: Na koniec dostajemy gotowe neurony. Kady neuron ma w przestrzeni pierwotnej swój odpowiednik. To jest punkt od którego odlego wynosi 0. (czerwony may punkcik na wykresie po lewej). Zazwyczaj nie ma tam adnej obserwacji. Ale s elementy które s najbliej tego punktu (czerwone box-y obok czerwonego punktu). I te elementy s podpite pod nasz neuron. (Warto w neuronach (np. 9 dla tego z czerwon obwódk) to waga - akurat tutaj jest jedna waga chocia powinny by 3 bo pierwotna przestrze miaa 3 wymiary). Ilo obserwacji podpitych pod neuron moemy zobrazowa przez counts plot (im bardziej niebiesko tym wicej obserwacji): Moemy te zrobi U_matrix. Tutaj patrzymy czy ssiadujce neurony s sobie bliskie. Poniej naurowny czerwona s blisko swoich ssiadów.Neuron biay (na dole po prawej) jest daleko od ssiadów. Dorze jest eby neurony bliskie tworzyy skupiska. Takie odseparowane skupiska, oznaczaj uchwycenie przez nasz map jakich struktur danych. Wane s te heat mapy. Robimy je ze wzgldu na konkretny wymiar z przestrzeni wyjciowej obserwacji. Poniej mamy dwie takie mapy. Ten po lewej, pokazuje jaki jest redni poziom bezrobocie dla obserwacji podpitych pod kady neuron. Ten poprawej jest dla zmiennej poziom edukacji. 4.4.1.10 Neural Networks (other than Kohonen) 4.4.1.11 Factor Analysis Interpretacja geometryczna Geometric interpretation of Factor Analysis parameters for 3 respondents to question a. The answer is represented by the unit vector , which is projected onto a plane defined by two orthonormal vectors and . The projection vector is and the error is perpendicular to the plane, so that . The projection vector may be represented in terms of the factor vectors as . The square of the length of the projection vector is the communality: . If another data vector were plotted, the cosine of the angle between and would be : the -entry in the correlation matrix. (Adapted from Harman Fig. 4.3)[2] 4.4.1.12 Latent semantic analysis 4.4.1.13 Autoencoders 4.4.1.14 Analiza glownych skladowych (PCA) [Principal Component Analysis] 4.4.1.15 Analiza glownych wspolrzednych (PCoA) [Principal Coordinates Analysis] 4.4.1.16 Analiza korespondencji (CA) [Correspondenca Analysis] 4.4.1.17 Asymetryczna analiza korespondencji (ACA) [Non-symmetric correspondence analysis] 4.4.1.18 Kanoniczna analiza korespondencji (CCA) [Canonical correspondence analysis] 4.4.1.19 Laczna analiza korespondencji (JCA) [Joint Correspondence Analysis] 4.4.1.20 Odwrocona analiza korespondencji (InvCA) [Inverse correspondence analysis] 4.4.1.21 Taksowkowa analiza korespondencji (TCA) [Taxicab Correspondence Analysis] 4.4.1.22 Rozmyta analiza korespondencji (FCA) [Fuzzy correspondence analysis] 4.4.1.23 Wieloraka analiza korespondencji (MCA) [Multiple correspondence analysis] 4.4.1.24 Regularyzowana wieloraka analiza korespondencji (RMCA) [Regularized Multiple Correspondence Analysis] 4.4.1.25 Analiza czynnikowa (FA) Factor analysis 4.4.1.26 Wieloraka analiza czynnikowa (MFA) [Multiple Factor Analysis] 4.4.1.27 Analiza skladowych niezaleznych (ICA) [Independent component analysis ] 4.4.1.28 (DCA) [Detreded Correspondence Analysis] 4.4.1.29 Nieliniowa Analiza Korespondencji (NPCA) [Nonlinear Principal Components Analysis] 4.4.1.30 (MDS) [Non-Metric Multidimensional Scaling] 4.4.1.31 UMAP Linki: pakiet w pythonie: link artyku porównujcy z t-SNE: link dobry artyku : link UMAP from scrach: link UMAP vs t-SNE: link Moje bardzo ogóle intuicyjne zrozumienie problemu: Jest to metoda oparta o graf. Czyli chcemy poczy obserwacje grafem w taki sposób eby obserwacje pooone bliej byy mocniej powizane a pooone dalej sabiej powizane. Graf jest dodatkowo waony (krawdzi przypisujemy liczny które te nam co mówi o odlegoci). Problem jest to jak e trzeba okreli regu tego które elementy ze sob czy. Mona to zrobi np. wyznaczajc wokó kadego punktu promie. I jeeli dla dwóch punktów ich promienie si zazbiaj to je czymy krawdzi. Problemem jest to jak szeroki promie dobra. Jeeli ustalimy jeden promie na sztywno dl wszystkich wyniki mog nie by satysfakcjonujce. Poniej punktu w rodku (niebiesko-zielone) nie bd z niczym poczona, ale wyraanie ukadaj si pewien wzorzec i powinny by jako poczone. Alternatywnie zamiast ustala z góry ustala promie mona ustali k najbliszych ssiadów. Ale wtedy bdziemy z kolej czasami mie za duo niepotrzebnych pocze. Tutaj z pomoc przychodzi nam geometria Rimmana. Moemy stworzy przestrze z tak metryk e nasz zbiór po przeksztaceniu bdzie rozoony równomiernie jak poniej: W tej nowej przestrzeni ustalany rednic promieni tak aby kady punkt mia mniej wicej k najbliszych ssiadów. Nastpnie wracany do przestrzeni z normaln metryk. Gemetria Rimmana powoduje e dla kadego punktu jest lokalnie okrelona inna metryka. Przez to po powrocie do wyjciowej przestrzeni otrzymamy promienie o rónej rednicy dla kadego punktu: Teraz moemy zbudowa graf: Problemem jest jeszcze to e odlegoci midzy punktami nie s symetryczne ( std krawdzie grafu s zaokrglone). Trzeba ja odpowiedni zsymetryzowa. Symetryzacj robi si inaczej ni w t-SNE. Dodatkowo odlegoci si nie standaryzuje. W tym momencie mamy graf i moemy zbudowa macierz odlegoci. Dalsza cz algorytmu jest podobna do t-SNE. Chcemy znale w przestrzeni o niszej liczbie wymiarów punktu rozlokowane tak aby struktura odlegoci midzy punktami bya jak najlepiej zachowana. Tutaj tak jak w t-SNE mamy funkcj straty i stosujemy gradient descent. Jednak nie uywamy tutaj odlegoci KL ale entropie krzyow aby porówna rónice w odlegociach midzy przestrzeni pierwotn i now o mniejszej liczbie wymiarów. 4.4.1.32 TriMAP opis algorytmu w PDF: link Intuitively, the algorithm tries to find an embedding (reprezentacje w przestrzeni o mniejszej liczbie wymiarów) that preserves the ordering ofdistances within a subset of triplets that mostly consist of a k-nearest neighbor and a further observation with respect to a focal observation, and a few additional randomized triplets which consist of two non-neighbors. As can be seen in the description above, the algorithm contains many design choices which work empirically, but it is not evident which of these choices are essential to creating good visualizations. 4.4.1.33 PaCMAP 4.4.2 Supervised 4.4.2.1 LDA 4.4.2.2 Partial Least Squares 4.4.3 Results diagnostics 4.4.4 Elements selection 4.4.5 IML 4.4.6 By problems 4.5 Casuality analysis 4.5.1 SEM Modele SEM su do analizowanie przyczynowoci/korelacji miedzy zmiennymi. Badacz tworzy okrelone zaoenia co do struktury przyczynowoci/korelacji midzy zmiennymi i potem to weryfikuje. W podejciu takim moemy równie stawia hipotezy na temat zmiennych ukrytych latent variables. Filmik gdzie mam pokazane co jest estymowane w podejciu ze zmiennymi ukrytymi: (link)[https://www.youtube.com/watch?v=nR94_juMpX0] typy analiz SEM typy path analysis i inne rzeczy o SEM 4.5.1.1 Confirmatory factor analysis 4.5.1.2 Confirmatory composite analysis 4.5.1.3 Path analysis W tym podejciu zakadamy e wszystkie zmiennej s jawna (nie ma zmiennych ukrytych). W katalogu material jest przykad numeryczny. 4.5.1.4 Partial least squares path modeling 4.5.1.5 Latent growth modeling 4.6 Dimentions decomoposition 4.6.1 Results diagnostics 4.6.2 Elements selection 4.6.3 IML 4.6.4 By problems 4.7 Generative models 4.7.1 Results diagnostics 4.7.2 Elements selection 4.7.3 IML 4.7.4 By problems "],["learning-with-target.html", "Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.2 Econometrical regression 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees 5.6 SVM 5.7 K-NN 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models 5.11 Ensembled models 5.12 Neural Networks 5.13 Stochastic processes 5.14 Results diagnostics 5.15 Elements selection 5.16 IML 5.17 By problems", " Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.1.1 Classification 5.1.2 Regression 5.2 Econometrical regression 5.2.1 Basic regression 5.2.2 Basic dynamic model 5.2.3 Generalisations and constrains 5.2.3.1 GLM Szczegóowo o zaoeniach i problemach: link Interpretacja parametrów w modelach GLM: link Zaoenia rónych modeli (rodo: link) Rodzaje reszt w modelach GLM: link Podstawowe cechy modeli regresyjnych: Traditional Linear Model response variable: a continuous variable error distribution: normal link function: identity Logistic Regression response variable: a proportion error distribution: binomial link function: logit Poisson Regression in Log Linear Model response variable: a count error distribution: Poisson link function: log Gamma Model with Log Link response variable: a positive, continuous variable error distribution: gamma link function: log W modelach GLM nie zapisuje si skadnika losowego bo: model modeluje nie warto y ale parametr (np. p jako prawdopodobiestwo w rozkadzie Bernulliego). Reszty moemy sobie dodatkowo doliczy. 5.2.3.1.1 logisti regression W przypadku regresji logistycznej parametry maj chyba rozkad normalny. Czym s reszty w regresji logistycznej: link Dlaczego nie ma error term: link, link 5.2.4 Bayesian inference 5.2.5 Multivariate models 5.2.6 Models with effects Zaómy, e mamy model regresji liniowej ze zmiennymi przecznikowymi (zmienne zero-jeden). Te zmienne przeczaj warto wyrazu wolnego pomidzy rónymi podzbiorami obserwacji. Podobne przeczniki moemy mie na parametrach kierunkowych. Jeeli zaoymy e te zrónicowane efekty nie s losowe, to mamy model efektami staymi. Zazwyczaj mona je estymowa MKN. Jeeli zaoymy e te przeczniki s losowane z jakiego rozkadu to mamy efekty losowe. Wtedy uywamy innych technik estymacji. S te modele gdzie mamy efekty mieszane, czyli zarówno losowe jak i nielosowe. Warto doda e efekty mona te zagnieda. To czy efekty s losowe czy nielosowe mona próbowa zweryfikowa testem Hausman-a. Modele panelowe Modele panelowe s szczególnym przypadkiem modelu z efektami. Mamy tutaj dane przekrojowa w czasie. Dla badanych obiektów (np. przedsibiorstw), mamy szeregi czasowe. Wic mamy wiele obserwacji na obiekt. Przeczniki dziaajc miedzy obiektami. Na kady przecznik mamy kilka obserwacji wic jestemy w stanie je wyestymowa. Modele panelowe wystpuj w wersji dynamicznej (zmiennymi objaniajcymi jest opóniona zmienna objaniana) oraz w wersji wielorównaniowej). 5.2.7 Nonparametric regression 5.2.7.1 MARS Splines Mars jest technik jednowymiarow (ma jedenowymiarowy output). Jego uogólnieniem wulowymiarowym (wielowymiarowy output) jest Polymars. Zaómy e mamy jedn zmienn predykcyjn. MARS polega na tym aby zmienn x podzieli na segmenty i nastpnie w ramach segmentów poprzez np. Metod Najmniejszych Kwadradów dopasowa wielomian. W najprostszym przypadku moe to by staa (wtedy dostaniemy schodki). Punkty rodzielajce segmenty to wzy (knots). W przypadki wielomianów wyszych stopni chcemy eby krzywe co najmniej si czyy (regresje liniowe) w wzch \\(f_k(x_k)=f_{k+1}(x_k)\\), albo jeeli to moliwe eby w wzach bya zachowana cigo. Ogólniej mówic na wielomian stopnia M nakada si warunek cigoci w wzach stopnia M-1 (czyli pochodne do stopnia M-1 musz by równe w wle dla ssiadujcych segmentów). Aby to osign na algorytm dopasowujcy wielomiany (np. MKN) trzeba naoy dodatkowe warunki ograniczajce. W modelu musimy zatem z góry ustali: ilo wzów i ich pooenie stopie wielomianu 5.2.8 Pros Works well with a large number of predictor variables Automatically detects interactions between variables It is an efficient and fast algorithm, despite its complexity Robust to outliers 5.2.9 Cons Susceptible to overfitting More difficult to understand and interpret than other methods Not good with missing data 5.2.9.1 Isotonic Linki: Przykad numeryczny z algorytmem PAVA: Online isotonic regression Wojciech Kotowski Opis algorytmu: analyticsvidhya Staramy si zminimalizowa: \\[ \\text{argmin}_x |y - x |^2 \\\\\\text{subject to } x_0 \\leq x_1 \\leq \\cdots \\leq x_n \\] Najczciej osiga si to dziki algorytmowi PAVA (Pool Adjacent Violators Algorithm). Wizualizacja dziaania algorytmu PAVA: 5.2.10 Other regression models 5.2.10.1 Canonical analysis 5.2.10.2 ANOVA MANOVA ANCOVA 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees Drzewo jest przykadem algorytmu zachannego (greedy) Dlaczego w praktyce uywa si tylko drzew binarnych: (link: stack_change) The number of possible splits goes up exponentially. If you are splitting on a continuous variable that has 1000 distinct values, there are 999 binary splits, but 999*998 trinary splits. There are: \\(\\binom{1000-1}{3-1} = 999*998/2\\) splits, actually. 5.5.1 pros Nie trzeba preprocesowa danych. Nie trzeba normalizowa zmiennych cigych. Zmiennych jakociowych nie trzeba rekodowa. moliwo pracy z danymi jakociowymi i ilociowymi s nieparametryczne. Nie maj zaoe o rozkadach nie ma problemu z brakami danych. Przy analizie zmiennej na splicie braki s prostu pomijane. atwa interpretacja szybkie wyliczenie predykcji przez nisk zoono obliczeniow O(log(m)). Tak naprawd same przeprowadzaj selekcje cech (feature selection). 5.5.2 cons atwo model przetrenowa. S niestabilne. Mae zmiany w danych generuj mocno rónice si drzewa. Przez te problemy wystpuje dua wariancja modelu i sabe uogólnianie. s algorytmem zachannym wic nie daj gwarancji znalezienia optimum globalnego. dosy dugo czas estymacji modelu wraliwo na rotacje danych (Geron 2018) s. 184. dhirajkumarblog.medium 5.5.3 Classification 5.5.4 Regression 5.6 SVM 5.6.1 Classification Linki: Fajny opis na kapita ludzki: czesc 1 czesc 2 Zaómy e mamy 2-wymiarowe dane, które dodatkowo maj 2 klasy 1 i -1. Dokadamy 3 wymiar na którym moemy odoy wartoci przypisane klasom (Geron 2018) s 166. Majc 3 wymiary tworzymy hiperpaszczyzn. Jej równanie to : \\(y=\\textbf{w}^T\\cdot \\textbf{x} + b\\) Przejdmy z powrotem do 2 wymiarów. Hiperpaszczyzna w pewnym miejscy przecina nasze 2 wyjciowe wymiary danych (y=0, czarna linie poniej). Dodatkowo na te 2 wymiary zrzutujemy 2 linie dla których wartoci hiperpaszczyzny wynosz 1 i -1 (linie niebieska i róowa): Obszar midzy liniami niebiesk i róow stanow margines. Chcemy stworzy tak eby: by jak najszerszy. by umieszczony w takim miejscu eby jak najlepiej separowa obserwacje. Szeroko marginesu jest pochodn nachylenia hiperpaszczyzny, zatem jest funkcj parametrów tej hiperpaszczyzny i jest równa dugoci dugoci wektora tych parametrów: ||w||. To jest warto któr maksymalizujemy. Ze wzgldu na uproszenie zwizane z liczeniem pochodnych maksymalizujemy: \\(\\frac{1}{2}||\\textbf{w}||^2\\) Aby speni drugi warunek (separowanie obserwacji) na powysz funkcj celu musimy doda warunki ograniczajce: $t{(i)}(T ^{(i)}+b ) $ \\(t^{(i)}\\) równa si -1 dla obserwacji przypisanych do klasy -1 o równa si 1 do obserwacji przypisanych do klasy 1 (1 * 1 = 1 oraz -1 * -1 = 1). Pozyej mamy ten problem e mamy tu doczynienia z twardym merginesami. Z powyszym jest taki problem e mamy tutaj twarde marginesy, a zatem staramy si zrobi idealn separacj. Problem optymalizacyjny jest tak sfomuowany e nie dopuszcza bdnych klasyfikacji. eby zaagodzi temu moemy wprowadzi slack variables. Jeeli do wartoci y przypisanej obserwacji na hiperpaszczynie dodamy w warunku zmienn \\(\\zeta\\), to ta zmienna spowoduje e nawet jeeli warto y bdzie nieodpowiednia to po dodaniu zmiennej \\(\\zeta\\) warunek jednak bdzie speniony. To wanie nam luzuje ograniczenie. Zatem nowa funkcja celu: \\(\\frac{1}{2}||\\textbf{w}||^2 + C\\sum_{i=1}^{m}{\\zeta^{(i)}}\\) $t{(i)}(T ^{(i)}+b ) , , , dla , , i = 1, 2, m $ W pierwszym równaniu drugi element sumy jest stay. Jeeli zwikszamy C to suma slack variables si zmniejsza i na odwrót. Zatem may C to due slack variables i dua tolerancja na bdy klasyfikacji, czyli szeroki margines. Powysze zagadnienie optymalizacyjne to zadanie programowania kwadratowego. Zadanie to jest sprowadzone z tzw. problemu pierwotnego do problemu dualnego. Warto zauway e w celu przechodzenia pomidzy wynikami rozwizania problemu dualnego i pierwotnego nasze problem powinien spenia warunki Karush-Kuhn-Tuckera (KKT). Ostateczne rozwizanie zwizane jest z przeliczeniem iloczynów skalarnych obserwacji, co jest wan uwag dla rozwaa o maszynach kernelizowanych. Maszyny kernelizowane eby obserwacje ze wzgldu na klasy byo atwiej separowa, lepiej przenie je do przestrzeni o wikszej liczbie wymiarów przy pomocy odpowiedniej funkcji. Do tego celu jest wykorzystywanych kilka najbardziej popularnych przeksztace. Jak wspomniano wczeniej, rozwizania zadania optymalizacyjnego dla SVM opiera si o iloczyny skalarne obserwacji. Przeksztacanie obserwacji do przestrzeni o wiksze iloci wymiarów i obliczanie iloczynów skalarnych z nowej przestrzeni jest obliczeniochonne. Dlatego moemy zastosowa kernel trick. Jeeli nasze funkcja przeksztacajca \\(\\Phi\\) spenia warunki Mercera to moemy zastosowa twierdzenie Mercera i stworzy tzw. funkcje jdrow. Funkcja ta umoliwia uzyskanie wyników iloczynu skalarnego obserwacji (wektorów) w przestrzeni o wyszej liczbie wymiarów bez koniecznoci przeksztacania wektorów do tej przestrzeni (funkcja pracuje na pierwotnych wektorach). Zatem iloczyn skalarny liczmy nie przez \\(&lt;\\Phi(a),\\Phi(b)&gt;\\) (gdzie musimy na wektorach pierwotnych a i b zastosowa przeksztacenie \\(\\Phi\\), eby uzyska nowe wektory w przestrzeni o wikszej iloci wymiarów), ale mamy prostsz funkcj jdrow K(a,b) dziaajc bezporedni na wektorach a i b. 5.6.2 Regression 5.7 K-NN 5.7.1 Classification Jak wyliczane jest prawdopodobiestwo w sklearn : link The class probabilities are the normalized weighted average of indicators for the k-nearest classes, weighted by the inverse distance. For example: Say we have 6 classes, and the 5 nearest examples to our test input have class labels F, B, D, A, and B, with distances 2, 3, 4, 5, and 6, respectively. Then the unnormalized class probabilities can by computed by: (1/2) * [0, 0, 0, 0, 0, 1] + (1/3) * [0, 1, 0, 0, 0, 0] + (1/4) * [0, 0, 0, 1, 0, 0] + (1/5) * [1, 0, 0, 0, 0, 0] + (1/6) * [0, 1, 0, 0, 0, 0] = [1/5 ,1/2, 0, 1/4, 0, 1/2] 5.7.1.1 KD - tree Jest to algorytm do partycjonowania przestrzeni. Pseudoalgorytm na budow drzewa KD (mój intuicyjny): S jego róne wersje. Jedna z nich jest taka: Na pocztek wybierz losowy wymiar x. Dla tego wymiaru policz mediane po wszystkich punktach. Znajd punkt najbliszy do mediany ze wzgldu na analizowany wymiar. W ten sposób otrzymamy punkt medianowy. Podziel przestrze na dwie czci (binarny podzia) w oparciu o ten punkt. Wybierz losowo kolejny wymiar. Dla kadej podprzestrzeni (mamy ju podzia z poprzedniego kroku), znowu policz mediany znajd punkty medianowe i znowu dziel przestrze binarnie dla dla kadego punktu medianowego. Kontynuujemy do momentu, a w wzach ilo obserwacji spadnie do ustalonego minimum. Wzy w których przerywamy liczenie staj si limi drzewa. Alternatywnie zamiast wybiera wymiary losowo moemy wybiera je cyklicznie (dla 3 wymiarów x,y,z wymieramy je po kolei na przemian: x,y,z; x,y,z; x,y,z; x,y,z. Poprzez podzia dostaje podzia przestrzeni na podprzestrzenie (pierwszy rysunek poniej). Poniewa robilimy to hierarchicznie to ten podzia mona zaprezentowa w postaci drzewa binarnego (drugi rysunek poniej). Kady wze drzewa to jeden podzia podprzestrzeni. Pseudoalgorytm na szukanie najbliszego ssiada w drzewie KD (mój intuicyjny). Mam now obserwacje i chce poszuka który sporód punktów (precyzyjniej mówic wzów) drzewa jest najbliszy. Lokalizuje w którym liciu jest punkt poprzez przeszukanie drzewa od root. Licz odlego do najbliszego punktu wewntrz tego licia. Jeeli list byby pusty to licz odlego do wza tworzcego podzia który okrela ten li. W ten sposób wyznaczam pierwszy proponowany najbliszy punkt dla nowej obserwacji. Nastpnie rysuje koo którego rodkiem jest nowy punkt a jego rednic wyznacza odlego nowego punktu od punktu który jest proponowany jako najbliszy. Jeeli koo przecina inne ssiadujce podziay (wyznaczone przez wzy) to musz je te przeanalizowa. Od razu wyliczam odlego do tych podziaów od mojego nowego punktu (od podziaów a nie od wzów bdcych obserwacjami tworzcymi te podziay. Patrz rysunek poniej jak liczony jest \\(dist_3\\) i \\(dist_4\\)) Dla tych podziaów szukam odpowiadajce im wzy na drzewie. Jeeli podziaów do przeanalizowania jest kilka to zaczynam od najbliszego. Od tego podziau schodz po drzewie w dó (kryterium takie, e w pierwszej kolejnoci staram si i tak ciek jak bym chcia si zblia do mojego nowego punktu . Inne alternatywy te sprawdzam (oddalanie si), ale w drugiej kolejnoci) i sprawdzam czy których z wzów nie zawiera punktu który jest bliej ni dotychczas zaproponowany najbliszy punkt. Jeeli taki wze znajduje, to aktualizuje moje koo ze rodkiem w nowym punkcie. Nowe koo bdzie oczywicie mniejsze. Wtedy znowu sprawdzam które podziay przecinaj si z koem (koo jest mniejsze wic powinno by ich mniej ). Ta metoda ma wady dla przestrzeni wielowymiarowych. Przy duej liczbie wymiarów prawie na pewno mój nowy element nie bdzie w rodku licia , ale blisko jakiego podziau. Wic rysujc koo najprawdopodobniej oka si e trzeba sprawdza dodatkowe wzy. Dodatkowo im mamy wicej wymiarów tym wicej wezów/podziaów ze sob ssiaduje. Tak wic koo bdzie przecinao wiele podziaów i algorytm zaczyna traci efektywno. Aby temu zapobiec zaproponowano algorytm ball tree. 5.7.1.2 Ball tree Ball tree radz sobie lepiej ni KD-tree w przestrzeniach o duej liczbie wymiarów. Pseudoalgorytm budowy ball tree (mój intuicyjny): Szukam dwóch najbardziej odlegych punktów. Jednak aby nie sprawdza wszystkich moliwych kombinacji (kady punkt z kadym) stosuje trick. Wybieram losowy punkt i szukam pierwszego punktu który ley najdalej i potem drugiego który ley najdalej od tego pierwszego. Ilo sprawdzanych kombinacji mocno spada, a prawdopodobiestwo, e uzyskane tym sposobem dwa punkty s jednymi z najbardziej oddalonych, jest bardzo due. Te 2 nowe punkt cz lini i wyznaczam rodek tej linii. Ten rodek i punkty wyznaczaj mi kul. To jest pierwsza kula. Linia któr narysowaem aby poczy 2 odlege punkty powinna wyznacza kierunek w którym wariancja zbioru jest najwiksza. Zatem rysujc paszczyzn prostopad do tej linii mog wyznaczy sensowny podzia zbioru na 2 mniejsze. Dla kadego z tych dwóch podzbiorów wyznaczam rodek (co na zasadzie centroidu). Dodatkowo metod z punktu pierwszego w dla kadego z podzbiorów wyznaczam najodleglejsze punkty. Tak wic mog teraz narysowa 2 kolejne koa. Powysz procedur kontynuuje a liczebno obserwacji w kilach zbliy si do ustalonego minimum. Sam schemat powstaje hierarchicznie i dlatego mona przestawi go jako drzewo. Chocia trzeba pamita e kula na tym samym poziomie hierarchii mog si czciowo nakada. Wydaje mi si e w wyniku podziau prawie kady punkt powinien si ostatecznie znale w jakim liciu drzewa. Ale przez to e wybór najdalszych punktów w zbiorze jest przybliony moe nie zawsze tak by w 100%. Taki punkty potem przy wybieraniu najbliszego ssiada chyba i tak s pomijane. Pseudoalgorytm na szukanie najbliszego ssiada w ball tree (mój intuicyjny - knn seach). Mam nowy punkt który dla którego chce poszuka najbliszego ssiada. Sprawdzam czy punkt jest w jakim klastrze bdcym liciem (najmniejsze klastry które ju nie byy bardziej dzielone). Jeeli jest wewntrz, to szukam najbliszego ssiada w tym klastrze. Potem rysuje koo i sprawdzam czy nie jest czasem bliej do granic innych klastrów. Jeeli tak jest, to te je przeszukuje. Tutaj dziaa taki schemat poruszania si po drzewie jak przy KD-tree. Jeeli nowy punkt jest na zewntrz lici, to sprawdzam do centrum którego klastra jest mu najbliej. Jeeli to klaster li to z tego najbliszego klastra wybieram mu ssiada. Jeeli to jest duy klaster zawierajcy podklastry to musz dalej szuka w tym duym klastrze najbliszym podklastrów, a zejd to klastra licia. (tutaj te mamy do czynienia z poruszaniem si w du drzewa po kolejnych klastrach czyli wzach, co podobnego do kd-tree). 5.7.1.3 Condensing (Hart algorithm) Ide jest eby ze zbioru treningowego Z tworz podzbiorów S w taki sposób e elementy które nie maj wpyw na klasyfikacj ( nie s pooone na granicach klas) usun. Oznaczenia: T(x) - Prawdziwa klasyfikacja elementu x ze wzgldu na klas S(x) - Klasyfikacja elementu x ze wzgldu na decyzj podjt w oparciu o nowy budowany podzbiór S. Z(x) - Klasyfikacja elementu x ze wzgldu na decyzj podjt w oparciu o peny zbiór Z. k - ilo najbliszych ssiadów uwzgldnianych w klasyfikacji metod knn. Na pocztku zbiór S jest pusty. Losuj k elementów z zbioru Z i dodaje do zbioru S. Wybieram losowy punkt x ze zbioru Z nie nalecy do zbioru S. Przeprowadzam klasyfikacj dla x regu k-nn ze wzgldu na zbiór Z i S (czyli licz Z(x) i S(x)). Jeeli wynik obu klasyfikacji jest taki sam to pomijam ten punkt i losuje nastpny. Jeeli jednak wynik Z(x) i S(x) jest róny to sprawdzam czy klasyfikacja S(x) jest taka sama jak T(x). Jeeli tak, to dodaje element do zbioru S. Jeeli nie to szukam sporód elementów Z nie nalecych do S elementu najbliszego w stosunku do x który naley do tej samej klasy co Z(x). Dziki temu dodam punkt który raczej poprawi zdolno klasyfikacji ni pogorszy. Inaczej dodabym punkt le zaklasyfikowany w oparciu o Z i raczej podniosoby bd predykcji. Powtarzam t proceduj, a nie bdzie sytuacji e dla punktu x nalecego dla Z a nie nalecego do S bdzie spenione Z(x) != S(x). Bardziej szczegóowy opis: Wybieram po kolei punkty ze zbioru x. Scan all elements of X, looking for an element x whose nearest prototype from S has a different label than x Remove x from X and add it to S Repeat the scan until no more prototypes are added to S S used instead of X for kNN classification. 5.7.1.4 Editing 5.7.1.5 Reducing 5.7.2 Regression 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models modele typy survival 5.10.1 Podstawowe pojcia Linki: link Obserwacje cenzurowane Cenzurowanie jest problemem odpowiednikiem problemu braku danych w analizie przeycia. Cenzurowanie wynika zasadniczo z dwóch przyczyn. Okres ycia obserwacji wykracza poza okres analizowany (np. kredyt jest dalej spacany po kocu analizowanego okresu i nie wiemy czy ostatecznie zosta spacony czy nie). Tutaj mamy sytuacje prawostronnego cenzurowania Obserwujemy obserwacje od pewnego momentu i nie wiemy co si z ni dziao wczeniej (np. nie wiemy jak dugo by spacany kredyt). To jest cenzurowanie lewostronne. Wiemy e zdarzenie zaszo w jakim momencie, ale nie wiemy dokadnie kiedy. Wiemy tylko w jakim przedziale czasowym to byo (interaval censoring). Od pewnego momentu w czasie trwania analizy nie mamy informacji o obserwacji (pacjent wycofa si z badania, utracilimy z nim kontakt, w bazie jest brak danych o tym co si dziao z kredytem od pewnego momentu). Jest to kolejny przykad cenzurowania prawostronnego. Tutaj wystpuje problem informatywnoci. Moe by tak e, u pacjentów u których leczenie zadziaao bardzo dobrze, moe by wysze ryzyko wycofania si z badania. Wtedy wycofanie si oznacza te mniejsze ryzyko zgonu. Czyli cenzorowanie danych jest skorelowane z ryzykiem zdarzenia. Jest jeszcze sytuacji kiedy badamy kredyty np. od stycznia 2020 a pewnie kredyt zosta wypacony z marciu 2020. Mamy o nim pene informacje (nie ma cenzurowania). Kredyt te by spacony przed kocem czasu badania (nie byo zdarzenia defaultu). Wtedy te nie ma cenzurowania. Co robi z problemem cenzurowania: Mona usun takie obserwacje. Mona wykona imputacje. W analizie kredytów wystpuje te problem te e kredyt po defaulcie moe z powrotem oy, co narusza zaoenia wikszoci modeli analizy przeycia. Wtedy ozdrowione kredyty moemy traktowa jako nowe obserwacje. Jednak powoduje to pewne obcienie wyników. Obserwacje obcite Nie do koca rozumiem czym s obserwacje ucite i czym si róni od cenzurowanych. Trzeba to doczyta Zdarzenie (event) np. mier pacjenta, defalut kredytu. Funkcja gstoci prawdopodobiestwa f(t) Okrela prawdopodobiestwo zdarzenia w czasie t, gdzie czas t to dowolny przedzia [t1,t2]: \\(P(t_1 &lt; T \\leq t_2)=\\int_{t1}^{t2}{f(t)}dt\\) Gdzie T to jest moment zdarzenia. Dystrybuanta funkcji gstoci prawdopodobiestwa F(t) Midzy ni a funkcj gstoci mamy zaleno: $$ f(t) = dF(t)/dt \\ F(t) = _{0}^{t}{f(u)du} $$ Funkcja przetrwania S(t) (survival function) Nazywa si te funkcj doycia lub funkcj trwania. Pokazuje e dana osoba przetrwa duej ni okrelony czas. Mona j wyliczy jako dopenienie dystrybuanty funkcji gstoci: S(t) = 1-F(t) Z funkcj gstoci cz j nastpujce zalenoci: \\[ f(t)=-dS(t)/dt S(t)=\\int_{t}^{\\infty}{f(u)du} \\] Znajomo funkcji (1) gstoci , (2) dystrybuanty i (3) przetrwania pozwala nam wyliczy prawdopodobiestwo zdarzenia w okrelonym czasie: \\[ P(t_1 &lt; T \\leq t_2)=\\int_{t1}^{t2}{f(t)}dt \\\\ P(t_1 &lt; T \\leq t_2)=F(t_2)-F(t_1) \\\\ P(t_1 &lt; T \\leq t_2)=S(t_2)-S(t_1) \\] Funkcja hazardu \\(\\lambda(t)\\) Nazywana te funkcj intensywnoci procesu lub natenia zdarze. Okrela ona prawdopodobiestwo zdarzenia w chwili t pod warunkiem e klient doy do tej chwili: \\[ \\lambda(t)=\\frac{f(t)}{S(t)} = \\frac{d[ln(S(t))]}{dt} \\] Skumulowana funkcja hazardu: Jak sama definicja mówi. Struktura danych - uwagi Wydaje mi si e jest tak (np. w Pythonie) e tutaj nas nie interesuj dokadne momenty rozpoczcia i zakoczenia zbierania danych o obserwacji. Potrzebujemy tylko czasu trwania obserwacji. Uwagi o podziale estymatorów. S tutaj podzielone na standardowe (bardziej klasyczna statystyka) i inne (bardziej machine learningu) Podzia metod standardowych mona przestawi tak: Powyej w metodach parametrycznych (zielony kolor) powinny by jeszcze dodane model proporcjonalnego hazardu ( w moim podziale dalej s uwzgldnione). 5.10.2 Estymatory standardowe-nieparametryczne 5.10.2.1 Tablice trwania ycia (life table) Tablica przeycia to najprostsze opisowe podejcie do tematu. Przykadowe najprostsza analiza przeycia: Age *Number living (At ri sk)** *Number d ead** *Number c ensored (ci którzy o pucili badanie ale nie umarli i r edukuj ilo obs erwacji w k olejnym wie rs zu)** Prob ability of death Survial prob ability Cum ulative S urvival Probab ility 0 14,353 2 0 0.0001 1 - 0.0001 = 0.9999 0.9999 1 14,353 - 0 - 2 = 1 4,351 248 1855 0.0173 1 - 0.0173 = 0.9827 0.9999 * 0.9827 = 0. 9826017 2 14,351 - 1855 - 240 = 1 2,248 155 758 0.0127 1 - 0.0127 = 0.9873 982 6017 * 0.9873 = 1. 9701227 5.10.2.2 Kaplan Mayer Jest to nieparametryczny estymator funkcji przeycia. Nie mona go niestety uywa do predykcji czyli szacowania czasu przeycia dla elementów które yy duej ni elementy w próbie (np. w probie \\(t_{max} = 10\\) , a ja chce wiedzie jak funkcja przeycia zachowa si dla t = 20) Estymator ma wzór: \\[ \\hat{S}(t)= \\prod_{i:\\,t_i\\leq t}{1-\\frac{d_i}{n_i}} \\] Czyli jest to iloczyn (czyli skumulowanie) prawdopodobiestw doycia do poprzednich okresów czasu. Przykad oblicze: Time, Years Number at Risk Nt Number of Deaths Dt Number Censored Ct Survival Pr obability St+1 = St*((N~ t +1-Dt+1~ )/Nt+1) 0 20 1 1 20 1 1* ((20-1)/20) = 0.950 2 19 1 950*((19-0 )/19)=0.950 3 18 1 0.950* ((18-1)/18) = 0.897 5 17 1 0.897* ((17-1)/17) = 0.844 6 16 1 0.844 9 15 1 0.844 10 14 1 0.844 11 13 1 0.844 12 12 1 0.844 13 11 1 0.844 14 10 1 0.760 17 9 1 1 0.676 18 7 1 0.676 19 6 1 0.676 21 5 1 0.676 23 4 1 0.507 24 3 3 0.507 Wariancja estymatora jest dana wzorem (Greenwood formula): \\[ \\hat \\sigma^2[\\hat S(t)] = \\widehat var[\\hat S(t)] = \\hat S(t)^2 \\sum_{i:t_i \\le t} \\frac{d_i}{n_i(n_i-d_i)} \\] Jeeli nie ma danych cenzorowanych wzór uprasza si do: \\[ \\hat \\sigma^2[\\hat S(t)] = \\frac{\\hat S(t) [1- \\hat S(t)]}{n} \\] Zaoymy e estymator funkcji przeycia Kaplana Mayera ma rozkad asymptotycznie normalny: \\[ \\hat S(t) \\simeq N(\\hat S(t), \\sigma(t)/\\sqrt(n)) \\]Wtedy moemy zbudowa przedziay ufnoci : \\[ \\bigg(\\hat S(t) \\pm z_{1-\\alpha/2} \\cdot \\hat \\sigma/\\sqrt(n) \\bigg), \\] Gdzie z to jest rozkad normalny standaryzowany. 5.10.2.3 Nelson Aalen Jest to nieparametryczny estymator skumulowanej funkcji hazardu. Podobnie jak przy estymatorze Kaplana Mayera nie moe by uywany do predykcji. Jest dany formu: $$ (t)= _{i:,t_it}{} $$ Gdzie \\(d_i\\) to zdarzenia w czasie i, a \\(n_i\\) to ilo elementów w okresie i . Estymator wariancji wynosi: $$ ar((t))= _{i:,t_it}{} $$ Porównywanie funkcji estymowanych Przy metodach estymacji takich jak Kampal Mayer czy Nelson Aalen nie moemy bada bezporednio wpywu jakich dodatkowych zmiennych objanianych na funkcj przeycia. eby np. zbada wpyw tego czy palenie wpywa nasz funkcj przeycia, jedyne co nam pozostaje to zrobi dwie oddzielne funkcje dla palcych i dla niepalcych i potem je porówna. S liczne statystyki na stawienie testowanie hipotezy czy krzywe s róne: wspóczynnik hazardu (hazard ratio) test istotnoci z test logarytmiczny rang (log-rank test) - dalej jest przykad numeryczny test WIlxona test Taronea test Peto zmodyfikowany test Peto test Fleminga Log rank test - przykad numeryczny: Zaomy e mamy 2 krzywe. Poniej pokazuje tabele wylicze dla obu: Pierwsza krzywa: Time, Months Number at Risk Nt Number of Deaths Dt Number Censored Ct Survival Pr obability ! 0 10 1 8 10 1 1 0.900 12 8 1 0.788 14 7 1 0.675 20 6 1 0.675 21 5 1 0.540 26 4 1 0.405 27 3 1 0.270 32 2 1 0.270 40 1 1 0.270 Druga krzywa: Time, Months Number at Risk Nt Number of Deaths Dt Number Censored Ct Survival Pr obability ! 0 10 1 25 10 2 1.000 28 8 1 0.875 33 7 1 0.750 37 6 1 0.750 41 5 1 0.600 43 4 1 0.600 48 3 3 0.600 Z powyszych dwóch tabel zrobimy jedn uwzgldniajc ilo obserwacji zgonów (deaths): Time, Months Number at Risk in Group 1 N1t Number at Risk in Group 2 N2t Number of Events (Deaths) in Group 1 O1t Number of Events (Deaths) in Group 2 O2t 8 10 10 1 0 12 8 10 1 0 14 7 10 1 0 21 5 10 1 0 26 4 8 1 0 27 3 8 1 0 28 2 8 0 1 33 1 7 0 1 41 0 5 0 1 Test opiera si na formule: \\[ \\chi^2=\\sum{\\frac{(\\sum{O_{jt}}-\\sum{E_{jt}})^2}{\\sum{E_{jt}}}} \\] Gdzie O to jest obserwowana ilo zdarze a E oczekiwana ilo zdarze. Oczekiwana ilo to ilo zdarze taka jaka powinna wystpowa jeeli krzywe si nie róni. Obliczanie oczekiwanej iloci zdarze dla kadej krzywej: T ime, Mon t hs N u mber at Risk in G roup 1 *N~ 1t ~ ** N u mber at Risk in G roup 2 *N~ 2t ~ ** T otal N u mber at R i sk N ~ t ~ N u mber of E v ents in G roup 1 *O~ 1t ~ ** N u mber of E v ents in G roup 2 *O~ 2t ~ ** T otal N u mber of Eve n ts O ~ t ~ *Exp e cted N u mber of E v ents i n ** G roup 1 E 1t = N 1t * (O~ t /N t ~) *Exp e cted N u mber of E v ents i n ** G roup 2 E 2t = N 2t * (O~ t /N t ~) 8 10 10 20 1 0 1 0 .500 0 .500 12 8 10 18 1 0 1 8 *(1 / 18)= 0 .444 0 .556 14 7 10 17 1 0 1 0 .412 0 .588 21 5 10 15 1 0 1 0 .333 0 .667 26 4 8 12 1 0 1 0 .333 0 .667 27 3 8 11 1 0 1 0 .273 0 .727 28 2 8 10 0 1 1 0 .200 0 .800 33 1 7 8 0 1 1 0 .125 0 .875 41 0 5 5 0 1 1 0 .000 1 .000 * 6 ** * 3 ** 2. 6 20 6. 3 80 5.10.3 Estymatora standardowe parametryczne Tutaj zakadamy e znamy funkcj gstoci rozkadu zdarze f(t). Podstawowe estymatory moemy podzieli na 2 klasy : przypieszonego ycia (AFT - accelerated failure time) proporcjonalnego hazardu W modelach AFT zakadamy, e predyktor ma multiplikatywny wpyw na funkcj przeycia (co si sprowadza do liniowego wpywu na logarytm funkcji przeycia). W modelu proporcjonalnego hazardu zakada si z kolei, e predyktor ma multiplikatywny wpyw na funkcj hazardu co si sprowadza do zalenoci: \\(\\lambda(t,x)=\\lambda_0(t)e^{x\\beta}\\). Zaoenia parametrycznych modeli proporcjonalnego hazardu: The true form of the underlying functions (hazard, survival) are specified correctly. The relationship between the predictors and the log hazard is linear. In the absence of interactions, the predictors act additively on the log hazard. The effect of the predictors is the same for all values of t. Dla modeli AFT stosuje si rozkady: Weibulla wykadniczy log-normalny log-logistyczny gamma Dla modeli proporcjonalnego hazardu stosuje si rozkady: weibulla wykadniczy gompertza Rozkady dopasowuje si najczciej metod najwikszej wiarygodnoci lub metod najmniejszych kwadratów. . Regresja liniowa Mona tutaj modelowa czas zdarzenia t. Zatem t = a0 + a1x + a2x +  + anx. Zakady e warunkowy rozkad t to rozkad normalny. 5.10.4 Estymatory standardowe semi-parametryczne 5.10.4.1 Cox link. W modelu tym funkcja estymuje si nieparametrycznie. Dodatkowo opiera si ona o pojcia hazardu bazowego czyli wartoci funkcji hazardu kiedy wszystkie x = 0. Jest zaliczany do metod semiparametrycznych. Z jednej strony nieparematrycznie estymuje funkcj hazardu jednak z drugiej strony s zaoenia (i.e., independence, changes in predictors produce proportional changes in the hazard regardless of time, and a linear association between the natural logarithm of the relative hazard and the predictors). Warto zwróci uwag na zaoenia staoci wartoci predyktorów w czasie. S uogólnienia gdzie predyktory mog si zmienia w czasie. W dla modelu Coxa wtedy hazard nie jest ju proporcjonalny. Model coxa wystpiue 5.10.5 Estymatory inne 5.10.5.1 Random Survival Forest Jest zaimplementowany w Pythonie Another feasible machine learning approach which can be used to avoid the proportional constraint of the Cox proportional hazards model is a random survival forest (RSF). The random survival forest is defined as a tree method that constructs an ensemble estimate for the cumulative hazard function. Constructing the ensembles from base learners, such as trees, can substantially improve the prediction performance. [13] Basically, RSF computes a random forest using the log-rank test as the splitting criterion. It calculates the cumulative hazards of the leaf nodes in each tree and averages them in following ensemble. The tree is grown to full size under the condition that each terminal node have no less than a prespecified number of deaths. [18] The out-of-bag samples are then used to compute the prediction error of the ensemble cumulative hazard function. 5.10.5.2 DeepSurv Jest to zastosowanie sieci neuronowych bdcych zamiennikiem dla modelu Coxa. Wic podobnie jak tam, badamy zaleno od wielu predyktorów zachowania si funkcji hazardu link . Oto architektura sieci: Jej funkcja loss, która jest minimalizowana to: Funkcja straty jest oparta o funkcj straty optymalizowan w modelu Cox-a link. Jest chyba optymalizowane niezalenie dla kadego punktu czasowego. Aby minimalizowa t funkcj musimy maksymalizowa wyraenie w czerwonej ramce. Lewa cz wyraenia musi by zatem maksymalizowana (hazard dla elementów dla których wystpio zdarzenie w analizowanym momencie). Prawa minimalizowana. W prawej czci s elementy dla których nie zaszo zdarzenie do analizowanego momentu czasowego. 5.10.6 Performence Dopasowanie rozkadów do danych bada si w oparciu o reszty: reszty coxa snella reszty martyngaowe reszty schoenfelda 5.11 Ensembled models 5.11.1 Bagging and Pasting 5.11.1.1 Random Forest 5.11.1.1.1 Out of Bag Error Pseudoalgorytm (mój intuicyjny) - Random Forest dla klasyfikacji. Dla kadego drzewa losuj prób z danych. Robi to albo boostrapowo (z powtarzeniem z n elementowej populacji losuje n etlementów), albo bez powtarzania. Buduj niezalenie drzewa. Kade drzewo zakadam e na kocu zwraca twarde labels a nie prawdopodobiestwa. W trakcie wyliczam Out-Of-Bag Error. Dana obserwacje jest predykowana przez wszystkie modele dla których nie zostaa ona wylosowana do zbioru uczcego. Out-Of-Bag error jest przydatny do ustalania optymalnej iloci drzew. Na koniec przeprowadzam ostateczn predykcj, poprzez gosowanie drzew. Kade drzewo ma tak sam wag. Pros: Random forest can solve both type of problems that is classification and regression and does a decent estimation at both fronts. One of benefits of Random Forest which exists me most is, the power of handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method. Further, the model outputs importance of variable, which can be a very handy feature. It has an effective method for estimating missing data and maintains accuracy when large proportion of the data are missing. It has methods for balancing errors in data sets where classes are imbalanced. The capability of the above can be extended to unlabeled data, leading to unsupervised clustering,data views and outlier detection. Random forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of data is not used for training and can be used to testing. These are called the OUT OF BAG samples. Error estimated on these out put bag samples is know as out of bag error. Study of error estimates by out of bag, gives avidenc to show that the out of bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out of bag error estimate removes the need for a set aside test set. Cons: It surely does a good job at classification but not as for regression problem as it does not gives precise continuous nature prediction. In case of regression, it doesnt predict beyond the range in the training data, and that they may over fit data sets that are particularly noisy. Random forest can feel like a black box approach for a statistical modelers we have very little control on what the model does. You can at best try different parameters and random seeds. 5.11.2 Boosting Pierwsza z siedmiu czci z artykuami o algorytmach typu boosting link:deep-and-shallow . Linki do kolejnych czci s na kocu artykuu. 5.11.2.1 Ada Boost Adaptive boosting (adaptacyjne wzmacnianie). Mam tutaj 2 rodzaje wag: Wagi modeli. Im lepszy model tym bdzie mia w finalnej klasyfikacji wiksz wag. \\(\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-total.error}{totl.error})\\) . Poniewa funkcja nie ma wartoci dla total_error równe 0 i 1 zazwyczaj dodaje si tutaj jak korekt dla zabezpieczenia. Total error to suma bdów waonych wagami obserwacji. Wagi obserwacji. Obserwacje le zaklasyfikowane przez i-ty model maj wiksz wag przy nastpnym modelu. Wagi mog by uywane do losowania waonego dla nastpnego modelu, albo do waonego Ginii index uywanego do obliczania impurity. Wagi dla obserwacji le zaklasyfikowanych liczy si ze wzoru : \\(nowa.waga = stara.waga \\cdot e^{waga.poprzedniego.modelu}\\). Wagi dla klasyfikacji dobrze zaklasyfikowanych liczy si ze wzoru: \\(nowa.waga = stara.waga \\cdot e^{- waga.poprzedniego.modelu}\\) . W tym wzorach mona doda wspóczynnik uczenia w wykadniku liczby e. Patrz: (Bonaccorso 2019) s 263. Ada boost- uwagi: Zazwyczaj bazuje na drzewach. Jeeli s to drzewa, to najczciej uywa si stumps, czyli drzew binarnych z tylko jednym podziaem. wystpuje w m.in nastpujcych wersjach: Bazowy AdaBoost do zagadnie binarnych. M1 - podstawowy algorytm dla zagadnienia klasyfikacyjnego. (Mirjalili 2019) s 234. M2 - (porównanie z M1 link) SUMME - jest uogólnieniem na zagadnienie wieloklasowego bez uywania podejcia jeden-przeciwko-wszystkim (One-vs-Rest). Jeeli robimy model binarny to podecie to redukuje si do standardowego AdaBoost M1. SUMME.R - (litera R od real - AdaBoost rzeczywisty) rozwinicie, gdzie wagi s liczone w oparciu o prawdopodobiestwa. Peny algorytm w (Bonaccarso2019?) s 268. R2 - AdaBoost dla zagadnienia regresyjnego. Peny algorytm w (Bonaccarso2019?) s 271. Pseudoalgorym (mój intuicyjny) - dla wariantu M1 w ramach zagadnienia binarnej klasyfikacji Ustalam równe wagi dla obserwacji. Ustalam ilo iteracji m. Ustalam Learning Rate Buduje pierwszy model For t = 1 to n: Buduje model na predykcjach z poprzedniego modelu. Wyliczam wag modelu proporcjonaln do jego dokadnoci: \\(\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-total.error}{totl.error})\\) Wyliczam nowe wagi dla obserwacji. Wagi s wiksze dla elementów le zapredykowanych. Wagi zale te od wagi modelu. Mao wany model ma mniejsze wagi dla elementów le zapredykowanych. W tym miejscu mona doda parametr uczcy. Waga dla obserwacji le zaklasyfikowanych: \\(nowa.waga = stara.waga \\cdot e^{waga.poprzedniego.modelu}\\). Waga dla obserwacji dobrze zaklasyfikowanych : \\(nowa.waga = stara.waga \\cdot e^{- waga.poprzedniego.modelu}\\) Na koniec jest robiona pena predykcja gdzie modele z wszystkich iteracji, gosuj z si adekwatn do ich wagi. SUMME W AdaBoost M1 dla przypadku binarnego, waga modelu w t-ej iteracji zdefiniowana jako \\(\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-total.error}{totl.error})\\) przyjmuje warto 0 jeeli model jest losowy, czyli dostajemy 50% le zaklasyfikowanych elementów (jeeli model zaklasyfikowa poprawnie mniej ni 50% to po prostu odwracamy jego predykcje i dostajemy model lepszy od losowego). Jeeli jednak mamy wicej klas to próg losowoci musi by inaczej zrobiony i zaleny od iloci klas. Model gdzie jest 10 równolicznych klas i dobrze sklasyfikowa 50% obserwacji jest duo lepszy od modelu losowego. Dlatego wzór na wag modelu musi zosta skorygowany. SUMMER.R Tutaj wagi modeli dla kadej iteracji s liczone w oparciu o prawdopodobiestwa przynalenoci do klas. Kady model ma inna wag dla kadej z klas. Nie jest tak jak w standardowym AdaBoost e jest jedna waga dla modelu. Wagi obserwacji te s liczone w oparciu o te prawdopodobiestwa. Przy tych wagach uwzgldniamy te faktyczne wartoci empiryczne targetu. eby policzy wag t-ego modelu dla k-tej klasy najpierw bierzemy obserwacje w tej klasy (przynaleno do klasy wynika z danych empirycznych, a nie jest estymowana z modelu). Dla tych obserwacji liczymy REDNIE wyestymowane z modelu prawdopodobiestwo przynaleenia obserwacji do tej klasy. Przy urednianiu prawdopodobiestwa powinny chyba powinny by uywane wagi obserwacji. Dla danej klasy jest tym wiksza waga modelu in wysze jest prawdopodobiestwo przynaleenia tej klasy wedug modelu. W modelu decyzja o klasyfikacji i-tej obserwacji jest podejmowana na podstawie wyboru klasy dla której suma wago modelu po wszystkich iteracjach jest najwiksza (pamitajmy e wagi modeli s per klasa). Algorytm SUMME.R daje wyniki zbiene do addytywnej regresji logistycznej. Jest uwaany za bardziej efektywny ni klasyczne wersja AdaBoosta NIE oparta na prawdopodobiestwach. AdaBoost w R od scratch-a: link:rpubs Assumptions Quality Data: Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality. Outliers: Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset. Noisy Data: Noisy data, specifically noise in the output variable can be problematic. If possible, attempt to isolate and clean these from your training dataset. Pros Cons Boosting technique learns progressively, it is important to ensure that you have quality data. AdaBoost is also extremely sensitive to Noisy data and outliers so if you do plan to use AdaBoost then it is highly recommended to eliminate them. AdaBoost has also been proven to be slower than XGBoost. 5.11.2.2 Gradient Boost W przeciwiestwie do AdaBoosta uywa si tutaj wikszych drzew (najczciej majcych od 8 do 32 lici). Dobre opracowanie: link . Pseudoalgorytm (mój intuicyjny): Wybieramy funkcj straty L bd c nasz funkcj celu któr bdziemy minimalizowa. Dla zagadnienia regresji najczciej jest to funkcja Mean Squared Error (MSE): \\(\\frac{1}{2}\\sum{(y_i-\\hat{y_i})^2}\\) oraz Mean Absolute Error (MAE): \\(\\sum{|y-\\hat{y}|}\\) Natomiast dla zagadnienia klasyfikacji binarnej jest to funkcja log(likelihood) oparta o rozkad Bernulliego: \\(\\sum_{i=1}^n y_i \\log p(x_i) + (1  y_i) \\log (1  p(x_i))\\) Inne ciekawe funkcje straty link:towardsdatascience Nastpnie ustalamy inne hiperparametry, jak warunki stopu, parametry dla weak learnes itp. Dla pierwszej iteracji musimy zbudowa wartoci \\(\\hat{y}\\) które bd punktem wyjcia. Poniewa jest to punkt startowy nie mam tutaj jeszcze oszacowanego modelu. Aby uzyska wyjciowe wartoci \\(\\hat{y}\\) bez budowania modelu zakadamy, e bd one identyczne dla wszystkich obserwacji stanowic nasz jedn niewiadom. To pojedyncz niewiadom szukamy jej wstawienie do równania które minimalizujemy: \\(\\underset{\\gamma}{argmin}=\\sum{L(y_i, \\gamma)}\\), gdzie L to funkcja straty. Tak wic jeeli funkcja jest róniczkowalna liczy jej pochodn, przyrównujemy do zera i szukamy naszej pojedynczej niewiadomej \\(\\gamma\\), któr potem potraktujemy jako nasze \\(\\hat{y}\\). For i in t (t - kolejna iteracja): Musz teraz wyznaczy wartoci w oparciu o której buduje model. Nie bierzemy tutaj czystych wartoci \\(y_i\\). Istot algorytmy jest aby predykowane wartoci które potem dodajemy do wartoci wyjciowych (dla pierwszej iteracji ta warto wyjciowa to warto wyliczona w punkcie 2, a dla kolejnych to ta wartoci plus skumulowane predykcje z kolejnych modeli), dodawa takie wartoci które podaj za gradientem funkcji celu. Przykadowo w przypadku funkcji celu MSE gradient sprowadza si do reszt \\(y_i-\\hat{y_i}\\). Dla funkcji log(likelihood) opartego o rozkad Bernulliego to bd z kolei rónice wartoci empirycznych {0,1} i prawdopodobiestw wyliczonych przez przeksztacenie log(odds). Poniewa jak wida, dla najpopularniejszych funkcji celu, gradient redukuje si do jakiej formy reszt, czsto nazywa si do pseudo-resztami. Dodatkowo wartoci oparte o pseudoreszy s dodawana w skumulowany sposób tak aby zminimalizowa rónice wartoci empirycznych i predykowanych, co kade je te traktowa jako skadowe reszt naszego caego modelu (strong learner-a). Buduj model weak learner w oparciu o wartoci pseudo-reszt wyznaczone w punkcie 1. Tutaj warto doda, e powstae drzewa si zazwyczaj przycina. Teraz wartoci predykcyjne (pseudo-reszty) z uzyskane w weak lernar-a dodatkowo przeksztacam, gównie eby ograniczy zjawisko overfittingu, ale równie po to eby wartoci z predykcji byy przeksztaci tak aby ich definicja bya spójna z wartociami z poprzednich iteracji, dziki czemu moemy dokona agregacji nowych wartoci z poprzednimi. Jeeli nasze weak learner jest drzewem to: Stosujemy procedur z punktu pierwszego, ale teraz indywidualnie do kadego licia. W ten sposób dla wielu wartoci w kadego licia dostajemy jedn bardziej syntetyczn odszumin warto predykcji. Dodatkowo w przypadku log(likelihood) opartego o rozkad Bernulliego , dostajemy przeksztacenie predykowanych przez model prawdopodobiestwa na log(odds), któr moemy doda do wartoci uzyskanych z sumowania wyników z poprzednich iteracji. W przypadku regresji i funkcji straty MSE obliczalimy reszty które bez problemu moemy doda do wartoci z poprzednich iteracji. Uzyskane wartoci, zanim dodamy do zsumowanych wartoci z poprzednich iteracji, mnoymy przez *learning rate\" aby zapobiega szybkiemu przetrenowaniu. Pseudoalgorytm: Miejsca gdzie GradienBoost moe optymalizowany. weak learners, zwaszcza chodzi tutaj o szybsze trenowanie drzew. problem wycieku informacji, polegajcy na tym e model w punkcie 3.2 algorytmu predykuje na wartociach na których by trenowany Wybór innej funkcji celu prepoces danych zwaszcza danych jakociowych, gównie pod ktem przypieszenia oblicze. Jest wiele propozycji algorytmów które optymalizacji tych obszarów. Najpopularniejsze to: xgboost catboost lgbm 5.11.2.3 XGBoost Oryginalna praca na temat Xgboosting-u: link (XGBoost: A Scalable Tree, by Tianqi Chen and Carlos Guestrin) Tutaj w przypadku funkcji celu w stosunku do GradienBoost-a stosujemy dodatkowo regularyzacj L1. Ponadto metoda ma swój specyficzny sposób budowy drzew. Zamiast wspóczynnika Gini-ego do badania optymalnoci podziau wza uywa si miary similarity. Uwaga: miara similarity zostaa wprowadzone w celu przyspieszenia oblicze. Dzieje si to w ten sposób e jest ona powizana w wartociami output z lici (wartoci które finalnie s agregowane w caym modelu). similarity to maksimum funkcji celu ze wzgldu na output z lici (zmienn s róne moliwe wartoci output-u podstawiane pod funkcj celu). To powizanie powoduje e po ostatecznym splicie majc policzone similarity moe szybko policzy output. Wyprowadzenie similarity jest w fimiku ze StatQuest na temat Xgboost (cz 3, 20:00). W takcie analizy podziau, dodatkowo jest wyliczany parametr cover, który musi mie odpowiednie wartoci aby zaakceptowa istnienie danego podziau wza. Jest on równy dla klasyfikacji binarnej mianownikowi similarity bez parametru \\(\\lambda\\). Wic mam tutaj wbudowane co w rodzaju hamowania rozrostu drzewa. Niezalenie od tego robi ci tutaj przycinanie, ale oparte o poprawi wartoci similarity (przypomnijmy, e ta miara jest uywana zamiast Gini). W przypadku duych zbiorów danych szukanie optymalnego punktu odcicia jest oparte o waone kwantylowe histogramy. Najpierw way si obserwacje w oparciu o wspomniany wczeniej parametr cover (tutaj mamy jego drugie zastosowanie). Tylko tutaj mamy warto per obserwacja, a wczeniej wartoci per obserwacja agregowalimy dla caego licia. Nastpnie buduje si kwantyle tak, aby w kadym kwantylu bya równa ilo obserwacji, ale w uwzgldnieniem wag. To powoduje, e jest tendencja do grupowania w kwantylach oddzielnie obserwacji które maj niskie i oddzielnie wysokie cover. Wysoki cover jest przypisany obserwacj które maj nisk ufno predykcji, czyli w np. przypadku zagadnienia binarnego model predykuje dla nich p równe ok 0.5. Obliczania waonych kwantylowych histogramów jest zaimplementowane tak aby byo moliwe obliczenia równolege. Algorytm ten dodatkowo ma zaimplementowany mechanizm raczenia sobie z brakami danych. Standardowo w GradienBoosting-u braki po prostu s nie brane pod uwag przy trenowaniu drzewa. W Xgboost wyglda to tak: Podsumowanie parametrów similarity i output dla regresji i klasyfikacji zwizanych z XGboost: 5.11.2.4 CatBoost Wicej o catboost: link:towardsdatascience W przeciwiestwie do Gradient Boosta tutaj: Domylnie buduje si symetryczne drzewa. These are trees the same features are responsible in splitting learning instances into the left and the right partitions for each level of the tree. Symetryczne drzewa daj nastpujce korzyci: Regularization: Since we are restricting the tree building process to have only one feature split per level, we are essentially reducing the complexity of the algorithm and thereby regularization. Computational Performance: One of the most time consuming part of any tree-based algorithm is the search for the optimal split at each nodes. But because we are restricting the features split per level to one, we only have to search for a single feature split instead of k splits, where k is the number of nodes in the level. Even during inference these trees make it lightning fast. It was shown to be 8X faster than XGBoost in inference. Aby zapobiec problemowy wyciekania informacji w trenowaniu drzew zastosowano *Ordered boosting* . Najpierw permutujemy zbiór. Permutacja wprowadza losowo dodatkowo zabezpieczajc przed overfittingiem. Nastpnie robimy log(m) (gdzie m to ilo obserwacji) podziaów danych na zbiór trenujcy i uczcy tak aby zbiory te byy rozczne. Na zbiorach trenujcych trenujemy a na testowych predykujemy. Jest moliwo automatycznego enkodowania danych jakociowych. Wtedy uywamy algorytmu: We replace a categorical value by the mean of all the targets for the training samples with the same categorical value. For example, we have a Categorical value called weather, which has four values  sunny, rainy, cloudy, and snow. The most naive method is something called Greedy Target Statistics, where we replace sunny with the average of the target value for all the training samples where weather was sunny. If M is the categorical feature we are encoding and \\(m_i\\) is the specific value in M, and n is the number of training samples with \\(M = m_i\\). \\(Geedy TS_{M=m_i} = \\frac{\\sum_{i}^{n}(y_i)}{n} \\quad For \\quad all \\quad M=m_i\\) But this is unstable when the number of samples with \\(m_i\\) is too low or zero. Therefore we use the Laplace Smoothing used in Naive Bayes Classifier to make the statistics much more robust. \\(Geedy TS_{M=m_i} = \\frac{\\sum_{i}^{n}(y_i)+ap}{n+a} \\quad For \\quad all \\quad M=m_i\\) where a &gt; 0 is a parameter. A common setting for p (prior) is the average target value in the dataset. Jest zaimplementowany dektor overfittingu: Another interesting feature in CatBoost is the inbuilt Overfitting Detector. CatBoost can stop training earlier than the number of iterations we set, if it detects overfitting. there are two overfitting detectors implemented in CatBoost  IncToDec Iter Iteris the equivalent of early stopping where the algorithm waits for n iterations since an improvement in validation loss value before stopping the iterations IncToDecis more slightly involved. It takes a slightly complicated route by keeping track of the improvement of the metric iteration after iteration and also smooths the progression using an approach similar to exponential smoothing and sets a threshold to stop training whenever that smoothed value falls below it. Jest zimplementowane raczenie sobie z brakami w danych: If you select Min, the missing values are processed as the minimum value for the feature. And if you select Max, the missing values are processed as the maximum value for the feature. In both cases, it is guaranteed that the split between missing values and others are considered in every tree split. Dokumentacja pod Python : link 5.11.2.5 LightGBM towardsdatascience W stosunku do podstawowego GradienBoosta: Budujemy drzewa matod life-wise, zamist level-wise. In LightGBM, the leaf-wise tree growth finds the leaves which will reduce the loss the maximum, and split only that leaf and not bother with the rest of the leaves in the same level. This results in an asymmetrical tree where subsequent splitting can very well happen only on one side of the tree. Leaf-wise tree growth strategy tend to achieve lower loss as compared to the level-wise growth strategy, but it also tends to overfit, especially small datasets. So small datasets, the level-wise growth acts like a regularization to restrict the complexity of the tree, where as the leaf-wise growth tends to be greedy. Zmienne przeksztacamy metod Exclusive Feature Bundling (EBF). Przykad w pliku Excel w materials. Aby zapobiec problemowy wyciekania informacji w trenowaniu drzew zastosowano Gradient-based One-Side Sampling (GOSS). Tutaj sortujemy pseudoreszty. Wypieramy top x % najwikszych reszt. Potem z pozostaych (niewybranych) dolosowujemy próbk. Oba zbiory czymy i dostajemy zbiór do trenowania modelu. Predykcja jest potem robiona na wszystkich obserwacjach. Poniewa nie wszystkie predykowane wartoci s w zbiorze treningowym, to ograniczamy wyciek informacji. Dodatkowo due pseud-reszty s liczniej reprezentowane w zbiorze uczcym co przypiesza uczenie (jest to rodzaj downsamplingu). Pros Cons It is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows. Split z uyciem histogramu Split finding algorithms are used to find candidate splits. One of the most popular split finding algorithm is the Pre-sorted algorithm which enumerates all possible split points on pre-sorted values. This method is simple but highly inefficient in terms of computation power and memory . The second method is the Histogram based algorithm which buckets continuous features into discrete bins to construct feature histograms during training. It costs O(#data * #feature) for histogram building and O(#bin * #feature) for split point finding. As bin &lt;&lt; data histogram building will dominate the computational complexity. GOSS (Gradient Based One Side Sampling) GOSS (Gradient Based One Side Sampling) is a novel sampling method which down samples the instances on basis of gradients. As we know instances with small gradients are well trained (small training error) and those with large gradients are under trained. A naive approach to downsample is to discard instances with small gradients by solely focussing on instances with large gradients but this would alter the data distribution. In a nutshell GOSS retains instances with large gradients while performing random sampling on instances with small gradients. Intuitive steps for GOSS calculation: 1. Sort the instances according to absolute gradients in a descending order 2. Select the top a * 100% instances. [ Under trained / large gradients ] 3. Randomly samples b * 100% instances from the rest of the data. This will reduce the contribution of well trained examples by a factor of b ( b &lt; 1 ) 4. Without point 3 count of samples having small gradients would be 1-a ( currently it is b ). In order to maintain the original distribution LightGBM amplifies the contribution of samples having small gradients by a constant (1-a)/b to put more focus on the under-trained instances. This puts more focus on the under trained instances without changing the data distribution by much. Dobieranie parametrów link If you need to speed up the things faster: Assign small values to max_bin. Make use of bagging by bagging fraction and bagging frequency. By setting feature_fraction use feature sub-sampling. To speed up data loading in the future make use of save_binary. If you want to good accuracy: With a big value of num_iterations make use of small learning_rate. Assign large values to max_bin. Assign big value to num_leaves. Your training data should be bigger in size. Make use of categorical features directly. If you want to deal with overfitting of the model Assign small values to max_bin and num_leaves. Make use of a large volume of training data. Make use of max_depth so as to avoid deep trees. Make use of bagging by setting bagging_fraction and bagging_freq. By setting feature_fraction use feature sub-sampling. Make use of l1 and l2 &amp; min_gain_to_split to regularization. link:tword_data_sience 5.11.3 Stacking 5.11.4 Twicing 5.11.5 Bandling 5.12 Neural Networks 5.12.1 Introduction 5.12.2 Basics 5.12.3 Reccurent 5.12.3.1 Simple reccurent 5.12.3.2 Bidirectorial 5.12.3.3 LSTM 5.12.3.4 GRU 5.12.3.5 Attention 5.12.4 CNN 5.12.5 Resnet 5.13 Stochastic processes 5.13.1 Basic trend models 5.13.2 Basic adaptative models 5.13.3 Econometric time series models 5.13.3.1 dynamic (for example error correction models) 5.13.3.2 SARIMAX 5.13.3.3 VARIMAX 5.13.3.4 ARCH class models 5.13.3.5 Cointegration (including ARLD approach) 5.13.4 Time series decomposition decomposition 5.13.5 Kalman filters 5.13.6 Neural Networks 5.13.6.1 Long short term memory 5.13.6.2 CNN 5.13.7 Panel Regression 5.13.8 Gaussian Process Linki: Gaussian processes for time-series modelling understanding-gaussian-process-the-socratic-way Intuicja jest nastpujca. Wielowymiarowy rozkad normalny jest okrelony przez macierz kowariancji i wektor wartoci oczekiwanych. Jednak co jeeli wymiarów jest duo albo nieskoczenie wiele. Wtedy uywanie macierzy jest nieporczna, albo niemoliwe. Uogólnieniem rozkadu normalnego na nieskoczenie-wymiarow przestrze s procesy gaussowskie (gaussian process). Tutaj warto oczekiwana i korelacje s funkcjami. Funkcje korelacji s wyraane przez tzw. jdra (kernels). Po podstawieniu dwóch wartoci (czyli wybieramy 2 wymiary) dostajemy korelacje dla dwóch rozkadów normalnych. Nieskoczenie wymiarowy rozkad uytecznie jest zrzutowa na 2 wymiary. W tym przypadku kady punkt na osi x bdzie odpowiada jednemu wymiarowy. Przykadowo poniej punkt w rozkady normalnego dwuwymiarowego zrzutowano w taki sposób: Poniej mamy wicej wymiarów i rzutujemy cae rozkady: Wymiarów moe by nieprzeliczalna ilo. Predykcja Wykonuje si j warunkowo: Powyej mamy dany zestaw obserwacji y2. Chcemy na jego odstawie zapredykowa y1. Predykcja bdzie zalee od tego jaki kernele opisujce korelacje przyjmiemy. Wystpuj pewne podstawowe rodzaje kerneli (patrz dalej) które moemy czy rónymi tranformacjami. Takie podstawowe kernele maj pewne parametry który mona dobra tak aby z jak najwikszym prawdopodobiestwem przewidywa nasze punkty empiryczne (y1). Wtedy moemy wykona predykcje dla innych wartoci. Kernele link White noise kernel \\[ k(x, x) = \\sigma^2 I_n \\] Exponentiated quadratic kernel \\[ k(x_a, x_b) = \\sigma^2 \\exp \\left(-\\frac{ \\left\\Vert x_a - x_b \\right\\Vert^2}{2\\ell^2}\\right) \\] 2 the overall variance ( is also known as amplitude).  the lengthscale. Rational quadratic kernel \\[ k(x_a, x_b) = \\sigma^2 \\left( 1 + \\frac{ \\left\\Vert x_a - x_b \\right\\Vert^2}{2 \\alpha \\ell^2} \\right)^{-\\alpha} \\] 2 the overall variance ( is also known as amplitude).  the lengthscale.  the scale-mixture ( &gt; 0). Periodic kernel \\[ k(x_a, x_b) = \\sigma^2 \\exp \\left(-\\frac{2}{\\ell^2}\\sin^2 \\left( \\pi \\frac{\\lvert x_a - x_b \\rvert}{p}\\right) \\right) \\] 2 the overall variance ( is also known as amplitude).  the lengthscale. p the period, which is the distance between repetitions. Local periodic kernel The local periodic kernel is a multiplication of the periodic kernel with the exponentiated quadratic kernel to allow the periods to vary over longer distances. Note that the variance parameters 2 are combined into one. \\[ k(x_a, x_b) = \\sigma^2 \\exp \\left(-\\frac{2}{\\ell_p^2}\\sin^2 \\left( \\pi \\frac{\\lvert x_a - x_b \\rvert}{p}\\right) \\right)\\exp \\left(-\\frac{ \\left\\Vert x_a - x_b \\right\\Vert^2}{2\\ell_{eq}^2}\\right) \\] 2 the overall variance ( is also known as amplitude). p lengthscale of the periodic function. p the period. eq the lengthscale of the exponentiated quadratic. Inne zagadnienia Porównanie Gaussian Process Regression i Kernel Ridge Regression (link) 5.13.9 Ensembled models 5.13.10 Martingales 5.13.11 Markov Process 5.13.12 Winer Process 5.14 Results diagnostics 5.14.1 Classification 5.14.1.1 Measures 5.14.1.1.1 Negative log likelihood Well, to calculate the likelihood we have to use the probabilities. To continue with the example above, imagine for some input we got the following probabilities: [0.1, 0.3, 0.5, 0.1], 4 possible classes. If the true answer would be the forth class, as a vector [0, 0, 0, 1], the likelihood of the current state of the model producing the input is: 0*0.3 + 0*0.1 + 0*0.5 + 1*0.1 = 0.1. NLL: -ln(0.1) = 2.3 Can I use it for binary classification? Yes, of course, but usually frameworks have its own binary classification loss functions. Can I use it for multi-label classification? Yes, you can. Take a look on this article about the different ways to name cross entropy loss. Hold on! cross entropy loss. Whats that? From wikipedia: 5.14.1.1.2 Cross entrophy Dobre przykady: link true labels = [1,0], [0,1], [0,1], [0,1], [0,1] # do ktorej z dwoch klas faktycznie nalezy obserwacja predicted = [0.1, 0.9], [.5, .5], [.1, .9], [.1, .9], [.2, .8] # prawdopodobienstwo z modelu CE = -[ ln(.1) + ln(0.5) + ln(0.9) + ln(0.9) + ln(0.8)] = 3.4 5.14.1.2 Scores calibration 5.14.1.2.1 Problem Kalibracja dotyczy prawdopodobiestwa które nie odpowiadaj poziomom ufnosci miara (scores) z modeli które nia sa prawdopodobiestwami (SVM zwraca score jako odleglosc obserwacji o hiperplaszczyzny separujacej, a w K-NN mozemy budowac miary oparte o odleglosci miedzy obserwacjami - wiecej w k-NN dla klasyfikacji) ale chcemy, aby te scory byly przerobione na prawdopodobienstwa nie wiem co z przypadkiem kiedy mamy same labels z modelu i czy mona je przeksztaca na prawdopodobiestwo. Jednak takie modele sa rzadkoscia: Nearly every classifier - ogistic regression, a neural net, a decision tree, a k-NN classifier, a support vector machine, etc.  can produce a score instead of (or in addition to) a class label.1 5.14.1.2.2 Kalibracja a problemy konkretnych modeli Random Forest: RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately 0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana 1: Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting. As a result, the calibration curve also referred to as the reliability diagram (Wilks 1995 2) shows a characteristic sigmoid shape, indicating that the classifier could trust its intuition more and return probabilities closer to 0 or 1 typically. LogisticRegression: Returns well calibrated predictions by default as it directly optimizes Log loss. In contrast, the other methods return biased probabilities; with different biases per method: GaussianNB: Tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features. Linear Support Vector Classification (LinearSVC): shows an even more sigmoid curve than RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana 1), which focus on difficult to classify samples that are close to the decision boundary (the support vectors). 5.14.1.2.3 Calibration curve (reliability diagram) how to make it First, the forecast values are partitioned into bins Bk, k = 1, . . . , K (which form a partition of the unit interval into nonoverlapping exhaustive subintervals). The Bk are often taken to be of equal width, but if the distribution of the forecast values is nonuniform, then choosing the bins so that they are equally populated is an attractive alternative. Next, for each i, it is established which of the K bins the forecast value Xi falls into. For each bin Bk, let Ik be the collection of all indices i for which Xi falls into bin Bk; that is, \\(I_k:=\\{i;X_i \\in B_k\\}\\) The corresponding observed relative frequency fk is the number of times the event happens, given that Xi  Bk, divided by the total number of forecast values Xi  Bk. This can be expressed as: \\(f_k=\\frac{\\sum_{i \\in I_k}^{}{Y_i}}{\\#I_k}\\) where #Ik denotes the number of elements in Ik. Each bin Bk is represented by a single typical forecast probability rk. Although the arithmetic center of the bin is often used to represent the forecast values in that bin, this method has a clear disadvantage: If the forecast is reliable, the observed relative frequency for a given bin Bk is expected to coincide with the average of the forecast values over that bin Bk, rather than with the arithmetic center of the bin. Plotting the observed relative frequency over the arithmetic center can cause even a perfect reliability diagram to be off the diagonal by up to half the width of a bin. In this paper, observed relative frequencies for a bin Bk are plotted versus the average of the forecast values over bin Bk. This average, denoted by rk, is: \\(r_k:=\\frac{\\sum_{i \\in I_k}{X_i}}{\\#I_k}\\) The reliability diagram comprises a plot of \\(f_k\\) versus rk for all bins \\(B_k\\). 5.14.1.2.4 Skalowanie Platta link Steps for applying Platt scaling Split the data set into train and test data set. Train the model on the training data set. Apply SGD (stochstic gradient descent) Classifier to minimize hinge loss. Apply Calibrated Classifier from sklearn and take SGD classifier as a base estimator. Sort the predicted probability scores in ascending order. Divide the sorted probability and actual y into multiple bins. Here, we are taking bin size as 50. Take the average of actual y and predicted probabilities for each bins. Plot average of actual y on y-axis and average of predicted probability on x-axis. Pros: Works well with a small dataset Cons: Could produce worse probabilities calibration wise if the assumptions do not hold Skalowanie dla zagadnienia multiklasowego platts-scaling-for-multi-label-classification There are a few multiclass variants of Platt scaling. The easiest approach is as you have described; simply perform one Platt scaling on each class. However, there are more sophisticated optionsa very simple one to implement is training a standard logistic regression on the logits (the values before the softmax activation is applied). This has called matrix scaling and can overfit pretty easily, so only use this if you have a large calibration set. Alternatively, a fewer-parameter version called vector scaling is relatively simple to implement, where the weights matrix inside the logistic regression is restricted to be a diagonal matrix. Finally, a very simple option that has been shown to work well for neural networks is temperature scaling, where all logits are simply scaled by a single scalar parameter. You can read more about these and their application to neural networks in Section 4.2 of On Calibration of Modern Neural Networks (2017) - available here 5.14.1.2.5 Regresja izotoniczna link Pros: Makes no assumption about the input probabilities. A benefit of isotonic regression is that it is not constrained by any functional form, such as the linearity imposed by linear regression, as long as the function is monotonic increasing. Cons: Requires more data points to work well 5.14.1.2.6 Calibration in sklearn There are 2 ways of using the sklearn CalibratedClassifierCV class : Pass a fitted model and thereby setting cv to prefit. It is important to note that the data used in fitting the base estimator and the calibrator is disjoint. Fit a base estimator using k-fold cross-validation and the probabilities for each of the folds are then averaged for prediction. 5.14.2 Regression 5.15 Elements selection 5.15.1 Feature selection 5.15.1.0.1 Feature Importance Przyklady w Pythonie: link MDI MDA Mean Decrease Accuracy, MDA, also known as permutation importance. The approach can be described in the following steps: 1. Train the baseline model and record the score (accuracy/R²/any metric of importance) by passing the validation set (or OOB set in case of Random Forest). This can also be done on the training set, at the cost of sacrificing information about generalization. 2. Re-shuffle values from one feature in the selected dataset, pass the dataset to the model again to obtain predictions and calculate the metric for this modified dataset. The feature importance is the difference between the benchmark score and the one from the modified (permuted) dataset. 5.15.1.0.2 Boruta nice explanation from tworddatascience link 5.15.2 Variables exogenity 5.15.2.1 Granger Exogenity 5.16 IML 5.16.1 Partial Dependence Plot link link Pseudoalgorytm (mój intuicyjny - dla analizy jednej zmiennej) Wybieram zmienn która mnie interesuje do analizy. Dla kadej wartoci ustalonej tej zmiennej wyliczam predykcje modelu (mojego wytrenowanego black boxa) podstawiajc wszystkie moliwe kombinacje wartoci pozostaych zmiennych (tych których nie badam). Nastpnie dla kadej wartoci uredniam wyniki. Teraz moemy zrobi 2 wykresy. W obu wykresach na osi x mamy wartoci badanej zmiennej, a na osi y wartoci predykcji z modeli. W pierwszym wykresie umiecimy wartoci nie urednione. Tutaj wybierany okrelon kombinacj zmiennych nie analizowanych i dla tej kombinacji umieszczamy wszystkie wystpujce wartoci zmiennej analizowanej. Punkty czymy lini. Nastpnie powtarzamy to dla wszystkich moliwych kolejnych kombinacji. W ten sposób uzyskamy ICE plot. Dura opcja to po prostu wrzucenie na wykres kolejnych wartoci urednionych i poczenie lini. Wtedy dostaniemy klasyczny PDP plot. pierwotny zbiór danych: Zbiór danych po utworzeniu wszystkich moliwych kombinacji wartoci zmiennych niebadanych dla kadej wartoci zmiennej badane (zmienna badana to A1). Policzenie dla nowego zbioru predykcji i urednienie wyników dla kadej wartoci zmiennej badanej (A1): Utworzenie wykresów (ICE po lewej (na razie bez czenie punktów) i PDP po prawej): Pokazanie zalenoci midzy oboma wykresami: Pros: Cons: Wad rozwizania jest to e w trakcie tworzenia zbioru rónych kombinacji zmiennych nie analizowanych, mog powsta sztuczne obserwacje bardzo odlege od tego co jest w zbiorze danych. Dzieje si to w sytuacji kiedy zmienna analizowana jest mocno skorelowana z któr z pozostaych zmiennych. 5.16.2 M-plot M-plot jest teoretyczny prób rozwizania problemu PDP zwizanym z powstawaniem sztucznych obserwacji odlegych od pierwotnego zbioru. Rozwizanie mogoby polega na tym e kombinacje wartoci zmiennych dobieramy do wartoci zmiennej analizowanej, warunkowo w oparciu o prawdziwy rozkady warunkowy zmiennych pozostaych wzgldem zmiennej analizowanej. Poniej jest przykad rozkadu warunkowego zmiennej x2 wzgldem analizowanej zmiennej x1. To rozwizanie ma t wad. Zaómy e analizowana zmienna x1 jest skorelowana dodatnio (jak na rysunku powyej) ze zmienn x2. Dodatkowo zaómy e zmienna x2 ma wpyw na predykcje z modelu a zmienna analizowana x1 nie ma. W tym wypadku z analizy wyjdzie nam e poprzez korelacje ze zmienn x2 zmienna wbrew stanowi faktycznego bdzie miaa wpyw na wartoci predykowane. Po prostu dla coraz wyszych wartoci x1 bdziemy losowali coraz wyszej wartoci zmiennej x2, a zakadamy przecie e zmiany x2 maj wpyw na predykcj. 5.16.3 ALE - Accumulated Local Effects W pythonie do ALE jest pakiet Alibi. ALE próbuje rozwiza problem jaki ma wykres M-Plot. Tutaj podobnie jak w M-Plot losujemy wartoci zmiennych uwzgldniajc rozkad warunkowy wzgldem A1. Tutaj jednak s pewne modyfikacje. Wartoci zmiennych losujemy z rozkadu warunkowego dla okrelonego przedziau [a,b] wartoci zmiennej analizowanej. Wartoci zmiennych s zastpowane ich redni wartoci w tym przedziale. Nastpnie dla tych urednionych wartoci liczymy predykcje modelu dla wartoci zmiennej analizowanej z pocztku przedzia (a) i koca przedziau (b). Zatem warto zmiennej analizowanej si zmienia, a warto pozostaych zmiennych jest STAA dziki czemu eliminujemy problem wpywu korelacji miedzy zmienn analizowan i pozostaymi zmiennymi. 5.16.4 H-statistics link Jest to statystyka do badania interakcji midzy zmiennymi. Jest to warto wyliczana w oparciu o wartoci jakie uzyskujemy przy obliczeniach do PDP. Jeeli midzy dwoma zmiennymi nie ma interakcji to moemy zaoy e: \\(PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\\) Czyli suma wartoci wyliczonych dla DPD dla kadej zmiennej z osobna jest taka sama jak w sytuacji kiedy analizujemy dwie zmienne na raz. Jeeli ta addytywno nie jest spenione to jest podejrzenie interaktywnoci. Pseudoalgorytm: obliczam wartoci PD przy analizowaniu kilku zmiennych na raz (wariant 1) i wartoci przy analizie pojedynczych zmiennych (wariant 2). Obliczam wariancje rónicy PD w obu wariantach. Badam jaka cz wariancji PD jest objaniona poprzez rónice midzy wariantami. Statystyka jest obliczana ze wzoru: \\(H^2_{jk}=\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})\\right]^2/\\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})\\) W liczniku mamy wariancje PD w rónych wariantach, a w mianowniku ca wariancj w wariancie 1 który jest wariantem bazowym. 5.16.5 LIME (Local surrogate) link link Jest to modelowanie lokalne per obserwacja. Pseudoalgorytm (mój intuicyjny dla danych tabular data): Wybieram interesujc mnie obserwacj. Z rozkadu (najczciej normalnego, z wartoci oczekiwan pokrywaj si z wartoci zmiennych dla interesujcego nas punktu) , dla zmiennych objaniajcych (features) losuje obserwacje. Po wylosowaniu obserwacji ustalam metryke aby zway wylosowane obserwacje. Te które s bliej intersujcego nas punktu wa wicej. Dla waonych obserwacji buduje model który jest nie jest black-boxem (model atwy w interpretacji). Moe to by np. model regresji liniowej. Parametry mojego modelu traktuje jako przyblion wskazówk jaki wpyw na warto predykcji dla mojej obserwacji maj konkretne zmienne objaniajce Pseudoalgorytm (mój intuicyjny dla danych tekstowych): zakadam e mam klasyfikator okrelajcy czy dany komentarz jest spamem czy nie. Chcemy odczarowa nasz black box i zobaczy w tych komentarzach jaki wpyw na predykcje maj poszczególne wyrazy. Wybieram konkretn obserwacj (komentarze) Dla tego komentarze tworz róne jego warianty wyczajc róne kombinacje wyrazów. Poniej jest przykad komentarza For Christmas Song visit my channel: Wartoci 0 dla kolejnych wyrazów oznaczaj ich wyczanie w rónych wariantach: Dla rónych kombinacji wyliczam predykcje ( w tym przypadku prawdopodobiestwo w przedostatniej kolumnie powyszej tabeli) modelu. Dodatkowo wyliczane s wagi dla poszczególnych kombinacji definiowane jako: The weight column shows the proximity of the variation to the original sentence, calculated as 1 minus the proportion of words that were removed, for example if 1 out of 7 words was removed, the proximity is 1 - 1/7 = 0.86. Czyli im mniej wyrazów zostay wyrzuconych tym kombinacja jest bardziej podobna do oryginau i ma wiksz wag. Nastpnie wyliczam lokalne wagi poszczególnych sów dziki czemu uzyskuje interpretowalno. Bior kolejne obserwacje i analizuje kolejne wyrazy w tych obserwacjach (w poniszej w tabeli mam 2 komentarze i dla kadego z nich po 3 wyrazy. Pierwszy komentarz nie jest spamem (case1), a drugi jest (case2) ). Sowo channel w case 2 zwiksza prawdopodobiestwo e mamy spam o 6.18% (nie wiem jak to zostao wyliczone, ale wydaje mi si e bierzemy redni predykcji prawdopodobiestwa ze wszystkich kombinacje bez sowa channel i drug redni z kombinacjami ze sowem channel i licz rónice) . For the non-spam comment (case no non-zero weight was estimated, because no matter which word is removed, the predicted class remains the same. Presudoalgorytm (mój intuicyjny dla zdj): Wybieram konkretne zdjcia Tworz tzw. Megapixele. Mog to by przylegajce obszary o podobnym kolorze. Nastpnie tworz róne kombinacje obrazka wyczajc róne megapixele i sprawdzam jak wyglda klasyfikacja. Potem licz lokalne wagi chyba w sposób podobny do tego jak to jest robione przy danych tekstowych. Pros: Even if you replace the underlying machine learning model, you can still use the same local, interpretable model for explanation. Suppose the people looking at the explanations understand decision trees best. Because you use local surrogate models, you use decision trees as explanations without actually having to use a decision tree to make the predictions. For example, you can use a SVM. And if it turns out that an xgboost model works better, you can replace the SVM and still use as decision tree to explain the predictions. Local surrogate models benefit from the literature and experience of training and interpreting interpretable models. When using Lasso or short trees, the resulting explanations are short (= selective) and possibly contrastive. Therefore, they make human-friendly explanations. This is why I see LIME more in applications where the recipient of the explanation is a lay person or someone with very little time. It is not sufficient for complete attributions, so I do not see LIME in compliance scenarios where you might be legally required to fully explain a prediction. Also for debugging machine learning models, it is useful to have all the reasons instead of a few. LIME is one of the few methods that works for tabular data, text and images. The fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest. LIME is implemented in Python (lime library) and R (lime package and iml package) and is very easy to use. The explanations created with local surrogate models can use other (interpretable) features than the original model was trained on.. Of course, these interpretable features must be derived from the data instances. A text classifier can rely on abstract word embeddings as features, but the explanation can be based on the presence or absence of words in a sentence. A regression model can rely on a non-interpretable transformation of some attributes, but the explanations can be created with the original attributes. For example, the regression model could be trained on components of a principal component analysis (PCA) of answers to a survey, but LIME might be trained on the original survey questions. Using interpretable features for LIME can be a big advantage over other methods, especially when the model was trained with non-interpretable features. Cons: The correct definition of the neighborhood is a very big, unsolved problem when using LIME with tabular data. In my opinion it is the biggest problem with LIME and the reason why I would recommend to use LIME only with great care. For each application you have to try different kernel settings and see for yourself if the explanations make sense. Unfortunately, this is the best advice I can give to find good kernel widths. Sampling could be improved in the current implementation of LIME. Data points are sampled from a Gaussian distribution, ignoring the correlation between features. This can lead to unlikely data points which can then be used to learn local explanation models. The complexity of the explanation model has to be defined in advance. This is just a small complaint, because in the end the user always has to define the compromise between fidelity and sparsity. Another really big problem is the instability of the explanations. In an article 38 the authors showed that the explanations of two very close points varied greatly in a simulated setting. Also, in my experience, if you repeat the sampling process, then the explantions that come out can be different. Instability means that it is difficult to trust the explanations, and you should be very critical. LIME explanations can be manipulated by the data scientist to hide biases 39. The possibility of manipulation makes it more difficult to trust explanations generated with LIME. Conclusion: Local surrogate models, with LIME as a concrete implementation, are very promising. But the method is still in development phase and many problems need to be solved before it can be safely applied.5.7.4 Advantages Even if you replace the underlying machine learning model, you can still use the same local, interpretable model for explanation. Suppose the people looking at the explanations understand decision trees best. Because you use local surrogate models, you use decision trees as explanations without actually having to use a decision tree to make the predictions. For example, you can use a SVM. And if it turns out that an xgboost model works better, you can replace the SVM and still use as decision tree to explain the predictions. Local surrogate models benefit from the literature and experience of training and interpreting interpretable models. When using Lasso or short trees, the resulting explanations are short (= selective) and possibly contrastive. Therefore, they make human-friendly explanations. This is why I see LIME more in applications where the recipient of the explanation is a lay person or someone with very little time. It is not sufficient for complete attributions, so I do not see LIME in compliance scenarios where you might be legally required to fully explain a prediction. Also for debugging machine learning models, it is useful to have all the reasons instead of a few. LIME is one of the few methods that works for tabular data, text and images. The fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest. LIME is implemented in Python (lime library) and R (lime package and iml package) and is very easy to use. The explanations created with local surrogate models can use other (interpretable) features than the original model was trained on.. Of course, these interpretable features must be derived from the data instances. A text classifier can rely on abstract word embeddings as features, but the explanation can be based on the presence or absence of words in a sentence. A regression model can rely on a non-interpretable transformation of some attributes, but the explanations can be created with the original attributes. For example, the regression model could be trained on components of a principal component analysis (PCA) of answers to a survey, but LIME might be trained on the original survey questions. Using interpretable features for LIME can be a big advantage over other methods, especially when the model was trained with non-interpretable features. 5.16.6 Shapley value Tak jak Lime jest wyliczany per obserwacja. Pseudoalgorytm (mój intuicyjny): Wyliczam redni predykcj dla caego zbioru. Nie to bdzie np. 100 Bior konkretn obserwacj i jej warto predykcji (np. 110). Chce zobaczy jaki wpyw maj poszczególne zmienne objaniajce na odchylenie predykcji dla obserwacji o redniej dla caego zbioru (110-100=10) Wybieram zmienn do analizy. Z pozostaych zmiennych tworz wszystkie moliwe koalicje. Zmienne w kolacji maj ustalone wartoci równe wartoci dla badanej obserwacji. Dla zmiennych spoza koalicji bdziemy robili losowania. Dla kadej kolacji robi 2 warianty. (1) Zmienna analizowana jest w koalicji, (2) zmienna analizowana nie jest w kolacji. Dla kadej koalicji robi losowania pozostaych zmiennych. Wyliczam redni z predykcji dla sytuacji kiedy zmienna analizowana jest w koalicji i jej nie ma. Nastpnie licz redni ze rednich po wszystkich koalicjach (oddzielnie dla sytuacji kiedy zmienna analizowana jest w koalicji i kiedy jej nie ma). Licz rónice midzy rednimi w dwóch wariantach. Ta rónica to kad w odchylenie wartoci predykcji od obserwacji od redniej z predykcji po wszystkich obserwacjach. Przykad interpretacji wyników Mamy redni predykcj dla wszystkich obserwacji 0.03. Dla konkretnej analizowanej obserwacji warto predykcji wynosi 0.57. Rónica midzy tymi wartociami to 0.54. Przykadowo zmienna wiek dla tej obserwacji ma warto 20 i jej wkad w rónic to ok 0.03. To co jest powyej mona adniej zaprezentowa jako force plot (poniej 2 przykady): Poniej wykres zbiorczy: Kade obserwacja ma tyle SHAP Values ile jest zmiennych. Mona to wrzuci na powyszy wykres. O Y to zmienne. Dla kadej zmiennej mamy SHAP po wszystkich obserwacjach. SHAP odczytujemy z osi X. Warto danej zmiennej na danej obserwacji odczytujemy po kolorach (od czerwonego do niebieskiego). Dodatkowo moemy zrobi co w rodzaju klastrowania obserwacji. Bierzemy wszystkie force ploty odwracamy o 90 stopni i wrzucamy na wykres. Nastpnie je sortujemy uywajc metryki podobiestwa pomidzy tymi plotami. W ten sposób mona próbowa wyodrbni klastry obserwacji które s podobne do siebie (O y to warto predykcji a o x podobiestwo). 5.17 By problems 5.17.1 Numerical 5.17.2 Categorical 5.17.3 Text 5.17.4 Sound 5.17.5 Vision "],["learning-hybrid.html", "Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.3 Self-supervised learning 6.4 Mult-instance learning", " Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.2.1 EM 6.2.2 CPLE 6.2.3 SVM and TSVM 6.2.4 Graphs 6.2.5 Neural Networks 6.2.6 By problems 6.2.6.1 Numerical and categorical 6.2.6.2 Text data 6.2.6.3 Sound 6.2.6.4 Vision 6.3 Self-supervised learning 6.4 Mult-instance learning "],["learning-reinforcement.html", "Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems", " Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems 7.5.1 Numerical and categorical 7.5.2 Text data 7.5.3 Sound 7.5.4 Vision "],["data-preprocessing.html", "Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.3 Data Problems", " Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.2.1 discretization 8.2.2 numeric variable continuous tranformations 8.2.3 nominal variables tranformations 8.2.3.1 encoding 8.2.3.1.1 one-hot 8.2.3.1.2 embedding In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well. Embedding is type of categorical variables encoding. It can be also treated as a way for dimentionality reduction. 8.2.3.2 other 8.3 Data Problems 8.3.1 Numeric and categorical 8.3.1.1 Extreme values Trzeba pamita o jednym wanym rozrónieniu. Wartoci nietypowe jako rónice si od typowych obserwacji w zbiorze Wartoci na które jest wraliwa dana metoda (algorytm) w oparciu o któr budujemy model (nadzorowany lub nienadzorowany). W tym przypadku chyba tylko w przypadku klasycznych modeli ekonometrycznych s dobrze okrelone miary umoliwiajce wykrywanie wartoci na które model jest wraliwy. W pozostaych przypadkach trzeba po prostu przetestowa róne sposoby wykrywania outliersów w rozumieniu definicji z punktu 1 i zobaczy czy dale to wspyw na algorytm. Rodzaje algorytmów: Porównanie wyników rónych algorytmów: 8.3.1.1.1 LOF (local outlier factor) Jest to algorytm dziaajcy w oparciu o lokaln analiz gstoci punktów (idea podobna np. do DBSCAN). Opis algorytmu (mój intuicyjny): Ustalam parametr k okrelajcy k najbliszych ssiadów. Wyznaczmy punktu znajdujce si w otoczeniu analizowanego punktu. W tym celu szukam dla analizowanego punktu w jego otoczeniu k najbliszych ssiadów. Uwaga: jeeli szukamy k najbliszych ssiadów moemy finalnie mie w analizowanym otoczeniu punktu A wicej punktów ni k punktów. Poniszy rysunek pokazuje jak to jest moliwe. Szukamy tutaj 2 najbliszych ssiadów dla punktu A. Pierwszy najbliszy punkt to C. Nastpny to np. punkt B. Zatem mamy ju dwóch ssiadów. Jednak okazuje Punkty D ley w takiej samej odlegoci od A co punkt B. Zatem te go uwzgldniamy. Zatem bdziemy mie w analizie ssiedztwa punktu A, 3 a nie 2 punkty czyli liczb rón od k. Obliczamy REACHABILITY DENSITY (RD). Jeeli odlego punktu od punktu analizowanego jest w promieniu wyznaczonym przez najdalszego ssiada to RD równa si temu promieniowi. Jeeli jest poza promieniem to RD jest równe odlegoci pomidzy punktami. Popatrzmy na przykad: Powyej mamy Xj jako punkt dla którego analizujemy otoczenie. Nastpnie rozpatrujemy 2 moliwe pooenia punktu Xi. W pierwszym jest wewntrz koa wyznaczonego przez najdalszych ssiadów. Jego RD jest równe niebieskiej linii, czyli promienia okrgu wyznaczonego przez najdalszych ssiadów. W drugim przypadku jest poza koem i jego RD jest równe odlegoci od Xj (pomaraczowa linia). Nastpnie obliczmy LOCAL REACHABILITY DENSITY (LRD): Nk(A) to jest zbiór punktów z wyznaczonego otoczenia dla punktu A. W powyszym wzorze || || nie oznacza normy tylko liczebno zbioru. Tak wic ||Nk(A)|| to ilo punktów w wyznaczonym otoczeniu. Miara jest liczona dla kadego puntu (A) przy zadanym k. LRD is inverse of the average reachability distance of A from its neighbors. Intuitively according to LRD formula, more the average reachability distance (i.e., neighbors are far from the point), less density of points are present around a particular point. This tells how far a point is from the nearest cluster of points. Low values of LRD implies that the closest cluster is far from the point. Nastpnie obliczmy: LOCAL OUTLIER FACTOR (LOF). Powysza miara jest liczona dla kadego punktu. Pierwszy czon to suma warto LRD wyznaczonych dla punktów znajdujcych si w wyznaczonym otoczeniu punktu A, podzielona przez ilo tych punktów. Drugi czon to odwrotno LRD dla analizowanego punktu. LRD of each point is used to compare with the average LRD of its K neighbors. LOF is the ratio of the average LRD of the K neighbors of A to the LRD of A. Intuitively, if the point is not an outlier (inlier), the ratio of average LRD of neighbors is approximately equal to the LRD of a point (because the density of a point and its neighbors are roughly equal). In that case, LOF is nearly equal to 1. On the other hand, if the point is an outlier, the LRD of a point is less than the average LRD of neighbors. Then LOF value will be high. Generally, if LOF&gt; 1, it is considered as an outlier, but that is not always true. Lets say we know that we only have one outlier in the data, then we take the maximum LOF value among all the LOF values, and the point corresponding to the maximum LOF value will be considered as an outlier. Przykad obliczeniowy: We have points: A(0,0), B(1,0), C(1,1) and D(0,3) and K=2. We will use LOF to detect one outlier among these 4 points. Following the procedure discussed above: Najpierw obliczamy dla kadego punktu odlego od k-tego najdalszego ssiada. Dla nas k = 2 wic szukamy 2 najbardziej oddalonego ssiada. Uyjemy odlegoci Manhattan: K-distance(A) &gt; since C is the 2 nearest neighbor of A &gt; distance(A, C) =2 K-distance(B) &gt; since A, C are the 2 nearest neighbor of B &gt; distance(B,C) OR distance(B,A) = 1 K-distance(C) &gt; since A is the 2 nearest neighbor of C &gt; distance(C,A) =2 K-distance(D) &gt; since A,C are the 2 nearest neighbor of D &gt; distance(D,A) or distance(D,C) =3 Zatem wszystkie kombinacje odlegoci to: Poniej dla kadego punktu jest lista punktów w wyznaczonym otoczeniu oraz ich liczebno. K-neighborhood (A) = {B,C} , ||N2(A)|| =2 K-neighborhood (B) = {A,C}, ||N2(B)|| =2 K-neighborhood (C)= {B,A}, ||N2(C)|| =2 K-neighborhood (D) = {A,C}, ||N2(D)|| =2 K-distance, the distance between each pair of points, and K-neighborhood will be used to calculate LRD. Local reachability density (LRD) will be used to calculate the Local Outlier Factor (LOF). Highest LOF among the four points is LOF(D). Therefore, D is an outlier. Pros: A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. The global approach may not consider that point as an outlier. But the LOF can effectively identify the local outliers. Cons: Since LOF is a ratio, it is tough to interpret. There is no specific threshold value above which a point is defined as an outlier. The identification of an outlier is dependent on the problem and the user. 8.3.1.1.2 Angle Idea jest prosta. Bior punktu i sprawdzam pod jakim ktem wzgldem niego s inne punktu. Jeeli wikszo punktów na podobny kt wzgldem punktu analizowanego to punkt analizowane jest wartoci odstajc (lewy rysunek poniej). Jeeli kty s bardzo zrónicowane to punkt nie jest wartoci odstajc (prawy rysunek poniej) Usually in implementation, not just the angle but the distance between the point is also divided so that distance is also taken into account.( Nearby points may also have very less angle but might not be outlier) So angular distance= (AB,AC) - dot product of AB AB, AC - distance between A and B, A and C So cosine= (AB, AC)/AB*AC cosine /distances= (AB, AC)/(AB^2*AC^2) to calculate angle based outlier factor of A, variance of all possible cosine/distance is taken. Lower value means more outlier-ness. 8.3.1.1.3 knn Opis algorytmu (mój intuicyjny): Budujemy model k-nn Dla kadego punktu obliczamy redni odlego od jego k najbliszych ssiadów. Urednione odlegoci dla kadego punktu wrzucamy na wykres: Najwysze wartoci moemy traktowa jako wartoci odstajce. 8.3.1.1.4 One class SVM Tutaj mamy SVM z przeformuowanym problemem. Zamiast tworzy hiperpaszczyzn separujc róne klasy zakadamy e wszystkie elementy nale do jednej klasy, a my tworzymy jak najmniejsz sfer która bdzie zawieraa jak najwicej elementów. Zatem funkcja celu to: $min,r^2 ,,subject,to, ||(x_i)-c||2r2 $ Czy minimalizujemy kwadrat promienia sfery (\\(r^2\\)) i ustalamy c (rodek sfery) tak aby kwadrat odlegoci obserwacji od rodka (obserwacje mog by przetransformowane do wyszych wymiarów (kernel trick) i std \\(\\Phi()\\) ) by mniejszy ni kwadrat rednicy (wszystkie obserwacje wewntrz sfery). Aby nie wszystkie obserwacje znalazy si wewntrz koa, funkcja celu ma wprowadzone warunki agodzce (slack variables) umoliwiajce wystpowaniem obserwacji poza sfer. Mamy tutaj parametr który umoliwia sterowanie wpywem slack variables i tym samym moemy okrela jak duy procent elementów ma by poza sfer. 8.3.1.1.5 z-score Z-socre jest analiz jednowymiarow. Obliczamy go dla i-tej obserwacji ze wzoru : \\(zscore_i = \\frac{x_i-x\\_mean}{standard\\_deviation}\\) Wyraa ilu odchyleniom standardowym równa si odlego obserwacji od redniej. Wartoci o wysokim z-score moemy traktowa jako wartoci ostajce. Warto progowa to czsto wartoci z przedziau [2,3]. Pros: It is a very effective method if you can describe the values in the feature space with a gaussian distribution. (Parametric) The implementation is very easy using pandas and scipy.stats libraries. Cons: It is only convenient to use in a low dimensional feature space, in a small to medium sized dataset. Is not recommended when distributions can not be assumed to be parametric. 8.3.1.1.6 PCA PCA jest przykadem zastosowania redukcji wymiarowoci do wykrywania outliersów. Ogólna idea jest taka e algorytm redukujcy wymiar powinien uchwyci najwaniejsze elementy struktury danych. Elementy nieistotne dla tej struktury w wyniku redukcji powinny istotnie zmieni swoje pooenie. Jeeli moemy przeprowadzi rekonstrukcj (powrót po redukcji do pierwotnej przestrzeni), wtedy atwo zbudowa metryk porównujc dane przez i po redukcji (dziki redukcji porównanie w tej samej pierwotnej przestrzeni). Poniej jest pokazana idea redukcji i rekonstrukcji: Elementy które mocno zmieniy pooenie bd outliersami. Jednak w przypadku PCA ta analiza ma swoje wady. Popatrzmy poniej: Punkt 1 po redukcji mocno zmieni swoje pooenie i jest dobrym przykadem outliersa. Jednak punkt 2 te powinien by outliersem, jednak przy zaoeniu e stosujemy PCA nim nie bdzie. 8.3.1.1.7 Autoencoders To kolejny przykad jak algorytm do redukcji wymiary moe by uyteczny do wykrywania outliersów. Autoencoder kompresuje dane i potem stara si zrobi rekonstrukcj. Bd rekonstrukcji obserwacji okrela czy nadaje si na outliersa. Jeeli obserwacja jest mao widzenia z punktu widzenia struktury danych to informacja na jej temat bdzie utracona w czasie kompresji i przy rekonstrukcji bdziemy mie duy bd. 8.3.1.1.8 Elliptic Envelope filmik: Lecture 15.7  Anomaly Detection | Multivariate Gaussian Distribution  [ Andrew Ng ] Algorytm sprawdza si jeeli dane maj rozkad normalny. Dopasowujemy rozkad normalny do danych (czyli po prosty estymujemy macierz wariancji/kowariancji i wektor wartoci oczekiwanych) , a nastpnie na jego podstawie rysujemy odpowiedni elipsoid (musimy ustali jakie p-value): To co jest poza elipsoid traktuje jako warto odstajc: 8.3.1.1.9 COPOD Algorytm oparty o kopuy. Copua jest w pewnym sensie podobna co do zaoe do Elliptic Envelope. Ale tutaj zakadamy e nie znamy wielowymiarowe rozkadu naszych zmiennych. Jednak moemy zbudowa ten rozkad wanie poprzez kopu. Kiedy bdziemy mie taki rozkad elementy o niskim prawdopodobiestwie traktujemy jako outliersy. 8.3.1.1.10 Isolation Forest Szczegóowe omówienie: link Opis algorytmu (mój intuicyjny): Dokonuje podziau przestrzeni obserwacjami, a uzyskam odizolowanie kadego punktu tak jak na lewym rysunku poniej: Podzia mog oczywicie przedstawi w postaci drzewa jak jak rysunku powyej po prawej. Dla kadej obserwacji mog policzy ile splitów trzeba byo przeprowadzi eby j w peni odseparowa od reszty. Dla czerwonej obserwacji udao si to zrobi ju po pierwszym splicie. Podziay ze wzgldu na cechy wykonuje si losowe. Punkty 1 i 2 powtarzam n razy, dziki czemu uzyskuj du ilo drzew: Uwaga: W praktyce przy duych zbiorach danych aby usprawni prac algorytmu przy budowie poszczególnych drzew stosuje si nastpujce uproszenia: Kade kolejne drzewo jest robione na podpróbkach danych Dla kadego kolejnego drzewa losuje podzbiór zmiennych na których buduje drzewo. Zazwyczaj ogranicza si gboko drzewa (np. do ceil(log_2(n))), tak wic drzewo nie odizoluje kadego punktu. Na koniec obliczam dla kadej obserwacji score którego warto mieci si w przedziale [0,1]: \\(s(x,n) = 2 \\frac{- E(h(x))}{c(n)}\\) Wartoci bliskie 1 s traktowane jako wartoci odstajce. Pros: There is no need of scaling the values in the feature space. It is an effective method when value distributions can not be assumed. It has few parameters, this makes this method fairly robust and easy to optimize. Scikit-Learns implementation is easy to use and the documentation is superb. Cons: The Python implementation exists only in the development version of Sklearn. Visualizing results is complicated. If not correctly optimized, training time can be very long and computationally expensive. Kolejna wada objawia si przy niektórych rozkadach danych: Powyej oczekiwalibymy e warto scoru bdzie wzrasta równomiernie we wszystkich kierunkach w miar oddalania si od centrum zbioru (punkt (0,0)). Jednak po prawej (im janiej tym mniejszy score) utworzy si nam krzy i niskie wartoci mamy te np. na samym dole w rodku. Problem wnika z tego drzewo dokonuje tylko poziomych i pionowych podziaów. Sytuacja wyglda jeszcze gorzej przy takim zbiorze danych: Dlatego istnieje algorytm Extended Isolation Forest gdzie dokonuje si podziaów pod rónym ktem: 8.3.1.2 Missing values 8.3.1.3 Untipical distributions (for example copula models, kernel estimators, logaritmic transofmations ect., mixed distributions) 8.3.1.4 Censored/truncated data 8.3.1.5 Aggregated date (decomposition) 8.3.1.6 Meassurement error (for example Kalman filter model) 8.3.1.7 Granularity of data 8.3.1.8 Imbalanced categories Uwaga. W przypadku klasyfikacji imbalanced data i skewed data jest najczciej uywane zamiennie. 8.3.1.8.1 Non synthetic methods 8.3.1.8.1.1 Under-sampling Pros: Cons: under-sampling the majority can end up leaving out important instances that provide important differences between the two classes. 8.3.1.8.1.2 Over-sampling: Pros: Cons: the random oversampling may increase the likelihood of overfitting occurring, since it makes exact copies of the minority class examples. In this way, a symbolic classifier, for instance, might construct rules that are apparently accurate, but actually cove one replicated example.\"  Page 83, Learning from Imbalanced Data Sets, 2018 8.3.1.8.2 Synthetic methods Metody syntetyczne balansowania próby polegaj na tworzeniu nowych obserwacji dla kategorii mniej licznej. W porównaniu do metod niesyntetycznych mamy tutaj wiksze moliwo do dokadaniu informacji do zbioru (bo tworzymy nowe obserwacje). 8.3.1.8.2.1 SMOTE SMOTE - Synthetic Minority Oversampling Technique. Pseudo-algorytm (wersja podstawowa): Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space. Zalecane jest aby SMOTE wykonywa w kombinacji w undersamplingiem. code R over = SMOTE(sampling_strategy=0.1) under = RandomUnderSampler(sampling_strategy=0.5) steps = [(&#39;o&#39;, over), (&#39;u&#39;, under)] pipeline = Pipeline(steps=steps) # transform the dataset X, y = pipeline.fit_resample(X, y) Pros: Cons: Overgeneralization: SMOTEs procedure is inherently dangerous since it blindly generalizes the minority area without regard to the majority class. This strategy is particularly problematic in the case of highly skewed class distributions (Skewed classes basically refer to a dataset, wherein the number of training example belonging to one class out-numbers heavily the number of training examples beloning to the other. Consider a binary classification, where a cancerous patient is to be detected based on some features) since, in such cases, the minority class is very sparse with respect to the majority class, thus resulting in a greater chance of class mixture. Lack of Flexibility: The number of synthetic samples generated by SMOTE is fixed in advance, thus not allowing for any flexibility in the re-balancing rate. SMOTE wystpuje w wariantach z licznymi modyfikacjami. Najczciej polegaj one na tym eby tworzy nowe obserwacje w obszarach gdzie s bdy klasyfikacyjne. Przykadem moe by SMOTE-Borderline. Pseudoalgorym (wersja Borderline): A popular extension to SMOTE involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model. 8.3.1.9 Imbalanced values Odpowiednikiem zagadnienia niezbalansowanej próby w zagadnieniu klasyfikacyjnym dla zagadnienia regresyjnego moe by to problem inflacji zer (zero-inflation). Przykad dla modlu Possona z problemem zero-inflation link . 8.3.1.10 Small samples problem 8.3.2 Text 8.3.2.1 TF-IDF Szczegóowe wprowadzeni teoretyczne z przykadami liczbowymi: link 8.3.3 Visual 8.3.3.1 Wykrywanie krewedzi 8.3.3.1.1 Hough pis algoytmu: link 8.3.3.2 Wykrywanie elementów 8.3.3.2.1 Haar Cascade link Corners - w przeciwienstwie do lines sa bardziej unikatowe dlatego lepiej jest je wykrywac. 8.3.3.2.2 ORB opis algorytmu: link 8.3.4 Sound "],["other-models-and-problems.html", "Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Hyperparameters tunning 9.2 Social Network 9.3 Queuing (kolejki) 9.4 Spacial model (modele przestrzenne) 9.5 SIX-Sigma (process quality control, quality control charts) 9.6 Process Analysis (Analiza procesu) 9.7 Reliability and Item Analysis (Analiza rzetelnoci) 9.8 Experimentla design (Planowanie dowiadcze) 9.9 Sequential analysis 9.10 Logic programming (programowanie logiczne) 9.11 Financial models 9.12 Biological/Medical Models 9.13 Case Studies", " Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Hyperparameters tunning Przegld rónych technik: link O bayesowkim algorytmie: link 9.1.1 Bayesian methods Podejcie jest nastpujce. Mamy jak prawdziw nieznan nam funkcj f okrelajc zale scoru modelu od wartoci hiperparametru (czarna przerywana linia na rysunku). Chcemy j wyestymowa eby zobaczy gdzie ma extrema, i w ten sposób zobaczy jaka kombinacja hiperparametrów jest najbardziej optymalna. eby rozwiza ten problem losujemy kilka wartoci parametrów i przeliczamy model eby dosta score (czarne kropki). Dodatkowo zakadamy e mamy surrogate-model który stara si na podstawie tych przeliczonych obserwacji estymowa przebieg funkcji f. Takim modele mog by estymator parzena. Im mamy wicej obserwacji tym surrogate-model lepiej przybliy nam funkcj f. Ale Problem polegam na tym jak efektywnie losowa miejsca do nastpnym przelicze aby to byo efektywne. Do tego poza naszym surrogate model (który nam tutaj bdzie podpowiada) , potrzeba mie funkcj estymujc uzysk z danego punktu. Ta funkcja podpowiada nam nastpny punkt z punktu widzenia dwóch kreteriów: chcemy losowa gdzie mamy wiksze prawdopodobiestwo trafienie na ekstremum chcemy losowa tak w miejscach o których nasz surrogate-model mao wie. S to najczciej punktu daleko od punktów ju wylosowanych. Dziki takiemu losowaniu moemy mie duy przyrost informacji uatwiajcy przyblianie przebiegu funkcji f. 9.1.1.1 Tree-structured Parzen Estimator (TPE) link Powyszy algorytm mona go streci tak link : Define a domain of hyperparameter search space, Create an objective function which takes in hyperparameters and outputs a score (e.g., loss, root mean squared error, cross-entropy) that we want to minimize, Get couple of observations (score) using randomly selected set of hyperparameters, Sort the collected observations by score and divide them into two groups based on some quantile. The first group (x1) contains observations that gave the best scores and the second one (x2) - all other observations, Two densities l(x1) and g(x2) are modeled using Parzen Estimators (also known as kernel density estimators) which are a simple average of kernels centered on existing data points, Draw sample hyperparameters from l(x1), evaluating them in terms of l(x1)/g(x2), and returning the set that yields the minimum value under l(x1)/g(x1) corresponding to the greatest expected improvement. These hyperparameters are then evaluated on the objective function. Update the observation list from step 3 Repeat step 4-7 with a fixed number of trials or until time limit is reached Zakadam e chce minimalizowa score. Tutaj mamy podzia dla próbki w punktu 4. O x to hiperparametr. Poniej mamy rozkady l(x) i g(x). Losujemy punktu z l(x). l(x) jest tworzone Estymatorem Parzena w oparciu o dostpn próbk. 9.1.2 Successive Halving losuje n kombinacji wartoci parametrów. Dla tych kombinacji zaczynam obliczenia. Wykonuje pierwsze k iteracji trenowania n modeli w oparciu o kombinacje wartoci hiperparametrów. Po k iteracjach sprawdzam Jaki wyniki maj modele. Usuwam poowe najgorszych modeli. Wracam do punktu 2. Powtarzam powysz ptle do wyczerpania zasobów (zasobem moe by ustalony z góry czas na obliczenia), lub gdy zostanie jeden model. Powysza metoda ma wady, dlatego powstao jej rozszerzenie w postaci algorytmu hyperband. Pros: szybsza ni klasyczna optymalizacja bayesowska Cons: If n (iloc kombinacji hiperparamatrów) is large, then some good configurations which can be slow to converge at the beginning will be killed off early. If B/n (czas na przeliczenie jednego modeli - ogranicza ilo iteracji)) is large, then bad configurations will be given a lot of resources, even though they could have been stopped before. 9.1.3 Hyperband Tutaj równolegle puszczamy kilka symulacji z których kada ma inn warto n. Wszystkie maj te same budety. Tym samym robi swego rodzaju grid search dla n. 9.1.4 Fabolas Fabolas, standing for fast Bayesian optimization for large datasets, described in [8] tries to increase the speed of Bayesian optimization by learning the ML model on a sampled dataset (but evaluating its performance on the full validation dataset). The idea is that a small portion of the data (less than 10 %) is sufficient to check if a hyper-parameter combination is promising or not. To do it efficiently, it adds another parameter, continuous from 0 to 1, which sets the fraction of the dataset on which to learn the model. It involves a complex acquisition function (which is derived from entropy search, which is an acquisition function based on the predicted information gain around the optimum) and a special covariance kernel. 9.1.5 BOHB BOHB (Bayesian Optimization and HyperBand) mixes the Hyperband algorithm and Bayesian optimization. It is described in [3] and also in this blog post. It uses Hyperband to determine how many configurations to try within a budget but instead of randomly sampling them, it uses a Bayesian optimizer (using a Parzen estimator with no tree structure). At the beginning of the run, it uses Hyperband capability to sample many configurations with a small budget to explore quickly and efficiently the hyper-parameter search space and get very soon promising configurations, but uses the Bayesian optimizer predictive power to propose good configurations close to the optimum. It is also parallel (as Hyperband) which overcomes a strong short-coming of Bayesian optimization. 9.1.6 Population-based training (PTB) The idea is the following: take k agents, each of them are given a model to train, randomly sampled from the search space. After p learning iterations, do the exploit phase of the algorithm: each of the worker compares itself to the other workers (either all of them, a random subset of them, or only one randomly chosen). If the validation performance of the worker is significantly worse (in a statistical meaning) than the best of the others, the worker copies both the hyper-parameters and the weights of the best performing model if the worker has changed its model, do the explore phase of the algorithm, i.e. perturb the hyper-parameters before restarting to learn (several strategies exist: for some randomly chosen hyper-parameters, re-sample them randomly from the search space or perturb them all by a multiplicative coefficient between e.g. 0.8 and 1.2) Loop the algorithm by relearning for p iterations. 9.2 Social Network 9.3 Queuing (kolejki) 9.4 Spacial model (modele przestrzenne) 9.5 SIX-Sigma (process quality control, quality control charts) 9.6 Process Analysis (Analiza procesu) 9.7 Reliability and Item Analysis (Analiza rzetelnoci) 9.8 Experimentla design (Planowanie dowiadcze) 9.9 Sequential analysis 9.10 Logic programming (programowanie logiczne) 9.11 Financial models 9.11.1 Distance to default 9.11.2 Copula methods (kopuy) 9.11.3 Black Scholes 9.11.4 Vasicek 9.11.5 Markovitz 9.11.6 KMV 9.11.7 Credit Metrics 9.11.8 Credit Plus 9.11.9 z-scores 9.11.10 CAPM 9.11.11 VaR - Value at risk 9.11.12 CVA 9.11.13 Acturial models 9.12 Biological/Medical Models 9.13 Case Studies 9.13.1 Score cards Budowanie score cary jest opisane w ksice Naeem Siddiqi Credit Risk Scorecards Developing and Implementing Intelligent Credit Scoring. Przykad przelicze jest na str. 115-117. Uwaga o linowych trendach z WoE: Krzysiek mówi e jeeli to jest naruszone to rozkad scorów nie bdzie zbliony do rozkadu normalnego. Na rpubs jest fajny przykad po R WOE, IV and Scorecards in Credit Risk Modelling 9.13.2 PD models 9.13.3 LGD models 9.13.4 Churn models 9.13.5 ICAAP Mamy szereg DR miesic po miesicu. DR jest liczony w okresie LIP. Czyli dla stycznie mamy styczniowe kredyty i badany ile proc. z nich wpado w default. I tak dalej. Problem jest tutaj taki e w naszym szeregu DR-ów mamy siln korelacj. Defaultu z okien czasowych (dugoci LIP), zazbiaj si dla poszczególnych miesicy. Czyli w przypadku LIP=3miesice dla DR styczniowe i lutowego nakadaj si okresy luty i marze okien czasowych. To mona rozwiza na 3 sposoby: Zrobi pomidzy DR-ami takie okresy eby okna si nie nakaday. Czyli dla LIP = 3miesice pierwszy DR mamy ze stycznia, a nastpny dopiero z kwietnia Zastosowa specjalne schematy losowanie tak aby zbiory obserwacji dla DR-ów si nie nakaday Ograniczy autokorelacje przez urednianie. Czyli robimy np. redni z 3 kolejnych miesicy. Tutaj w ICAAP byo podobno tak e nie bya to rednia ruchowa, ale byy rednie kwartalne i rednia kwartalna bya jedn wartoci zastpujc 3 miesice. Czyli z 12 obserwacji rocznie mamy 4 obserwacje. Kiedy mamy szereg naszych DR-ów przyjmujemy e maj one rozkad Gamma. Na bazie wartoci empirycznych szacujemy zmienno szeregu bdc parametrem rozkadu Gamma. Szeregów jest kilka. Po pierwsze jest kilka portfeli, a po drugie s róne koszyki defaultowe. Np. pierwszy koszyk to obserwacje które zdefautoway o max 30 dni. Drugi (30,90), trzeci &gt;90 itp. Poniewa mamy kilka szeregów to moemy zastosowa PCA. Wczeniej rozkady Gamma przeksztacamy na rozkady normalne. Dziki temu mamy gwarancje e PCA zachowa nam rodzaj rozkadu (PCA to transformacja liniowa zmiennych, a transformacja linowa rozkadu normalnego to rozkad normalny). Bierzemy top x najlepszych wektorów wasnych wg. przypisanych im wartoci wasnych. Nastpnie wracamy do danych pierwotnych wyliczajc tzw wagi czynników. Robimy tutaj dekompozycje na cz zalen od tych top wektorów i reszt (czynniki systematyczne i specyficzne). Ta reszt to wartoci z rozkadu normalnego standardowego mnoone przez wag specyficzn wyliczan przez dopenianie do jednoci kwadratu wag systematycznych. Kiedy sumujemy iloczyny wag i wektorów wasnych dostajemy formua na powrót do zmiennych pierwotnych. Losujemy z rozkadów normalnych naszych wektorów wasnych wartoci podstawiamy do równania i dostajemy kolejne wartoci pierwotne. Wartoci pierwotne nastpnie transformujemy z powrotem do rozkadu Gamma. Robimy transformacje z powrotem do rozkadu beta. Nastpnie robimy losowanie z naszych rozkadów. Wylosowane wartoci (dodatkowo analogicznie jak dla PD robimy analiz dla LGD (zalenego od Cure Rate i Recovery Rate). W ten sposób losowe wartoci PD, LGD podstawiamy pod wzór na strat dostajc jego rozkad. Obcinamy ogon i to jest nasza strata nieoczekiwana. 9.13.6 AMA 9.13.7 Stress tests 9.13.8 Master Scale 9.13.9 Model summer Problem: Mam osoby na wakacjach kredytowych i musz zaprognozowa czy po wakacjach (kiedy bd musieli wróci do skadania kredytu) nie bd w defaulcie. Niestety wakacji wystpuj pierwszy raz i nie mam z przeszoci próby eby wyestymowa model. Algorytm rozwizania: Wakacje dostaj osoby które wczeniej miay dobr histori kredytow. S tutaj róne kryteria zwizane z tym czy sytuacja kredytobiorcy jest dobra. Zostaa zbudowany model klastrujcy klientów którym przyznano wakacje. Mimo e jest to grupa z dobr histori, to przy pomocy takich zmiennych jak dochód, odchylenie od dochodów, maksymalne DPD itp. uda si ich podzieli na dwie kategorie: gorszych i lepszych (przy podziale nie brano oczywicie defaultu bo nie wiemy jak si zachowaj w przyszoci. Nie mamy tutaj próbki z przeszoci i dlatego te robimy model klastrujcy na innych zmiennych które na podstawie naszego dowiadczenia wiemy e mog si przyda do predykcji przeterminowania). Nastpnie z bazy z przeszoci losujemy próbk klientów podobnych do tych którzy mieli przyznane wakacje. Stosujemy tutaj takie kryteria jak np. produkt. Losujemy róne próbki z tej populacji i w oparciu o przesze dane robimy model predykujcy czy bd po pewnym czasie defaultowali. Nastpnie dla obecnych osób w wakacjami robimy modele z poprzedniego punktu predykcje czy bd defaultowali. Otrzymujemy 2 kategorie : bdzie default / nie bdzi default. Na koniec w celu uzyskania ostatecznych wyników robimy gosowanie modeli klastrujcego i predykcynego. Jeeli model klastrujcy mówi e osoba jest w grupie dobrych klientów, a model predykujcy mówi e spaci, to taki klient lduje w grupie niskiego ryzyka. Jeeli predykcje modeli si wykluczaj to dajemy rednie ryzyko. Jeeli model klastrujcy mówi e to gorszy klient, a model predykcyjny mówi e klient bdzie defaultowa to dajemy go to wysokiego ryzyka. "],["appendicies.html", "Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.3 Appendix C Different entries 10.4 Appendix D DICTIONARY POLISH - ENGLISH 10.5 Links - important 10.6 ciga latex 10.7 Courses notes", " Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.2.1 Central Limit 10.2.2 Fisher-Tippett-Gnedenko 10.3 Appendix C Different entries 10.3.1 out-of-bag error 10.3.2 hyperparameters 10.3.3 information leakage 10.3.4 apriori vs aposteriori 10.3.5 colaborative filtering 10.3.6 embedding 10.4 Appendix D DICTIONARY POLISH - ENGLISH loss function - funkcja straty 10.5 Links - important ! - wykrzyknik oznacza wyjtkowo wartociowy materia. 10.5.1 books ksiazka do IML ksiazki w bookdownie statystyka z R-em kasiazka Hastie rozne ksiazki w PDF ekonometria w R ksika o modelach predykcyjnych. Jest tutaj np. analiza reszt: link Lab Guide to Quantitative Research Methods in Political Science, Public Policy &amp; Public Administration: link ! ksizka do machine learningu z przykadami w R link 10.5.2 strony internetowy podrcznik StatSoft lista rozkadów prawdopodobiestwa lista hase statystycznych na wikipedii 101 modeli w R papers with code idre open AI blog machine leargnin mastery kaggle kaggle-blog pyimage search smartdatacollective whatsthebigdata analyticsvidhya AI news sci-hub towardsdatascience kdnuggets web.vu.lt fajny podrecznik do ekonometrii. S tutaj szczegóowo opisane np. modele GLM. explanatory model analysis real statistics Jest tu omówiona factro analysis link - lista metryk i statystyk w pakiecie statsmodels. link manual do MLR pod R-em link 10 najlepszych blogów DataScienceCentral SimplyStatistics datafloq ML Compiled przegld algorytmów ML. ! link lista przykadów algorytmów na geeksforgeeks: 10.5.3 youtube StatQuest Kilcher - analizy artykulow deep learning ai s tutaj wykady Ng Andrew krishnaik stanford machine learning KrishNaik ritvikmath Bardzo dobry kana ze szczegóami matematycznymi. 10.5.4 narzedzia colab google 10.6 ciga latex SPACJA: \\, boldowanie: nowa linia: \\\\ Symbol Kod \\(\\sum_{n=1}^{10}{x^2}\\) _{n=1}{10}{x2} \\(\\sum_{}^{}{x^2}\\) _{}{}{x2} \\(\\prod_{n=1}^{10}{x}\\) \\prod_{n=1}^{10}{x} \\(\\int_{a}^{b} x^2 \\,dx\\) \\int_{a}^{b} x^2 \\,dx \\(\\frac{1}{2}\\) \\(\\begin{pmatrix} 1 &amp; 2 &amp; 3\\\\ a &amp; b &amp; c \\\\ \\end{pmatrix}\\) \\begin{pmatrix} 1 &amp; 2 &amp; 3\\\\ a &amp; b &amp; c \\\\ \\end{pmatrix} \\(\\begin{Bmatrix} 1 &amp; 2 &amp; 3\\\\ a &amp; b &amp; c \\\\ \\end{Bmatrix}\\) \\begin{Bmatrix} 1 &amp; 2 &amp; 3\\\\ a &amp; b &amp; c \\\\ \\end{Bmatrix} \\(z \\in w\\) \\(z \\notin w\\) z w z w \\(\\exists\\) \\(\\forall\\) \\(\\lor\\) \\lor \\(\\land\\) \\(\\neg\\) \\(\\Rightarrow\\) \\Rightarrow \\(\\subset\\) \\(\\subseteq\\) \\(\\infty\\) infty \\(\\{\\) \\(\\langle\\) \\(\\rangle\\) \\(\\to\\) \\(\\cup\\) \\(\\cap\\) \\(\\cong\\) \\(\\sim\\) \\(\\leq\\) \\(\\geq\\) \\(\\div\\) \\(\\cdot\\) \\(\\overline{A}\\) \\(\\hat{A}\\) } \\(\\underset{\\gamma}{argmin}\\) \\underset{\\gamma}{argmin} \\(\\to\\) \\(\\gamma\\) \\(\\Gamma\\) \\(\\beta\\) \\(\\alpha\\) \\(\\theta\\) \\(\\lambda\\) \\(\\epsilon\\) \\(\\zeta\\) \\(\\omega\\) \\(\\partial\\) \\(\\chi\\) \\(\\phi\\) \\(\\nabla\\) \\(\\Omega\\) \\(\\Phi\\) 10.7 Courses notes 10.7.1 Udacity Warto uywa przestrzni kolorów HSV i HSL. W OpenCV kolory domyslnie sa ladownenie nie jako RGB ale jako BGR. Canny Edge detector - etapy jego dziaania: link Ostatnia faza Canny detector (hystheresis) wymaga podania dwóch thresholdów. HougLine detector jest dobrze stosowa po Canny edges detector. Bonaccorso. 2019. Algorytmy Uczenia Maszynowego: Zaawansowane Techniki Implementacji. Helion. Eugeniusz, Gatnar. 2008. Podejcie Wielomodelowe w Zagadnieniach Dyskryminacji i Regresji. 1st ed. PWN. Gatner. 2009. Statystyczna Analiza Danych: Z Wykorzystaniem Programu r. PWN. Geron. 2018. Uczenie Maszynowe z Uyciem Scikit_learn i Tensorflow. Helion. Mirjalili, Raschka. 2019. Python  Uczenie Maszynowe. 2nd ed. Helion. "]]
