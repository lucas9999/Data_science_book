[["index.html", "Data Science Book Chapter 1 INTRO", " Data Science Book ukasz Muszyski 2021-03-30 Chapter 1 INTRO W tej ksice bd znajdoway si g√≥wnie informacje o samych modelach a nie o ich implementacjach w jzykach programowania. "],["introduction.html", "Chapter 2 INTRODUCTION 2.1 TO DO 2.2 Introduction 2.3 AI problems and algorithms classification 2.4 Meta issues", " Chapter 2 INTRODUCTION 2.1 TO DO 2.2 Introduction 2.3 AI problems and algorithms classification 2.4 Meta issues 2.4.1 Knowledge representation 2.4.2 Model complexity 2.4.3 Overfittng / underfitting 2.4.4 Bias / Variance trade off 2.4.5 Curse of dimentionality 2.4.6 Sparious phenomenon "],["auxiliary-fields.html", "Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.8 Math_2  Statistics 3.9 Math_3  Game theory 3.10 Math_4  Optimisation 3.11 Math_4  Other issues", " Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.7.1 Basics 3.7.2 Univariate distributions 3.7.3 Multivariate distributions 3.7.4 Stochastic processes 3.8 Math_2  Statistics 3.8.1 Descriptive statistics 3.8.1.1 Univariate 3.8.1.2 Multivariate 3.8.2 Statistical and ecometrical inference 3.8.2.1 Basics 3.8.2.2 Pameters estimation algorithms 3.8.2.2.1 General moments methods 3.8.2.2.2 Maxium likehood 3.8.2.2.3 Nonparametric techniques 3.8.2.3 Sampling techniques 3.8.2.3.1 Holdout 3.8.2.3.2 Boostrap 3.8.2.3.3 Jacknife 3.8.2.3.4 Permutation 3.8.2.3.5 Cross validation 3.8.2.3.6 Monte Carlo 3.8.2.3.7 Stratified sampling 3.8.3 Other issues 3.8.3.1 Similarity and dissimilarity meassures for observations 3.8.3.2 Similarity and dissimilarity meassures for distributions 3.8.3.3 Measures of disorder/randomness 3.8.3.4 Paradoxes 3.9 Math_3  Game theory 3.10 Math_4  Optimisation 3.10.1 Functions optimum 3.10.2 Functions optimum  constrained 3.10.2.1 Lagrange Multipliers 3.10.2.2 Linear programming 3.10.2.3 Nonlinear programming 3.10.2.4 Regularization 3.10.3 Functional optimum 3.10.3.1 Dynamic programming 3.10.3.2 Calculus of variations 3.10.4 Cost functions 3.10.5 Back propagation 3.10.6 Heuristic algorithms 3.10.6.1 Swarm algorithm 3.10.6.2 Ants algoritms 3.10.6.3 Genetics algorithms 3.11 Math_4  Other issues 3.11.1 Functions usefull in Data Science 3.11.2 Useful tricks 3.11.3 Linear algebra 3.11.4 Combinatorics 3.11.5 Numerical methods 3.11.6 Equations 3.11.6.1 x as number 3.11.6.2 X as derivative: differential and difference equations 3.11.7 Fuzzy logic 3.11.8 Graphs "],["learning-patterns-discovering.html", "Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 4.3 Association 4.4 Dimentionality reduction 4.5 Casuality analysis 4.6 Dimentions decomoposition 4.7 Generative models", " Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 4.2.1 Introduction 4.2.2 Hierarchical 4.2.2.1 Agglomerative/Divisive 4.2.2.2 CHAMELEON 4.2.2.3 BIRCH 4.2.2.4 HDBSCAN 4.2.2.5 ROCK 4.2.2.6 Echidna 4.2.2.7 Diana 4.2.2.8 Agnes 4.2.3 Partititonal 4.2.3.1 k-methods 4.2.3.1.1 k-means 4.2.3.1.2 k-centroids 4.2.3.1.3 k-modes 4.2.3.1.4 k-mini-batches 4.2.3.2 PAM 4.2.3.3 CLARANAS 4.2.3.4 CLARA 4.2.3.5 FCM 4.2.3.6 FCMdC 4.2.3.7 Fanny 4.2.4 Density /Latent distibutions 4.2.4.1 DBSCAN 4.2.4.2 OPTICS 4.2.4.3 PreDeCon 4.2.4.4 SUBCLU 4.2.4.5 DENCLUE 4.2.4.6 DBCLASD 4.2.4.7 Graph based clustering 4.2.4.7.1 Spectral clustering 4.2.4.8 Mean Shift 4.2.4.9 Substractive Methods 4.2.5 Grids 4.2.5.1 STING 4.2.5.2 CLIQUE 4.2.5.3 WaveCluster 4.2.5.4 OptiGrid 4.2.6 Model Based 4.2.6.1 EM 4.2.6.2 CLASSIT 4.2.6.3 SOMs 4.2.6.4 COBWEB 4.2.6.5 Neural Networks 4.2.7 Other 4.2.7.1 Affinity propagation 4.2.7.2 Fuzzy c-means 4.2.8 By problems 4.2.8.1 Numerical and categorical 4.2.8.2 Text data 4.2.8.3 Sound 4.2.8.4 Vision 4.2.9 Results diagnostics 4.2.10 Elements selection 4.2.11 IML 4.2.12 By problems 4.3 Association 4.3.1 Apriori 4.3.2 Euclat 4.3.3 FP-growth 4.3.4 ASSOC 4.3.5 OPUS 4.3.6 Neural Networks 4.3.7 Results diagnostics 4.3.8 Elements selection 4.3.9 IML 4.3.10 By problems 4.4 Dimentionality reduction 4.4.1 Unsupervised 4.4.1.1 PCA 4.4.1.1.1 Basic PCA 4.4.1.1.2 Incremental PCA 4.4.1.1.3 kernel PCA 4.4.1.2 ICA 4.4.1.3 t-SNE 4.4.1.4 Local Linear Embedding 4.4.1.5 Isomap 4.4.1.6 LDA as dimentional reduction 4.4.1.7 Partial Least Squares 4.4.1.8 Multidimentional Scaling 4.4.1.9 Correspondence Analysis 4.4.1.10 Kohonen Networks 4.4.1.11 Neural Networks (other than Kohonen) 4.4.1.12 Factor Analysis 4.4.1.13 Latent semantic analysis 4.4.1.14 Autoencoders 4.4.2 Supervised 4.4.2.1 LDA 4.4.2.2 Partial Least Squares 4.4.3 Results diagnostics 4.4.4 Elements selection 4.4.5 IML 4.4.6 By problems 4.5 Casuality analysis 4.5.1 SEM 4.5.1.1 Confirmatory factor analysis 4.5.1.2 Confirmatory composite analysis 4.5.1.3 Path analysis 4.5.1.4 Partial least squares path modeling 4.5.1.5 Latent growth modeling 4.6 Dimentions decomoposition 4.6.1 Results diagnostics 4.6.2 Elements selection 4.6.3 IML 4.6.4 By problems 4.7 Generative models 4.7.1 Results diagnostics 4.7.2 Elements selection 4.7.3 IML 4.7.4 By problems "],["learning-with-target.html", "Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.2 Econometrical regression 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees 5.6 SVM 5.7 K-NN 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models 5.11 Ensembled models 5.12 Neural Networks 5.13 Stochastic processes 5.14 Results diagnostics 5.15 Elements selection 5.16 IML 5.17 By problems", " Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.1.1 Classification 5.1.2 Regression 5.2 Econometrical regression 5.2.1 Basic regression 5.2.2 Basic dynamic model 5.2.3 Generalisations and constrains 5.2.4 Bayesian inference 5.2.5 Multivariate models 5.2.6 Models with effects 5.2.7 Nonparametric regression 5.2.7.1 Splines 5.2.7.2 Isotonic 5.2.8 Other regression models 5.2.8.1 Canonical analysis 5.2.8.2 ANOVA MANOVA ANCOVA 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees 5.5.1 Classification 5.5.2 Regression 5.6 SVM 5.6.1 Classification 5.6.2 Regression 5.7 K-NN 5.7.1 Classification 5.7.2 Regression 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models 5.11 Ensembled models 5.11.1 Bagging (for example Random Forests) 5.11.2 Boosting (for example Gradient and Ada Boosing) 5.11.3 Stacking 5.11.4 Twicing 5.11.5 Bandling 5.12 Neural Networks 5.12.1 Introduction 5.12.2 Basics 5.12.3 Reccurent 5.12.3.1 Simple reccurent 5.12.3.2 Bidirectorial 5.12.3.3 LSTM 5.12.3.4 GRU 5.12.3.5 Attention 5.12.4 CNN 5.12.5 Resnet 5.13 Stochastic processes 5.13.1 Basic trend models 5.13.2 Basic adaptative models 5.13.3 Econometric time series models 5.13.3.1 dynamic (for example error correction models) 5.13.3.2 SARIMAX 5.13.3.3 VARIMAX 5.13.3.4 ARCH class models 5.13.3.5 Cointegration (including ARLD approach) 5.13.4 Time series decomposition decomposition 5.13.5 Kalman filters 5.13.6 Neural Networks 5.13.6.1 Long short term memory 5.13.6.2 CNN 5.13.7 Panel Regression 5.13.8 Gaussian Mixtures 5.13.9 Ensembled models 5.13.10 Martingales 5.13.11 Markov Process 5.13.12 Winer Process 5.14 Results diagnostics 5.14.1 Classification 5.14.1.1 Scores callibration 5.14.2 Regression 5.15 Elements selection 5.15.1 Variables exogenity 5.15.1.1 Granger Exogenity 5.16 IML 5.17 By problems 5.17.1 Numerical 5.17.2 Categorical 5.17.3 Text 5.17.4 Sound 5.17.5 Vision "],["learning-hybrid.html", "Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.3 Self-supervised learning 6.4 Mult-instance learning", " Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.2.1 EM 6.2.2 CPLE 6.2.3 SVM and TSVM 6.2.4 Graphs 6.2.5 Neural Networks 6.2.6 By problems 6.2.6.1 Numerical and categorical 6.2.6.2 Text data 6.2.6.3 Sound 6.2.6.4 Vision 6.3 Self-supervised learning 6.4 Mult-instance learning "],["learning-reinforcement.html", "Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems", " Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems 7.5.1 Numerical and categorical 7.5.2 Text data 7.5.3 Sound 7.5.4 Vision "],["data-preprocessing.html", "Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.3 Data Problems", " Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.2.1 discretization 8.2.2 numeric variable continuous tranformations 8.2.3 nominal variables tranformations 8.3 Data Problems 8.3.1 Numeric and categorical 8.3.1.1 Extreme values 8.3.1.2 Missing values 8.3.1.3 Untipical distributions (for example copula models, kernel estimators, logaritmic transofmations ect., mixed distributions) 8.3.1.4 Censored/truncated data 8.3.1.5 Aggregated date (decomposition) 8.3.1.6 Meassurement error (for example Kalman filter model) 8.3.1.7 Granularity of data 8.3.1.8 Imbalanced categories 8.3.1.9 Small samples problem 8.3.2 Text 8.3.3 Visual 8.3.4 Sound "],["other-models-and-problems.html", "Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Social Network 9.2 Queuing (kolejki) 9.3 Spacial model (modele przestrzenne) 9.4 SIX-Sigma (process quality control, quality control charts) 9.5 Process Analysis (Analiza procesu) 9.6 Reliability and Item Analysis (Analiza rzetelnoci) 9.7 Experimentla design (Planowanie dowiadcze) 9.8 Sequential analysis 9.9 Logic programming (programowanie logiczne) 9.10 Financial models 9.11 Biological/Medical Models 9.12 Case Studies", " Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Social Network 9.2 Queuing (kolejki) 9.3 Spacial model (modele przestrzenne) 9.4 SIX-Sigma (process quality control, quality control charts) 9.5 Process Analysis (Analiza procesu) 9.6 Reliability and Item Analysis (Analiza rzetelnoci) 9.7 Experimentla design (Planowanie dowiadcze) 9.8 Sequential analysis 9.9 Logic programming (programowanie logiczne) 9.10 Financial models 9.10.1 Distance to default 9.10.2 Black Scholes 9.10.3 Vasicek 9.10.4 Markovitz 9.10.5 KMV 9.10.6 Credit Metrics 9.10.7 Credit Plus 9.10.8 z-scores 9.10.9 CAPM 9.10.10 VaR - Value at risk 9.10.11 CVA 9.10.12 Acturial models 9.11 Biological/Medical Models 9.12 Case Studies 9.12.1 Score cards 9.12.2 PD models 9.12.3 LGD models 9.12.4 Churn models 9.12.5 ICAAP 9.12.6 AMA 9.12.7 Stress tests 9.12.8 Master Scale "],["appendicies.html", "Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.3 Appendix C Different entries 10.4 Appendix D DICTIONARY POLISH - ENGLISH", " Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.2.1 Central Limit 10.2.2 Fisher-Tippett-Gnedenko 10.3 Appendix C Different entries 10.3.1 out-of-bag error 10.3.2 hyperparameters 10.3.3 information leakage 10.3.4 apriori vs aposteriori 10.3.5 colaborative filtering 10.4 Appendix D DICTIONARY POLISH - ENGLISH "]]
