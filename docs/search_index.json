[["index.html", "Data Science Book Chapter 1 INTRO", " Data Science Book ukasz Muszyski 2021-04-06 Chapter 1 INTRO This is notebok about AI algorithms. "],["introduction.html", "Chapter 2 INTRODUCTION 2.1 TO DO 2.2 draft 2.3 Introduction 2.4 AI problems and algorithms classification 2.5 Meta issues", " Chapter 2 INTRODUCTION 2.1 TO DO Feature encoding iml ensembled models conformal prediction calibration of probability isitonic regression possion model correlation between imbalanced variables survuval analysis miary do modeli klasyfikacyjnych feature importance isolated forest GLM - gdzie sa reszty? blad w implementacji xg boosta w sklearn - w kolejnych krokach obserwacje si z prob nakladaja. notat. ARIMA skalowanie platta : svm i score w kalibracji prawdopodobienstwa. optymalizacja byesowska w grid searchu mutual information , entrophy, cross entrophy, od Darii - katy do szukania outliers wezly czyli metoda MARS calibracja w ryzyku kredytowym - dodawanie zewnetrznych informacji git random forest w szeregach czasowych - nie lapie trendow. Uniform Manifold Approximation and Projection (UMAP) autokorelacja i autokorelacja czastkowa colaborative filtering 2.2 draft 2.3 Introduction 2.4 AI problems and algorithms classification 2.5 Meta issues 2.5.1 Knowledge representation 2.5.2 Types of learning 2.5.2.1 Ensemble (zespoowe) Dowód skutecznoci podejcia wielomodelowego: (Eugeniusz 2008) s 84 Ensemlbe jest traktowany jako metoda dedykowana pod uczenie nadzorowane. Dla klastrowania istnieje odpowiednik nazywany Consensus clustering. Zakadamy e: Mamy e w danych mamy dosy skomplikowane zalenoci przy których nie poradzimy sobie z uyciem prostego modelu Zakadamy e moemy zbudowa n prostych modeli (weak learners) z których kady bdzie mia znaczco rónic si wiedz od pozostaych modeli. Ale kady model jest jednak na tyle dobry, e nie jest modelem dajcym czysto losowe rezultaty. Wtedy moe by sensowne uczenie metod *ensemble*. Nie jestemy w stanie zbudowa jednego bardzo skomplikowanego i duego modelu (np. groba overfittingu jeeli dany bardzo gbokie drzewa itp). Wane jednak jest aby modele byy róne pod ktem wiedzy. Uzyskujemy to poprzez: budowanie kadego modelu innym algorytmem stosowanie algorytmów które maj du wariancj (niewielka zmiana w danych moe istotnie wpyn na predykcje). Dlatego najpopularniejszym algorytmem jest drzewo. budowanie modeli na innych podpróbkach budowanie modeli na innym zestawie zmiennych objaniajcych Intuicyjne wyjanienie w (Geron 2018) 2.5.2.1.1 Bagging and Pasting W bagging tworzymy kolejne podpróbki poprzez losowanie boostrapowe. W pasting tworzymy kolejne próbki poprzez losowanie ze zwracaniem. Advantages: Many weak learners aggregated typically outperform a single learner over the entire set, and has less overfit Removes variance in high-variance low-bias data sets[7] Can be performed in parallel, as each separate bootstrap can be processed on its own before combination[8] Disadvantages: In a data set with high bias, bagging will also carry high bias into its aggregate[7] Loss of interpretability of a model. Can be computationally expensive depending on the data set 2.5.2.1.2 Boosting 2.5.2.1.3 Stacking 2.5.3 Model complexity 2.5.4 Overfittng / underfitting 2.5.5 Bias / Variance trade off 2.5.6 Curse of dimentionality 2.5.7 Sparious phenomenon "],["auxiliary-fields.html", "Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.8 Math_2  Statistics 3.9 Math_3  Game theory 3.10 Math_4  Optimisation 3.11 Math_4  Other issues", " Chapter 3 AUXILIARY FIELDS 3.1 Introduction 3.2 Philosophy 3.3 Logic 3.4 Psychology 3.5 Linguistics 3.6 Cybernetics (control theory) 3.7 Math_1  Probability 3.7.1 Basics 3.7.2 Univariate distributions 3.7.3 Multivariate distributions 3.7.4 Stochastic processes 3.8 Math_2  Statistics 3.8.1 Descriptive statistics 3.8.1.1 Univariate 3.8.1.2 Multivariate 3.8.1.2.1 Correlation Semipartial correlation The semipartial (or part) correlation statistic is similar to the partial correlation statistic. Both measure variance after certain factors are controlled for, but to calculate the semipartial correlation one holds the third variable constant for **either X or Y**, whereas for partial correlations one holds the third variable constant for **both**.[6] The semipartial correlation measures unique and joint variance while the partial correlation measures unique variance[clarification needed]. The semipartial (or part) correlation can be viewed as more practically relevant because it is scaled to (i.e., relative to) the total variability in the dependent (response) variable. [7] Conversely, it is less theoretically useful because it is less precise about the unique contribution of the independent variable. Although it may seem paradoxical, the semipartial correlation of X with Y is always less than or equal to the partial correlation of X with Y 3.8.2 Statistical and ecometrical inference 3.8.2.1 Basics 3.8.2.2 Pameters estimation algorithms 3.8.2.2.1 General moments methods 3.8.2.2.2 Maxium likehood 3.8.2.2.3 Nonparametric techniques 3.8.2.3 Sampling techniques 3.8.2.3.1 Holdout 3.8.2.3.2 Boostrap link W bootstrapie z probki n-elementowej losujemy k raz podpróbki n-elementowe ALE ze zwracaniem. Losowanie próbek z podpróbek powinno symulowa losowanie próbek z populacji (The population is to the sample as the sample is to the bootstrap samples). Bootstrap dzielimy na: parametryczny: losowanie jest wykonywane z zadanego rozkadu a nie z samej próbki (czyli zakadamy e znamy klas rozkadu, np. jest to rozkad normalny) semi-parametryczny : The semiparametric bootstrap assumes that the population includes other items that are similar to the observed sample by sampling from a smoothed version of the sample histogram. It turns out that this can be done very simply by first taking a sample with replacement from the observed sample (just like the nonparametric bootstrap) and then adding noise. nieparametryczny. Elementy s losowanie z konkretnych elementów z samej próbki. Boostrap dla regresji (Biecek2008?) s 13. (link)[https://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients] Sample paired response-predictor: Randomly resample pairs of \\(y_ix_i\\), and apply linear regression to each run. After m runs, we obtain a collection of estimated coefficients \\(\\hat{_j},j=1,,m\\).. Finally, compute the quantile of \\(\\hat{_j}\\). Sample error: First apply linear regression on the original observed data, from this model we obtain \\(\\hat{_o}\\) and the error \\(_i\\). Afterwards, randomly resample the error \\(_i\\) and compute the new data with \\(\\hat{_o}\\) and \\(y_i=\\hat{_o}x_i+_i\\). Apply once again linear regression. After m runs, we obtain a collection of estimated coefficeints \\(\\hat{_j},j=1,,m\\). Finally, compute the quantile of \\(\\hat{_j}\\). 3.8.2.3.3 Jacknife 3.8.2.3.4 Permutation (Biecek2008?) s 13 3.8.2.3.5 Cross validation 3.8.2.3.6 Monte Carlo Hasting algorithm. 3.8.2.3.7 Stratified sampling 3.8.3 Other issues 3.8.3.1 Similarity and dissimilarity meassures for observations 3.8.3.2 Similarity and dissimilarity meassures for distributions 3.8.3.3 Measures of disorder/randomness 3.8.3.4 Paradoxes 3.9 Math_3  Game theory 3.10 Math_4  Optimisation 3.10.1 Functions optimum 3.10.2 Functions optimum  constrained 3.10.2.1 Lagrange Multipliers 3.10.2.2 Linear programming 3.10.2.3 Nonlinear programming 3.10.2.4 Regularization 3.10.3 Functional optimum 3.10.3.1 Dynamic programming 3.10.3.2 Calculus of variations 3.10.4 Cost functions 3.10.5 Back propagation 3.10.6 Heuristic algorithms 3.10.6.1 Swarm algorithm 3.10.6.2 Ants algoritms 3.10.6.3 Genetics algorithms 3.11 Math_4  Other issues 3.11.1 Functions usefull in Data Science 3.11.2 Useful tricks 3.11.3 Linear algebra 3.11.4 Combinatorics 3.11.5 Numerical methods 3.11.6 Equations 3.11.6.1 x as number 3.11.6.2 X as derivative: differential and difference equations 3.11.6.3 Choas 3.11.6.3.1 logistic map link: varitasium channel 3.11.7 Fuzzy logic 3.11.8 Graphs "],["learning-patterns-discovering.html", "Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 4.3 Association 4.4 Dimentionality reduction 4.5 Casuality analysis 4.6 Dimentions decomoposition 4.7 Generative models", " Chapter 4 LEARNING: PATTERNS DISCOVERING 4.1 Introduction 4.2 Clustering 4.2.1 Introduction 4.2.2 Hierarchical 4.2.2.1 Agglomerative/Divisive 4.2.2.2 CHAMELEON 4.2.2.3 BIRCH 4.2.2.4 HDBSCAN 4.2.2.5 ROCK 4.2.2.6 Echidna 4.2.2.7 Diana 4.2.2.8 Agnes 4.2.3 Partititonal 4.2.3.1 k-methods 4.2.3.1.1 k-means 4.2.3.1.2 k-centroids 4.2.3.1.3 k-modes 4.2.3.1.4 k-mini-batches 4.2.3.2 PAM 4.2.3.3 CLARANAS 4.2.3.4 CLARA 4.2.3.5 FCM 4.2.3.6 FCMdC 4.2.3.7 Fanny 4.2.4 Density /Latent distibutions 4.2.4.1 DBSCAN 4.2.4.2 OPTICS 4.2.4.3 PreDeCon 4.2.4.4 SUBCLU 4.2.4.5 DENCLUE 4.2.4.6 DBCLASD 4.2.4.7 Graph based clustering 4.2.4.7.1 Spectral clustering 4.2.4.8 Mean Shift 4.2.4.9 Substractive Methods 4.2.5 Grids 4.2.5.1 STING 4.2.5.2 CLIQUE 4.2.5.3 WaveCluster 4.2.5.4 OptiGrid 4.2.6 Model Based 4.2.6.1 EM 4.2.6.2 CLASSIT 4.2.6.3 SOMs 4.2.6.4 COBWEB 4.2.6.5 Neural Networks 4.2.7 Other 4.2.7.1 Affinity propagation 4.2.7.2 Fuzzy c-means 4.2.8 By problems 4.2.8.1 Numerical and categorical 4.2.8.2 Text data 4.2.8.3 Sound 4.2.8.4 Vision 4.2.9 Results diagnostics 4.2.10 Elements selection 4.2.11 IML 4.2.12 By problems 4.3 Association 4.3.1 Apriori 4.3.2 Euclat 4.3.3 FP-growth 4.3.4 ASSOC 4.3.5 OPUS 4.3.6 Neural Networks 4.3.7 Results diagnostics 4.3.8 Elements selection 4.3.9 IML 4.3.10 By problems 4.4 Dimentionality reduction 4.4.1 Unsupervised 4.4.1.1 PCA 4.4.1.1.1 Basic PCA 4.4.1.1.2 Incremental PCA 4.4.1.1.3 kernel PCA 4.4.1.2 t-SNE 4.4.1.3 Local Linear Embedding 4.4.1.4 Isomap 4.4.1.5 LDA as dimentional reduction 4.4.1.6 Partial Least Squares 4.4.1.7 Multidimentional Scaling 4.4.1.8 Correspondence Analysis 4.4.1.9 Kohonen Networks 4.4.1.10 Neural Networks (other than Kohonen) 4.4.1.11 Factor Analysis 4.4.1.12 Latent semantic analysis 4.4.1.13 Autoencoders 4.4.1.14 Analiza glownych skladowych (PCA) [Principal Component Analysis] 4.4.1.15 Analiza glownych wspolrzednych (PCoA) [Principal Coordinates Analysis] 4.4.1.16 Analiza korespondencji (CA) [Correspondenca Analysis] 4.4.1.17 Asymetryczna analiza korespondencji (ACA) [Non-symmetric correspondence analysis] 4.4.1.18 Kanoniczna analiza korespondencji (CCA) [Canonical correspondence analysis] 4.4.1.19 Laczna analiza korespondencji (JCA) [Joint Correspondence Analysis] 4.4.1.20 Odwrocona analiza korespondencji (InvCA) [Inverse correspondence analysis] 4.4.1.21 Taksowkowa analiza korespondencji (TCA) [Taxicab Correspondence Analysis] 4.4.1.22 Rozmyta analiza korespondencji (FCA) [Fuzzy correspondence analysis] 4.4.1.23 Wieloraka analiza korespondencji (MCA) [Multiple correspondence analysis] 4.4.1.24 Regularyzowana wieloraka analiza korespondencji (RMCA) [Regularized Multiple Correspondence Analysis] 4.4.1.25 Analiza czynnikowa (FA) Factor analysis 4.4.1.26 Wieloraka analiza czynnikowa (MFA) [Multiple Factor Analysis] 4.4.1.27 Analiza skladowych niezaleznych (ICA) [Independent component analysis ] 4.4.1.28 (DCA) [Detreded Correspondence Analysis] 4.4.1.29 Nieliniowa Analiza Korespondencji (NPCA) [Nonlinear Principal Components Analysis] 4.4.1.30 (MDS) [Non-Metric Multidimensional Scaling] 4.4.2 Supervised 4.4.2.1 LDA 4.4.2.2 Partial Least Squares 4.4.3 Results diagnostics 4.4.4 Elements selection 4.4.5 IML 4.4.6 By problems 4.5 Casuality analysis 4.5.1 SEM Modele SEM su do analizowanie przyczynowoci/korelacji miedzy zmiennymi. Bada tworzy okrelone zaoenia co do struktury przyczynowoci/korelacji midzy zmiennymi i potem to weryfikuje. W podejciu takim moemy równie stawia hipotezy na temat zmiennych ukrytych latent variables. Filmik gdzie mam pokazane co jest estymowane w podejciu ze zmiennymi ukrytymi: (link)[https://www.youtube.com/watch?v=nR94_juMpX0] typy analiz SEM typy path analysis i inne rzeczy o SEM 4.5.1.1 Confirmatory factor analysis 4.5.1.2 Confirmatory composite analysis 4.5.1.3 Path analysis W tym podejciu zakadamy e wszystkie zmiennej s jawna (nie ma zmiennych ukrytych). 4.5.1.4 Partial least squares path modeling 4.5.1.5 Latent growth modeling 4.6 Dimentions decomoposition 4.6.1 Results diagnostics 4.6.2 Elements selection 4.6.3 IML 4.6.4 By problems 4.7 Generative models 4.7.1 Results diagnostics 4.7.2 Elements selection 4.7.3 IML 4.7.4 By problems "],["learning-with-target.html", "Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.2 Econometrical regression 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees 5.6 SVM 5.7 K-NN 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models 5.11 Ensembled models 5.12 Neural Networks 5.13 Stochastic processes 5.14 Results diagnostics 5.15 Elements selection 5.16 IML 5.17 By problems", " Chapter 5 LEARNING: WITH TARGET 5.1 Introduction 5.1.1 Classification 5.1.2 Regression 5.2 Econometrical regression 5.2.1 Basic regression 5.2.2 Basic dynamic model 5.2.3 Generalisations and constrains 5.2.4 Bayesian inference 5.2.5 Multivariate models 5.2.6 Models with effects 5.2.7 Nonparametric regression 5.2.7.1 Splines 5.2.7.2 Isotonic 5.2.8 Other regression models 5.2.8.1 Canonical analysis 5.2.8.2 ANOVA MANOVA ANCOVA 5.3 LDA &amp; QDA 5.4 Bayesian models 5.5 Trees Drzewo jest przykadem algorytmu zachannego (greedy) Dlaczego w praktyce uywa si tylko drzew binarnych: (link: stack_change) The number of possible splits goes up exponentially. If you are splitting on a continuous variable that has 1000 distinct values, there are 999 binary splits, but 999*998 trinary splits. There are \\(\\binom{1000-1}{3-1} = 999*998/2\\) splits, actually. 5.5.1 pros Nie trzeba preprocesowa danych. Nie trzeba normalizowa zmiennych cigych. Zmiennych jakociowych nie trzeba rekodowa. moliwo pracy z danymi jakociowymi i ilociowymi s nieparametryczne. Nie maj zaoe o rozkadach nie ma problemu z brakami danych. Przy analizie zmiennej na splicie braki s prostu pomijane. atwa interpretacja szybkie wyliczenie predykcji przez nisk zoono obliczeniow O(log(m)). Tak naprawd same przeprowadzaj selekcje cech (feature selection). 5.5.2 cons atwo model przetrenowa. S niestabilne. Mae zmiany w danych generuj mocno rónice si drzewa. Przez te problemy wystpuje dua wariancja modelu i sabe uogólnianie. s algorytmem zachannym wic nie daj gwarancji znalezienia optimum globalnego. dosy dugo czas estymacji modelu wraliwo na rotacje danych (Geron 2018) s. 184. dhirajkumarblog.medium 5.5.3 Classification 5.5.4 Regression 5.6 SVM 5.6.1 Classification 5.6.2 Regression 5.7 K-NN 5.7.1 Classification Jak wyliczane jest prawdopodobiestwo w sklearn : link The class probabilities are the normalized weighted average of indicators for the k-nearest classes, weighted by the inverse distance. For example: Say we have 6 classes, and the 5 nearest examples to our test input have class labels F, B, D, A, and B, with distances 2, 3, 4, 5, and 6, respectively. Then the unnormalized class probabilities can by computed by: (1/2) * [0, 0, 0, 0, 0, 1] + (1/3) * [0, 1, 0, 0, 0, 0] + (1/4) * [0, 0, 0, 1, 0, 0] + (1/5) * [1, 0, 0, 0, 0, 0] + (1/6) * [0, 1, 0, 0, 0, 0] = [1/5 ,1/2, 0, 1/4, 0, 1/2] 5.7.2 Regression 5.8 Log-linear model 5.9 Similarity learning 5.10 Survival models modele typi survival +====================================================================+ ++ 5.11 Ensembled models 5.11.1 Bagging and Pasting 5.11.1.1 Random Forest 5.11.1.1.1 Out of Bag Error 5.11.2 Boosting 5.11.2.1 Ada Boost Adaptive boosting (adaptacyjne wzmacnianie). Mam tutaj 2 rodzaje wag: Wagi modeli. Im lepszy model tym bdzie mia w finalnej klasyfikacji wiksz wag. \\(\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-total.error}{totl.error})\\) . Poniewa funkcja nie ma wartoci dla total_error równe 0 i 1 zazwyczaj dodaje si tutaj jak korekt dla zabezpieczenia. Total error to suma bdów waonych wagami obserwacji. Wagi obserwacji. Obserwacje le zaklasyfikowane przez i-ty model maj wiksz wag przy nastpnym modelu. Wagi mog by uywane do losowania waonego dla nastpnego modelu, albo do waonego Ginii index uywanego do obliczania impurity. Wagi dla obserwacji le zaklasyfikowanych liczy si ze wzoru : \\(nowa.waga = stara.waga \\cdot e^{waga.poprzedniego.modelu}\\). Wagi dla klasyfikacji dobrze zaklasyfikowanych liczy si ze wzoru: \\(nowa.waga = stara.waga \\cdot e^{- waga.poprzedniego.modelu}\\) . W tym wzorach mona doda wspóczynnik uczenia w wykadniku liczby e. Patrz: (Bonaccorso 2019) s 263. Ada boost: Zazwyczaj bazuje na drzewach. Jeeli s to drzewa, to najczciej uywa si stumps czyli drzew binarnych z tylko jednym podziaem. wystpuje w m.in nastpujcych wersjach: Bazowy AdaBoost do zagadnie binarnych. M1 - podstawowy algorytm dla zagadnienia klasyfikacyjnego. (Mirjalili 2019) s 234. M2 - (porównanie z M1 link) SUMME - jest uogólnieniem na zagadnienie wieloklasowego bez uywania podejcia jeden-przeciwko-wszystkim (One-vs-Rest). Jeeli robimy model binarny to podecie to redukuje si do standardowego AdaBoost M1. SUMME.R - (litera R od real - AdaBoost rzeczywisty) rozwinicie, gdzie wagi s liczone w oparciu o prawdopodobiestwa. Peny algorytm w (Bonaccarso2019?) s 268. R2 - AdaBoost dla zagadnienia regresyjnego. Peny algorytm w (Bonaccarso2019?) s 271. SUMME W AdaBoost M1 dla przypadku binarnego, waga modelu w t-ej iteracji zdefiniowana jako \\(\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-total.error}{totl.error})\\) przyjmuje warto 0 jeeli model jest losowy, czyli dostajemy 50% le zaklasyfikowanych elementów (jeeli model zaklasyfikowa poprawnie mniej ni 50% to po prostu odwracamy jego predykcje i dostajemy model lepszy od losowego). Jeeli jednak mamy wicej klas to próg losowoci musi by inaczej zrobiony i zaleny od iloci klas. Model gdzie jest 10 równolicznych klas i dobrze sklasyfikowa 50% obserwacji jest duo lepszy od modelu losowego. Dlatego wzór na wag modelu musi zosta skorygowany. SUMMER.R Tutaj wagi modeli dla kadej iteracji s liczone w oparciu o prawdopodobiestwa przynalenoci do klas. Kady model ma inna wag dla kadej z klas. Nie jest tak jak w standardowym AdaBoost e jest jedna waga dla modelu. Wagi obserwacji te s liczone w oparciu o te prawdopodobiestwa. Przy tych wagach uwzgldniamy te faktyczne wartoci empiryczne targetu. eby policzy wag t-ego modelu dla k-tej klasy najpierw bierzemy obserwacje w tej klasy (przynaleno do klasy wynika z danych empirycznych, a nie jest estymowana z modelu). Dla tych obserwacji liczymy REDNIE wyestymowane z modelu prawdopodobiestwo przynaleenia obserwacji do tej klasy. Przy urednianiu prawdopodobiestwa powinny chyba powinny by uywane wagi obserwacji. Dla danej klasy jest tym wiksza waga modelu in wysze jest prawdopodobiestwo przynaleenia tej klasy wedug modelu. W modelu decyzja o klasyfikacji i-tej obserwacji jest podejmowana na podstawie wyboru klasy dla której suma wago modelu po wszystkich iteracjach jest najwiksza (pamitajmy e wagi modeli s per klasa). Algorytm SUMME.R daje wyniki zbiene do addytywnej regresji logistycznej. Jest uwaany za bardziej efektywny ni klasyczne wersja AdaBoosta NIE oparta na prawdopodobiestwach. AdaBoost w R od scratch-a: link:rpubs Assumptions Quality Data: Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality. Outliers: Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset. Noisy Data: Noisy data, specifically noise in the output variable can be problematic. If possible, attempt to isolate and clean these from your training dataset. Pros Cons Boosting technique learns progressively, it is important to ensure that you have quality data. AdaBoost is also extremely sensitive to Noisy data and outliers so if you do plan to use AdaBoost then it is highly recommended to eliminate them. AdaBoost has also been proven to be slower than XGBoost. 5.11.2.2 LightGBM link:tword_data_sience 5.11.3 Stacking 5.11.4 Twicing 5.11.5 Bandling 5.12 Neural Networks 5.12.1 Introduction 5.12.2 Basics 5.12.3 Reccurent 5.12.3.1 Simple reccurent 5.12.3.2 Bidirectorial 5.12.3.3 LSTM 5.12.3.4 GRU 5.12.3.5 Attention 5.12.4 CNN 5.12.5 Resnet 5.13 Stochastic processes 5.13.1 Basic trend models 5.13.2 Basic adaptative models 5.13.3 Econometric time series models 5.13.3.1 dynamic (for example error correction models) 5.13.3.2 SARIMAX 5.13.3.3 VARIMAX 5.13.3.4 ARCH class models 5.13.3.5 Cointegration (including ARLD approach) 5.13.4 Time series decomposition decomposition 5.13.5 Kalman filters 5.13.6 Neural Networks 5.13.6.1 Long short term memory 5.13.6.2 CNN 5.13.7 Panel Regression 5.13.8 Gaussian Mixtures 5.13.9 Ensembled models 5.13.10 Martingales 5.13.11 Markov Process 5.13.12 Winer Process 5.14 Results diagnostics 5.14.1 Classification 5.14.1.1 Scores calibration 5.14.1.1.1 Problem Kalibracja dotyczy prawdopodobiestwa które nie odpowiadaj poziomom ufnosci miara (scores) z modeli które nia sa prawdopodobiestwami (SVM zwraca score jako odleglosc obserwacji o hiperplaszczyzny separujacej, a w K-NN mozemy budowac miary oparte o odleglosci miedzy obserwacjami - wiecej w k-NN dla klasyfikacji) ale chcemy, aby te scory byly przerobione na prawdopodobienstwa nie wiem co z przypadkiem kiedy mamy same labels z modelu i czy mona je przeksztaca na prawdopodobiestwo. Jednak takie modele sa rzadkoscia: Nearly every classifier - ogistic regression, a neural net, a decision tree, a k-NN classifier, a support vector machine, etc.  can produce a score instead of (or in addition to) a class label.1 5.14.1.1.2 Kalibracja a problemy konkretnych modeli Random Forest: RandomForestClassifier shows the opposite behavior: the histograms show peaks at approximately 0.2 and 0.9 probability, while probabilities close to 0 or 1 are very rare. An explanation for this is given by Niculescu-Mizil and Caruana 1: Methods such as bagging and random forests that average predictions from a base set of models can have difficulty making predictions near 0 and 1 because variance in the underlying base models will bias predictions that should be near zero or one away from these values. Because predictions are restricted to the interval [0,1], errors caused by variance tend to be one-sided near zero and one. For example, if a model should predict p = 0 for a case, the only way bagging can achieve this is if all bagged trees predict zero. If we add noise to the trees that bagging is averaging over, this noise will cause some trees to predict values larger than 0 for this case, thus moving the average prediction of the bagged ensemble away from 0. We observe this effect most strongly with random forests because the base-level trees trained with random forests have relatively high variance due to feature subsetting. As a result, the calibration curve also referred to as the reliability diagram (Wilks 1995 2) shows a characteristic sigmoid shape, indicating that the classifier could trust its intuition more and return probabilities closer to 0 or 1 typically. LogisticRegression: Returns well calibrated predictions by default as it directly optimizes Log loss. In contrast, the other methods return biased probabilities; with different biases per method: GaussianNB: Tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features. Linear Support Vector Classification (LinearSVC): shows an even more sigmoid curve than RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana 1), which focus on difficult to classify samples that are close to the decision boundary (the support vectors). 5.14.1.1.3 Calibration curve (reliability diagram) how to make it First, the forecast values are partitioned into bins Bk, k = 1, . . . , K (which form a partition of the unit interval into nonoverlapping exhaustive subintervals). The Bk are often taken to be of equal width, but if the distribution of the forecast values is nonuniform, then choosing the bins so that they are equally populated is an attractive alternative. Next, for each i, it is established which of the K bins the forecast value Xi falls into. For each bin Bk, let Ik be the collection of all indices i for which Xi falls into bin Bk; that is, \\(I_k:=\\{i;X_i \\in B_k\\}\\) The corresponding observed relative frequency fk is the number of times the event happens, given that Xi  Bk, divided by the total number of forecast values Xi  Bk. This can be expressed as: \\(f_k=\\frac{\\sum_{i \\in I_k}^{}{Y_i}}{\\#I_k}\\) where #Ik denotes the number of elements in Ik. Each bin Bk is represented by a single typical forecast probability rk. Although the arithmetic center of the bin is often used to represent the forecast values in that bin, this method has a clear disadvantage: If the forecast is reliable, the observed relative frequency for a given bin Bk is expected to coincide with the average of the forecast values over that bin Bk, rather than with the arithmetic center of the bin. Plotting the observed relative frequency over the arithmetic center can cause even a perfect reliability diagram to be off the diagonal by up to half the width of a bin. In this paper, observed relative frequencies for a bin Bk are plotted versus the average of the forecast values over bin Bk. This average, denoted by rk, is: \\(r_k:=\\frac{\\sum_{i \\in I_k}{X_i}}{\\#I_k}\\) The reliability diagram comprises a plot of \\(f_k\\) versus rk for all bins \\(B_k\\). 5.14.1.1.4 Skalowanie Platta link Steps for applying Platt scaling Split the data set into train and test data set. Train the model on the training data set. Apply SGD (stochstic gradient descent) Classifier to minimize hinge loss. Apply Calibrated Classifier from sklearn and take SGD classifier as a base estimator. Sort the predicted probability scores in ascending order. Divide the sorted probability and actual y into multiple bins. Here, we are taking bin size as 50. Take the average of actual y and predicted probabilities for each bins. Plot average of actual y on y-axis and average of predicted probability on x-axis. Pros: Works well with a small dataset Cons: Could produce worse probabilities calibration wise if the assumptions do not hold Skalowanie dla zagadnienia multiklasowego platts-scaling-for-multi-label-classification There are a few multiclass variants of Platt scaling. The easiest approach is as you have described; simply perform one Platt scaling on each class. However, there are more sophisticated optionsa very simple one to implement is training a standard logistic regression on the logits (the values before the softmax activation is applied). This has called matrix scaling and can overfit pretty easily, so only use this if you have a large calibration set. Alternatively, a fewer-parameter version called vector scaling is relatively simple to implement, where the weights matrix inside the logistic regression is restricted to be a diagonal matrix. Finally, a very simple option that has been shown to work well for neural networks is temperature scaling, where all logits are simply scaled by a single scalar parameter. You can read more about these and their application to neural networks in Section 4.2 of On Calibration of Modern Neural Networks (2017) - available here 5.14.1.1.5 Regresja izotoniczna link Pros: Makes no assumption about the input probabilities. A benefit of isotonic regression is that it is not constrained by any functional form, such as the linearity imposed by linear regression, as long as the function is monotonic increasing. Cons: Requires more data points to work well 5.14.1.1.6 Calibration in sklearn There are 2 ways of using the sklearn CalibratedClassifierCV class : Pass a fitted model and thereby setting cv to prefit. It is important to note that the data used in fitting the base estimator and the calibrator is disjoint. Fit a base estimator using k-fold cross-validation and the probabilities for each of the folds are then averaged for prediction. 5.14.2 Regression 5.15 Elements selection 5.15.1 Feature selection 5.15.1.0.1 Feature Importance MDI MDA Mean Decrease Accuracy, MDA, also known as permutation importance. The approach can be described in the following steps: 1. Train the baseline model and record the score (accuracy/R²/any metric of importance) by passing the validation set (or OOB set in case of Random Forest). This can also be done on the training set, at the cost of sacrificing information about generalization. 2. Re-shuffle values from one feature in the selected dataset, pass the dataset to the model again to obtain predictions and calculate the metric for this modified dataset. The feature importance is the difference between the benchmark score and the one from the modified (permuted) dataset. 5.15.2 Variables exogenity 5.15.2.1 Granger Exogenity 5.16 IML 5.17 By problems 5.17.1 Numerical 5.17.2 Categorical 5.17.3 Text 5.17.4 Sound 5.17.5 Vision "],["learning-hybrid.html", "Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.3 Self-supervised learning 6.4 Mult-instance learning", " Chapter 6 LEARNING: HYBRID 6.1 Introduction 6.2 Semi-supervised learning 6.2.1 EM 6.2.2 CPLE 6.2.3 SVM and TSVM 6.2.4 Graphs 6.2.5 Neural Networks 6.2.6 By problems 6.2.6.1 Numerical and categorical 6.2.6.2 Text data 6.2.6.3 Sound 6.2.6.4 Vision 6.3 Self-supervised learning 6.4 Mult-instance learning "],["learning-reinforcement.html", "Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems", " Chapter 7 LEARNING: REINFORCEMENT 7.1 Introduction 7.2 TD 7.3 SARSA 7.4 Q-learning 7.5 By problems 7.5.1 Numerical and categorical 7.5.2 Text data 7.5.3 Sound 7.5.4 Vision "],["data-preprocessing.html", "Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.3 Data Problems", " Chapter 8 DATA PREPROCESSING 8.1 Introduction 8.2 Variables tranformations 8.2.1 discretization 8.2.2 numeric variable continuous tranformations 8.2.3 nominal variables tranformations 8.2.3.1 encoding 8.2.3.1.1 one-hot 8.2.3.1.2 embedding In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well. Embedding is type of categorical variables encoding. It can be also treated as a way for dimentionality reduction. 8.2.3.2 other 8.3 Data Problems 8.3.1 Numeric and categorical 8.3.1.1 Extreme values 8.3.1.2 Missing values 8.3.1.3 Untipical distributions (for example copula models, kernel estimators, logaritmic transofmations ect., mixed distributions) 8.3.1.4 Censored/truncated data 8.3.1.5 Aggregated date (decomposition) 8.3.1.6 Meassurement error (for example Kalman filter model) 8.3.1.7 Granularity of data 8.3.1.8 Imbalanced categories 8.3.1.9 Small samples problem 8.3.2 Text 8.3.3 Visual 8.3.4 Sound "],["other-models-and-problems.html", "Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Social Network 9.2 Queuing (kolejki) 9.3 Spacial model (modele przestrzenne) 9.4 SIX-Sigma (process quality control, quality control charts) 9.5 Process Analysis (Analiza procesu) 9.6 Reliability and Item Analysis (Analiza rzetelnoci) 9.7 Experimentla design (Planowanie dowiadcze) 9.8 Sequential analysis 9.9 Logic programming (programowanie logiczne) 9.10 Financial models 9.11 Biological/Medical Models 9.12 Case Studies", " Chapter 9 OTHER MODELS AND PROBLEMS 9.1 Social Network 9.2 Queuing (kolejki) 9.3 Spacial model (modele przestrzenne) 9.4 SIX-Sigma (process quality control, quality control charts) 9.5 Process Analysis (Analiza procesu) 9.6 Reliability and Item Analysis (Analiza rzetelnoci) 9.7 Experimentla design (Planowanie dowiadcze) 9.8 Sequential analysis 9.9 Logic programming (programowanie logiczne) 9.10 Financial models 9.10.1 Distance to default 9.10.2 Copula methods (kopuy) 9.10.3 Black Scholes 9.10.4 Vasicek 9.10.5 Markovitz 9.10.6 KMV 9.10.7 Credit Metrics 9.10.8 Credit Plus 9.10.9 z-scores 9.10.10 CAPM 9.10.11 VaR - Value at risk 9.10.12 CVA 9.10.13 Acturial models 9.11 Biological/Medical Models 9.12 Case Studies 9.12.1 Score cards 9.12.2 PD models 9.12.3 LGD models 9.12.4 Churn models 9.12.5 ICAAP 9.12.6 AMA 9.12.7 Stress tests 9.12.8 Master Scale "],["appendicies.html", "Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.3 Appendix C Different entries 10.4 Appendix D DICTIONARY POLISH - ENGLISH 10.5 Links - important", " Chapter 10 APPENDICIES 10.1 Appendix A INDEX OF STATISTICAL TEST 10.2 Appendix B Most important theorems in Statistics and probability calculus 10.2.1 Central Limit 10.2.2 Fisher-Tippett-Gnedenko 10.3 Appendix C Different entries 10.3.1 out-of-bag error 10.3.2 hyperparameters 10.3.3 information leakage 10.3.4 apriori vs aposteriori 10.3.5 colaborative filtering 10.3.6 embedding 10.4 Appendix D DICTIONARY POLISH - ENGLISH 10.5 Links - important 10.5.1 books ksiazka do IML ksiazki w bookdownie statystyka z R-em kasiazka Hastie rozne ksiazki w PDF ekonometria w R 10.5.2 strony internetowy podrcznik StatSoft lista rozkadów prawdopodobiestwa lista hase statystycznych na wikipedii 101 modeli w R papers with code idre open AI blog machine leargnin mastery kaggle pyimage search AI news sci-hub towardsdatascience 10.5.3 youtube StatQuest Kilcher - analizy artykulow deep learning ai s tutaj wykady Ng Andrew krishnaik stanford machine learning Bonaccorso. 2019. Algorytmy Uczenia Maszynowego: Zaawansowane Techniki Implementacji. Helion. Eugeniusz, Gatnar. 2008. Podejcie Wielomodelowe w Zagadnieniach Dyskryminacji i Regresji. 1st ed. PWN. Geron. 2018. Helion. Mirjalili, Raschka. 2019. Python  Uczenie Maszynowe. 2nd ed. Helion. "]]
